"use strict";(self.webpackChunkclarity=self.webpackChunkclarity||[]).push([[1404],{91140:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>i,default:()=>l,frontMatter:()=>r,metadata:()=>o,toc:()=>h});var n=t(17624),s=t(4552);const r={id:"cec3_task2_data",title:"Task 2 Data",sidebar_label:"Data",sidebar_position:20},i=void 0,o={id:"cec3/task_2/cec3_task2_data",title:"Task 2 Data",description:"The Task 2 data consists of real acoustic scenes that have been recorded over hearing aid shells, each with three channels (front, middle, back). The scenes are based on the domestic environments used in previous Clarity challenges consisting of a target sentence and either two or three interferers. The interferers can be speech, music or noise from domestic appliances in any combination. Data has been split into two sets: 6,000 scenes for training and a 2,500 scene development set. A further 1,500 scenes were recorded and have been set aside for evaluation.",source:"@site/docs/cec3/task_2/task2_data.mdx",sourceDirName:"cec3/task_2",slug:"/cec3/task_2/cec3_task2_data",permalink:"/docs/cec3/task_2/cec3_task2_data",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:20,frontMatter:{id:"cec3_task2_data",title:"Task 2 Data",sidebar_label:"Data",sidebar_position:20},sidebar:"tutorialSidebar_cec3",previous:{title:"Overview",permalink:"/docs/cec3/task_2/cec3_task2_overview"},next:{title:"Rules",permalink:"/docs/cec3/task_2/cec3_task2_rules"}},d={},h=[{value:"Recording setup",id:"recording-setup",level:2},{value:"Audio and Headtracking Data format",id:"audio-and-headtracking-data-format",level:2},{value:"Speaker adaptation data",id:"speaker-adaptation-data",level:2},{value:"Metadata Formats",id:"metadata-formats",level:2}];function c(e){const a={a:"a",code:"code",em:"em",h2:"h2",p:"p",pre:"pre",...(0,s.M)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.p,{children:"The Task 2 data consists of real acoustic scenes that have been recorded over hearing aid shells, each with three channels (front, middle, back). The scenes are based on the domestic environments used in previous Clarity challenges consisting of a target sentence and either two or three interferers. The interferers can be speech, music or noise from domestic appliances in any combination. Data has been split into two sets: 6,000 scenes for training and a 2,500 scene development set. A further 1,500 scenes were recorded and have been set aside for evaluation."}),"\n",(0,n.jsx)(a.p,{children:"The data download contains the following directories,"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-text",children:"clarity_CEC3_data\n|\u2500\u2500 manifest\n|\u2500\u2500 task1\n|\u2500\u2500 task2\n|   \u2514\u2500\u2500 clarity_data\n|       |\u2500\u2500 metadata  - metadata for each scene\n|       |\u2500\u2500 train\n|       |   |\u2500\u2500 scenes  - audio and headtracking data for training scenes\n|       |   |\u2500\u2500 interferers - complete set of interferer signals available for training\n|       |   \u2514\u2500\u2500 targets - complete set of target signals available for training\n|       \u2514\u2500\u2500 dev\n|           |\u2500\u2500 scenes\n|           |\u2500\u2500 interferers\n|           |\u2500\u2500 targets\n|           \u2514\u2500\u2500 speaker_adapt - speaker adaptation utterances (see below)\n\u2514\u2500\u2500 task3\n"})}),"\n",(0,n.jsxs)(a.p,{children:["A full description of the data will be appearing here shortly. In the meantime, you can download the data from the ",(0,n.jsx)(a.a,{href:"./cec3_download",children:"download page"}),"."]}),"\n",(0,n.jsx)(a.p,{children:"The sections below describe the recording setup, the format of the audio and headtracking data, and the metadata provided for each scene."}),"\n",(0,n.jsx)(a.h2,{id:"recording-setup",children:"Recording setup"}),"\n",(0,n.jsx)(a.p,{children:"The scenes were recorded over the period of two weeks in a recording room at the University of Sheffield. The target and interferer signals were played using a set of 13 loudspeakers arranged around a listener. The listener was wearing hearing aid shells with three microphones (front, middle, back) and a pair of glasses with reflective markers that were tracked with a Vicon motion capture system. Nine loudspeakers were placed in front of the listener arranged at semi-randomly selected locations on a polar grid (the grid has distances of 2 m to 4 m spaced at 0.5 m and angles from -60 degrees to 60 degrees spaced at 7.5 degrees). The remaining four loudspeakers were placed in a line behind the listener. For each scene, one of the front loudspeakers was chosen to play the target utterances, and two or three of the 12 remaining speakers (front and back) were chosen to play the interferers."}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.em,{children:"Add a few pictures here"})}),"\n",(0,n.jsx)(a.p,{children:"Scenes were recording in blocks of 125 where each block had the same target speaker and the same speaker locations. The front speakers were labelled 1 to 9 so that they could be easily identified by the listener. For each scene one of the 9 front speakers, other than the target speaker, was chosen as the initial look direction speaker. Before the scene is played, the look-direction speaker plays it's number and the listener is instructed to turn to face it. Then after a short pause the multisource scene plays. Within the scene, the target speaker starts about two seconds after the interfering noise sources. The listener is asked to attend to the target speech source and turn to face it when it starts, and also to note down the speaker number. This process is repeated 125 times with a short pause between scenes. The entire block recording lasts about 20 minutes and once recording is complete the audio and headtracking data is segmented into individual scenes."}),"\n",(0,n.jsx)(a.p,{children:"After each recording block the front speakers are moved to a new set of locations on the grid. Each of the 80 blocks has its own unique speaker layout. The speaker locations were randomised such there was only ever one loudspeaker along each radial direction and that the loudspeaker were spread across the full range of angles. Some examples are shown below.\nThe precise locations of the speakers are provided in the metadata for each scene."}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.em,{children:"Add a diagram here"})}),"\n",(0,n.jsx)(a.p,{children:"Following CEC2, the scenes were designed to produce a large range of SNRs for the target. This was set to be between -12 dB and 6 dB in the condition that all the loudspeaker signals are instanteously added. Note, the actual SNRs recorded at the microphones will vary further depending on the rooms acoustics and the listeners head rotation etc, e.g., SNRs will be higher than that recording in the meta data if the target is closer to the listener than the interferers, and lower if the target is further away. (We plan to release estimates of the SNRs at the microphones in the future.)"}),"\n",(0,n.jsx)(a.h2,{id:"audio-and-headtracking-data-format",children:"Audio and Headtracking Data format"}),"\n",(0,n.jsx)(a.p,{children:"All audio data is provided in 16-bit PCM format at a sample rate of 48 kHz. File names have been designed to be compatible with previous Clarity challenges. For each scene the folloiwng audio files are provided:"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-text",children:"- <SCENE_ID>_mix_CH1.wav - the left and right stereo pair from the front microphone.\n- <SCENE_ID>_mix_CH2.wav - the left and right stereo pair from the middle microphone.\n- <SCENE_ID>_mix_CH3.wav - the left and right stereo pair from the back microphone.\n- <SCENE_ID>_target.wav - the target speech signal.\n- <SCENE_ID>_interferer.wav - the interferer signals stored as either 2 or 3 channel audio files.\n- <SCENE_ID>_reference.wav - the signal to be used as the reference for HASPI evaluation.\n"})}),"\n",(0,n.jsx)(a.p,{children:"These signals are provided for both the training and development data set. When the evaluation data is released only the mix files will be provided."}),"\n",(0,n.jsx)(a.p,{children:"The headtracking data is provided in a CSV file with a frame rate of 250 Hz,"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-text",children:"- <SCENE_ID>_hr.csv - the audio-aligned 6 DOF head track\n"})}),"\n",(0,n.jsx)(a.p,{children:"The headtracking has been carefully aligned with the audio recordings. The csv files have six columns: TX, TY, TZ, RX, RY and RZ. (TX, TY, TZ) are the locations of the head with respect to the tracker origin. (RX, RY, RZ) is the rotation of the head with respect to a level, forward facing head. The angles are stored in radians in `helical' form, i.e. they represent a vector which whose direction is the axis of rotation and whose magnitude is the size of the rotation in radian."}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.em,{children:"Add figure here"})}),"\n",(0,n.jsx)(a.p,{children:"The headtracking data is provided for both the training and development data set and will be provided for optional use during evaluation as well."}),"\n",(0,n.jsx)(a.h2,{id:"speaker-adaptation-data",children:"Speaker adaptation data"}),"\n",(0,n.jsx)(a.p,{children:"The scenes that you have been asked to enhance often contain speech signals as interferers. This means that the task of enhancing the target speaker is ambiguous unless you are told which of the speaker to use as target. In this task, we follow the approach used in CEC2 and provide a small set of clean target speaker example utterances. So, for each scene, the ID of the target speaker is provided in the metadata, and systems can then use the examples and select the target as the one that has the matching voice."}),"\n",(0,n.jsx)(a.p,{children:"Note, these same utterances will be used in the final subjective listening tests, i.e. the listeners will be presented with the examples before listening to the processed signal and told that these are examples of the target speaker that they are meant to be listening to."}),"\n",(0,n.jsx)(a.h2,{id:"metadata-formats",children:"Metadata Formats"}),"\n",(0,n.jsx)(a.p,{children:"The following metadata files are provided"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-text",children:"- scenes.train.csv - metadata for the training scenes\n- rooms.train.csv - metadata for the training rooms\n"})})]})}function l(e={}){const{wrapper:a}={...(0,s.M)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},4552:(e,a,t)=>{t.d(a,{I:()=>o,M:()=>i});var n=t(11504);const s={},r=n.createContext(s);function i(e){const a=n.useContext(r);return n.useMemo((function(){return"function"==typeof e?e(a):{...a,...e}}),[a,e])}function o(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),n.createElement(r.Provider,{value:a},e.children)}}}]);