"use strict";(self.webpackChunkclarity=self.webpackChunkclarity||[]).push([[1986],{16934:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>d,default:()=>f,frontMatter:()=>l,metadata:()=>h,toc:()=>u});var a=n(74848),r=n(28453),s=n(11470),i=n(19365),o=n(86025);const l={id:"cec3_task1_data",title:"Task 1 Data",sidebar_label:"Data",sidebar_position:20},d=void 0,h={id:"cec3/task_1/cec3_task1_data",title:"Task 1 Data",description:"The Task 1 data consists of simulated hearing aid inputs that have been constructed using a set of real high-order ambisonic impulses that were recorded for the challenge. The scenes follow the construction used in the 2nd Clarity Enhancement Challenge and consist of a target sentence and either two or three interferers. The interferers can be speech, music or noise from domestic appliances in any combination. We have published a 2,500 scene development set. A further 1,500 scenes were recorded and have been set aside for evaluation. The training data from CEC2 is available for use in training.",source:"@site/docs/cec3/task_1/task1_data.mdx",sourceDirName:"cec3/task_1",slug:"/cec3/task_1/cec3_task1_data",permalink:"/docs/cec3/task_1/cec3_task1_data",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:20,frontMatter:{id:"cec3_task1_data",title:"Task 1 Data",sidebar_label:"Data",sidebar_position:20},sidebar:"tutorialSidebar_cec3",previous:{title:"Overview",permalink:"/docs/cec3/task_1/cec3_task1_overview"},next:{title:"Rules",permalink:"/docs/cec3/task_1/task1_rules"}},c={},u=[{value:"Recording setup",id:"recording-setup",level:2},{value:"The hearing aid signal simulation",id:"the-hearing-aid-signal-simulation",level:2},{value:"Audio data format",id:"audio-data-format",level:2},{value:"Metadata formats",id:"metadata-formats",level:2},{value:"The room metadata",id:"the-room-metadata",level:3},{value:"The scene metadata",id:"the-scene-metadata",level:3}];function m(e){const t={a:"a",code:"code",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.p,{children:"The Task 1 data consists of simulated hearing aid inputs that have been constructed using a set of real high-order ambisonic impulses that were recorded for the challenge. The scenes follow the construction used in the 2nd Clarity Enhancement Challenge and consist of a target sentence and either two or three interferers. The interferers can be speech, music or noise from domestic appliances in any combination. We have published a 2,500 scene development set. A further 1,500 scenes were recorded and have been set aside for evaluation. The training data from CEC2 is available for use in training."}),"\n",(0,a.jsxs)(t.p,{children:["The data is organised into the following directories, and can be obtained from the ",(0,a.jsx)(t.a,{href:"./../cec3_download",children:"download page"}),"."]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-text",children:"clarity_CEC3_data\n|\u2500\u2500 manifest\n|\u2500\u2500 task1\n|   \u2514\u2500\u2500clarity_data\n|       |\u2500\u2500 metadata\n|       |\u2500\u2500 train (use CEC2)\n|       \u2514\u2500\u2500 dev\n|           |\u2500\u2500 scenes\n|           \u2514\u2500\u2500 speaker_adapt\n|\u2500\u2500 task2\n\u2514\u2500\u2500 task3\n"})}),"\n",(0,a.jsx)(t.p,{children:"The sections below describe the impulse response recording setup, the signal mixing and the format of the audio files and metadata provided for each scene."}),"\n",(0,a.jsx)(t.h2,{id:"recording-setup",children:"Recording setup"}),"\n",(0,a.jsxs)(t.p,{children:["The task uses a novel set of impulse responses that were recorded at the University of Salford using an ",(0,a.jsx)(t.a,{href:"https://mhacoustics.com/products",children:"mh acoustics em64 Eigenmike"}),". These responses can then be used in place of the 6th order ambisonic impulse responses that were generated using room simulation in the previous challenge."]}),"\n",(0,a.jsx)(t.p,{children:"Recordings were made for 32 random configurations of a listener, a target and up to three interferers. Configurations were randomised in advanced and marked out on the floor of the recording room. For each configuration, the microphone is placed at the position of the listener and a loudspeaker is placed, in turn, at each of the sound source positions and directed towards the microphone. The sine-sweep method is then used to estimate the impulse response. The process is repeat for all 32 configurations and for target and three interferers, i.e. 32 x (3 + 1) = 128 impulse responses are recorded in total."}),"\n",(0,a.jsx)(t.p,{children:"The room is an acoustically treated recording room with approximate dimensions of 5m x 5m x 2m. Some images are provided below for context."}),"\n",(0,a.jsxs)(s.A,{children:[(0,a.jsx)(i.A,{value:"scene",label:"The recording room",children:(0,a.jsxs)("figure",{id:"fig1",children:[(0,a.jsx)("img",{width:"700",src:(0,o.A)("/img/CEC3/salford_room1.jpg")}),(0,a.jsx)("figcaption",{children:"Figure 1. A view of the recording room with floor markings."})]})}),(0,a.jsx)(i.A,{value:"scene2",label:"...another view",children:(0,a.jsxs)("figure",{id:"fig2",children:[(0,a.jsx)("img",{width:"700",src:(0,o.A)("/img/CEC3/salford_room2.jpg")}),(0,a.jsx)("figcaption",{children:"Figure 2. Recording one of the impulse responses with the em64 Eigenmike."})]})}),(0,a.jsx)(i.A,{value:"scene3",label:"... and another.",children:(0,a.jsxs)("figure",{id:"fig3",children:[(0,a.jsx)("img",{width:"700",src:(0,o.A)("/img/CEC3/salford_pano.jpg")}),(0,a.jsx)("figcaption",{children:"Figure 3. A panoramic shot of the recording room."})]})})]}),"\n",(0,a.jsx)(t.p,{children:"The room configurations were randomly generated by independently selecting the x and y coordinates of the listener, target and interferers. The positions were chosen uniformly at random within the dimensions of the room, excluding a 1m border around the walls. It was also imposed that no sound source should be within 1 m of the listener, i.e., samples were rejected and redrawn if this was the case. As with CEC2,  for each configuration a height (z) was randomly chosen to be either 1.2m (simulating sitting) or 1.6m (simulating standing) and the microphone and loudspeakers were placed at this height."}),"\n",(0,a.jsxs)(t.p,{children:["The figure below shows the layouts of the 16 rooms used in the development data. The rooms are identified by the room number (R20001 to R20016) and the precise positions of the listener, target and interferers are given in the metadata files (see Section ",(0,a.jsx)("a",{href:"#metadata-formats",children:"Metadata Formats"}),")."]}),"\n",(0,a.jsxs)("figure",{id:"fig4",children:[(0,a.jsx)("img",{width:"700",src:(0,o.A)("/img/CEC3/cec3_salford_rooms.jpg")}),(0,a.jsx)("figcaption",{children:"Schematics showing the 16 room layouts used in the development data. T = Target talker; I = Interferer; L = Listener."})]}),"\n",(0,a.jsx)(t.h2,{id:"the-hearing-aid-signal-simulation",children:"The hearing aid signal simulation"}),"\n",(0,a.jsx)(t.p,{children:"Details to appear"}),"\n",(0,a.jsx)(t.h2,{id:"audio-data-format",children:"Audio data format"}),"\n",(0,a.jsx)(t.p,{children:"All audio data is provided in 16-bit PCM format at a sample rate of 44100 kHz. File names have been designed to be compatible with previous Clarity challenges. For each scene the following audio files are provided:"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-text",children:"- <SCENE_ID>_mix_CH1.wav - the left and right stereo pair from the front microphone.\n- <SCENE_ID>_mix_CH2.wav - the left and right stereo pair from the middle microphone.\n- <SCENE_ID>_mix_CH3.wav - the left and right stereo pair from the back microphone.\n- <SCENE_ID>_hr.wav - the head rotation signal\n- <SCENE_ID>_reference.wav - the signal to be used as the reference for HASPI evaluation.\n"})}),"\n",(0,a.jsx)(t.h2,{id:"metadata-formats",children:"Metadata formats"}),"\n",(0,a.jsx)(t.p,{children:"The following metadata files are provided"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-text",children:"# The description of the scenes\n- scenes.dev.json - metadata for the dev scenes\n- rooms.dev.json - metadata for the dev rooms\n- hrir_data.json - HRIRs using in the simulation\n\n# Listener information\n- scenes_listeners.dev.json - the listeners/scene pairings to for the standard development set\n- listeners.json - the audiograms of the listeners\n\n# Materials used to make up the scenes\n- masker_music_list.json - the list of music interferers\n- masker_nonspeech_list.json - the list of non-speech interferers\n- masker_speech_list.json - the list of speech interferers\n- target_speech_list.json - the list target utterances\n"})}),"\n",(0,a.jsxs)(t.p,{children:["Most of these files follow the same format as in CEC2. The most important are the ",(0,a.jsx)(t.code,{children:"scenes.json"})," and ",(0,a.jsx)(t.code,{children:"rooms.json"})," files and these are described below."]}),"\n",(0,a.jsxs)(t.p,{children:["The ",(0,a.jsx)(t.code,{children:"room"})," describes the locations of the three loudspeakers and the microphone. There are 16 different 'rooms' each describing a different loudspeaker and microphone layout (as shown in the earlier figure). The 'scenes' are represented by a 'room' (i.e., a loudspeaker configuration) and a description of the target and interferers and which of the loudspeakers they were played from. Note, there is a one-to-many relationship between rooms and scenes, i.e., each room is re-used in multiple scenes although not always with the same selection of interferer locations."]}),"\n",(0,a.jsxs)(t.p,{children:["The ",(0,a.jsx)(t.code,{children:"room"})," and ",(0,a.jsx)(t.code,{children:"scene"})," metadata files are described in more detail below."]}),"\n",(0,a.jsx)(t.h3,{id:"the-room-metadata",children:"The room metadata"}),"\n",(0,a.jsx)(t.p,{children:"The room metadata is stored in a JSON file as a list of dictionaries, with one dictionary representing each room. There are 16 room layouts released for development data. A separate set of 15 will be used for evaluation. The metadata for the evaluation set will remain hidden."}),"\n",(0,a.jsx)(t.p,{children:"The format is as follows:"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-json",children:'[\n  {\n    "name": "R001",  // The Room identifier (R001 to R080)\n    "dimensions": "5.0x5.0x2.0",  // Approximate room dimensions (fixed)\n     "target": { // The target (i.e., the talker position)\n      "position": [ // The target position\n        3.0,\n        4.2,\n        1.2  // Heights are either 1.2 or 1.6 and target, listener and inferferer heights are always match\n      ],\n      "view_vector": [ // The target direction - will be towards the listener\n        -0.179,\n        0.984,\n        0.0\n      ]\n    },\n    "listener": { // The listener (i.e., the microphone position)\n      "position": [ // The listener position\n        3.4,\n        2.0,\n        1.2\n      ],\n      "view_vector": [ // The listener default view direction\n        0.179,\n        -0.984,\n        0.0\n      ]\n    },\n    "interferers": [ // A list of 3 interferer positions\n      {\n        "position": [\n          2.2,\n          2.0,\n          1.2\n        ]\n      }, // ... followed by two more positions \n    ]\n  },\n  ... //  more rooms \n]\n'})}),"\n",(0,a.jsx)(t.h3,{id:"the-scene-metadata",children:"The scene metadata"}),"\n",(0,a.jsx)(t.p,{children:"The scene metadata is stored in a JSON file as a list of dictionaries with one dictionary for each scene. There are 2500 scenes in the development set. A further 1500 have been retained for the final evaluation and these will use a different set of room layouts."}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-json",children:'[\n  {\n    "dataset": "dev", // The dataset (dev or eval)\n    "room": "R20001", // The room identifier (R20001 to R20016) corresponds to R001 to R016 in the rooms metadata\n    "scene": "S06001", // The Scene ID  (S6001 to S8500)\n    "target": {\n      "name": "T030_A08_01034", // The utterance ID which starts with the talker ID (T001 to T040)\n      "time_start": 80491, // The sample at which the target starts\n      "time_end": 214114 // The sample at which the target ends\n    },\n    "duration": 258214, // The duration of the scene in samples\n    "interferers": [ // Either 2 or 3 interferers in positions 1, 2 and/or 3\n      {\n        "position": 1, // The loudspeaker number (indexed 1 to 3). The location is in the rooms metadata. \n        "time_start": 0, // The sample at which the interferer starts (always 0)\n        "time_end": 258214, // The sample at which the interferer ends (always the end of the scene)\n        "type": "music", // The type of interferer (speech, music, noise)\n        "name": "51/662051.low.mp3",   // The interferer ID\n        "offset": 2563402 // The offset of the interferer in the complete audio file\n      },\n      // ... followed by one or two more interferers\n    ],\n    "SNR": -9.5306,  // The SNR of the target signal (-12 to 6 dB)\n    "listener": {\n      "rotation": [ // Describes a rotation in the horizontal plane\n        {\n          "sample": 74252.5369, // The time (in samples) at which the rotation starts\n          "angle": 71.2240 // The initialial angle in degrees\n        },\n        {\n          "sample": 82883.5369,  // The time  (in samples) at which the rotation starts\n          "angle": 97.4871 // The final angle degrees\n        }\n      ],\n      "hrir_filename": [ // The HRIR used to simulate the hearing aid inputs\n        "BuK-ED",  // The \'ear drum\' HRIR\n        "BuK-BTE_fr", // The front microphone HRIR\n        "BuK-BTE_mid",  // The middle microphone HRIR\n        "BuK-BTE_rear"  // The rear microphone HRIR\n      ]\n    }\n  },\n // ... more scenes\n]\n'})}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:["All times in the ",(0,a.jsx)(t.code,{children:"scenes.json"})," file are measured in samples at 44100 Hz."]}),"\n",(0,a.jsxs)(t.li,{children:["In order to simulate the hearing aid signals, the HRIRs are taken from the  ",(0,a.jsx)(t.a,{href:"https://uol.de/mediphysik/downloads/hearingdevicehrtfs",children:"OlHeadHRTF database"})," with permission . Different scenes have used different heads as indicated by the ",(0,a.jsx)(t.code,{children:"hrir_filename"})," field."]}),"\n"]})]})}function f(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(m,{...e})}):m(e)}},19365:(e,t,n)=>{n.d(t,{A:()=>i});n(96540);var a=n(34164);const r={tabItem:"tabItem_Ymn6"};var s=n(74848);function i(e){let{children:t,hidden:n,className:i}=e;return(0,s.jsx)("div",{role:"tabpanel",className:(0,a.A)(r.tabItem,i),hidden:n,children:t})}},11470:(e,t,n)=>{n.d(t,{A:()=>j});var a=n(96540),r=n(34164),s=n(23104),i=n(56347),o=n(205),l=n(57485),d=n(31682),h=n(89466);function c(e){return a.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,a.isValidElement)(e)&&function(e){const{props:t}=e;return!!t&&"object"==typeof t&&"value"in t}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function u(e){const{values:t,children:n}=e;return(0,a.useMemo)((()=>{const e=t??function(e){return c(e).map((e=>{let{props:{value:t,label:n,attributes:a,default:r}}=e;return{value:t,label:n,attributes:a,default:r}}))}(n);return function(e){const t=(0,d.X)(e,((e,t)=>e.value===t.value));if(t.length>0)throw new Error(`Docusaurus error: Duplicate values "${t.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[t,n])}function m(e){let{value:t,tabValues:n}=e;return n.some((e=>e.value===t))}function f(e){let{queryString:t=!1,groupId:n}=e;const r=(0,i.W6)(),s=function(e){let{queryString:t=!1,groupId:n}=e;if("string"==typeof t)return t;if(!1===t)return null;if(!0===t&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:t,groupId:n});return[(0,l.aZ)(s),(0,a.useCallback)((e=>{if(!s)return;const t=new URLSearchParams(r.location.search);t.set(s,e),r.replace({...r.location,search:t.toString()})}),[s,r])]}function p(e){const{defaultValue:t,queryString:n=!1,groupId:r}=e,s=u(e),[i,l]=(0,a.useState)((()=>function(e){let{defaultValue:t,tabValues:n}=e;if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(t){if(!m({value:t,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${t}" but none of its children has the corresponding value. Available values are: ${n.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return t}const a=n.find((e=>e.default))??n[0];if(!a)throw new Error("Unexpected error: 0 tabValues");return a.value}({defaultValue:t,tabValues:s}))),[d,c]=f({queryString:n,groupId:r}),[p,g]=function(e){let{groupId:t}=e;const n=function(e){return e?`docusaurus.tab.${e}`:null}(t),[r,s]=(0,h.Dv)(n);return[r,(0,a.useCallback)((e=>{n&&s.set(e)}),[n,s])]}({groupId:r}),b=(()=>{const e=d??p;return m({value:e,tabValues:s})?e:null})();(0,o.A)((()=>{b&&l(b)}),[b]);return{selectedValue:i,selectValue:(0,a.useCallback)((e=>{if(!m({value:e,tabValues:s}))throw new Error(`Can't select invalid tab value=${e}`);l(e),c(e),g(e)}),[c,g,s]),tabValues:s}}var g=n(92303);const b={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var v=n(74848);function w(e){let{className:t,block:n,selectedValue:a,selectValue:i,tabValues:o}=e;const l=[],{blockElementScrollPositionUntilNextRender:d}=(0,s.a_)(),h=e=>{const t=e.currentTarget,n=l.indexOf(t),r=o[n].value;r!==a&&(d(t),i(r))},c=e=>{let t=null;switch(e.key){case"Enter":h(e);break;case"ArrowRight":{const n=l.indexOf(e.currentTarget)+1;t=l[n]??l[0];break}case"ArrowLeft":{const n=l.indexOf(e.currentTarget)-1;t=l[n]??l[l.length-1];break}}t?.focus()};return(0,v.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,r.A)("tabs",{"tabs--block":n},t),children:o.map((e=>{let{value:t,label:n,attributes:s}=e;return(0,v.jsx)("li",{role:"tab",tabIndex:a===t?0:-1,"aria-selected":a===t,ref:e=>l.push(e),onKeyDown:c,onClick:h,...s,className:(0,r.A)("tabs__item",b.tabItem,s?.className,{"tabs__item--active":a===t}),children:n??t},t)}))})}function x(e){let{lazy:t,children:n,selectedValue:r}=e;const s=(Array.isArray(n)?n:[n]).filter(Boolean);if(t){const e=s.find((e=>e.props.value===r));return e?(0,a.cloneElement)(e,{className:"margin-top--md"}):null}return(0,v.jsx)("div",{className:"margin-top--md",children:s.map(((e,t)=>(0,a.cloneElement)(e,{key:t,hidden:e.props.value!==r})))})}function T(e){const t=p(e);return(0,v.jsxs)("div",{className:(0,r.A)("tabs-container",b.tabList),children:[(0,v.jsx)(w,{...e,...t}),(0,v.jsx)(x,{...e,...t})]})}function j(e){const t=(0,g.A)();return(0,v.jsx)(T,{...e,children:c(e.children)},String(t))}},28453:(e,t,n)=>{n.d(t,{R:()=>i,x:()=>o});var a=n(96540);const r={},s=a.createContext(r);function i(e){const t=a.useContext(s);return a.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),a.createElement(s.Provider,{value:t},e.children)}}}]);