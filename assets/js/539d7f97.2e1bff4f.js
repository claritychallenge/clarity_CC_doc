"use strict";(self.webpackChunkclarity_cec_1=self.webpackChunkclarity_cec_1||[]).push([[6513],{3905:(e,t,n)=>{n.d(t,{Zo:()=>h,kt:()=>g});var a=n(67294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var s=a.createContext({}),c=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},h=function(e){var t=c(e.components);return a.createElement(s.Provider,{value:t},e.children)},p="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,i=e.originalType,s=e.parentName,h=l(e,["components","mdxType","originalType","parentName"]),p=c(n),m=r,g=p["".concat(s,".").concat(m)]||p[m]||u[m]||i;return n?a.createElement(g,o(o({ref:t},h),{},{components:n})):a.createElement(g,o({ref:t},h))}));function g(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=n.length,o=new Array(i);o[0]=m;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[p]="string"==typeof e?e:r,o[1]=l;for(var c=2;c<i;c++)o[c]=n[c];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},10807:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>o,default:()=>u,frontMatter:()=>i,metadata:()=>l,toc:()=>c});var a=n(87462),r=(n(67294),n(3905));const i={slug:"One approach to our enhancement challenge",title:"One approach to our enhancement challenge",authors:[{name:"Trevor Cox",title:"Clarity Team Member",url:"http://trevorcox.me/trevor-cox",image_url:"https://avatars.githubusercontent.com/trevorjcox"}],tags:["DNN","enhancement","evaluation","GAN","hearing aid","knowledge distillation"]},o=void 0,l={permalink:"/blog/One approach to our enhancement challenge",source:"@site/blog/2020-07-29-one-approach-to-our-enhancement-challenge.md",title:"One approach to our enhancement challenge",description:"Improving hearing aid processing using DNNs blog. A suggested approach to overcome the non-differentiable loss function.",date:"2020-07-29T00:00:00.000Z",formattedDate:"July 29, 2020",tags:[{label:"DNN",permalink:"/blog/tags/dnn"},{label:"enhancement",permalink:"/blog/tags/enhancement"},{label:"evaluation",permalink:"/blog/tags/evaluation"},{label:"GAN",permalink:"/blog/tags/gan"},{label:"hearing aid",permalink:"/blog/tags/hearing-aid"},{label:"knowledge distillation",permalink:"/blog/tags/knowledge-distillation"}],readingTime:3.59,hasTruncateMarker:!0,authors:[{name:"Trevor Cox",title:"Clarity Team Member",url:"http://trevorcox.me/trevor-cox",image_url:"https://avatars.githubusercontent.com/trevorjcox",imageURL:"https://avatars.githubusercontent.com/trevorjcox"}],frontMatter:{slug:"One approach to our enhancement challenge",title:"One approach to our enhancement challenge",authors:[{name:"Trevor Cox",title:"Clarity Team Member",url:"http://trevorcox.me/trevor-cox",image_url:"https://avatars.githubusercontent.com/trevorjcox",imageURL:"https://avatars.githubusercontent.com/trevorjcox"}],tags:["DNN","enhancement","evaluation","GAN","hearing aid","knowledge distillation"]},prevItem:{title:"Clarity Challenge pre-announcement",permalink:"/blog/Clarity Challenge pre-announcement"},nextItem:{title:"The speech-in-noise problem part two",permalink:"/blog/The speech-in-noise problem part two"}},s={authorsImageUrls:[void 0]},c=[{value:"References",id:"references",level:2}],h={toc:c},p="wrapper";function u(e){let{components:t,...i}=e;return(0,r.kt)(p,(0,a.Z)({},h,i,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("p",null,"Improving hearing aid processing using DNNs blog. A suggested approach to overcome the non-differentiable loss function."),(0,r.kt)("p",null,"The aim of our Enhancement Challenge is to get people producing new algorithms for processing speech signals through hearing aids. We expect most entries to replace the classic hearing aid processing of Dynamic Range Compressors (DRCs) with ",(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Deep_learning"},"deep neural networks (DNN)")," (although all approaches are welcome!). The first round of the challenge is going to be all about improving speech intelligibility."),(0,r.kt)("p",null,"Setting up a DNN structure and training regime for the task is not as straightforward as it might first appear. Figure 1 shows an example of a naive training regime. An audio example of Speech in Noise (SPIN) is randomly created (",(0,r.kt)("em",{parentName:"p"},"audio sample generation"),", bottom left), and a listener is randomly selected with particular hearing loss characteristics (",(0,r.kt)("em",{parentName:"p"},"random artificial listener generation"),", top left). The DNN Enhancement model (represented by the bright yellow box) then produces improved speech in noise. (Audio signals in pink are two-channel, left and right because this is for binaural hearing aids.)"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"schematic",src:n(6347).Z,width:"1149",height:"446"})),(0,r.kt)("p",null,"Figure 1"),(0,r.kt)("p",null,"Next the improved speech in noise is passed to the Prediction Model in the lime green box, and this gives an estimation of the Speech Intelligibility (SI). Our baseline system will include algorithms for this. We\u2019ve already blogged about the Hearing Loss Simulation. Our current thinking is that the intelligibility model will be using a binaural form of the Short-Time Objective Intelligibility Index (STOI) ","[1]",". The dashed line going back to the enhancement model shows that the DNN will be updated based on the reciprocal of the Speech Intelligibility (SI) score. By minimising (1/SI), the enhancement model will be maximising intelligibility."),(0,r.kt)("p",null,"The difficulty here is that updating the Enhancement Model DNN during training requires the error to be known at the DNN\u2019s output (the point labelled \u201cimproved SPIN\u201d). But we don\u2019t know this, we only know the error on the output of the prediction model at the far right of the diagram. This wouldn\u2019t be a problem if the prediction model could be inverted, because we could then run the 1/SI error backwards through the inverse model."),(0,r.kt)("p",null,"As the inverse of the prediction model isn\u2019t available, one solution is to train another DNN to mimic its behaviour (Figure 2). As this new Prediction Model is a DNN, the 1/SI error can be passed backwards through it using standard neural network training formulations."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"schematic",src:n(87779).Z,width:"873",height:"432"})),(0,r.kt)("p",null,"This DNN prediction model could be trained first using knowledge distillation (",(0,r.kt)("a",{parentName:"p",href:"http://usir.salford.ac.uk/id/eprint/56234/"},"this is something I\u2019ve previous done for a speech intelligibility model"),"), and then the weights frozen while the Enhancement Model is trained. But there is a \u2018chicken and egg\u2019 problem here. The difficulty is generating all the training data for the prediction model. Until you train the enhancement model, you won\u2019t have a representative examples of \u201cimproved SPIN\u201d to train the prediction model. But without the prediction model, you can\u2019t train the enhancement model."),(0,r.kt)("p",null,"One solution is to train the two DNNs in tandem, with an approach analogous to how pairs of networks are trained in a ",(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Generative_adversarial_network"},"Generative Adversarial Network")," (GAN). iMetricGan developed by Li et al. ","[2]"," is an example of this being done for speech enhancement, although the authors weren\u2019t trying to include hearing loss simulation. They aren\u2019t the only ones looking at trying to solve problems where a non-differentiable or black-box evaluation function is in the way of DNN training ","[3][4]","."),(0,r.kt)("p",null,"We hope our entrants will come up with lots of other ways of overcoming this problem. How would you tackle it?"),(0,r.kt)("h2",{id:"references"},"References"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"[1]"," Andersen, A.H., Haan, J.M.D., Tan, Z.H. and Jensen, J., 2015. A binaural short time objective intelligibility measure for noisy and enhanced speech. In the ",(0,r.kt)("em",{parentName:"li"},"Sixteenth Annual Conference of the International Speech Communication Association"),"."),(0,r.kt)("li",{parentName:"ul"},"[2]"," Li, H., Fu, S.W., Tsao, Y. and Yamagishi, J., 2020. iMetricGAN: Intelligibility Enhancement for Speech-in-Noise using Generative Adversarial Network-based Metric Learning. ",(0,r.kt)("em",{parentName:"li"},"arXiv preprint arXiv:2004.00932"),"."),(0,r.kt)("li",{parentName:"ul"},"[3]"," Gillhofer, M., Ramsauer, H., Brandstetter, J., Sch\xe4fl, B. and Hochreiter, S., 2019. A GAN based solver of black-box inverse problems. Proceedings of the ",(0,r.kt)("em",{parentName:"li"},"NeurIPS 2019 Workshop"),"."),(0,r.kt)("li",{parentName:"ul"},"[4]"," Kawanaka, M., Koizumi, Y., Miyazaki, R. and Yatabe, K., 2020, May. Stable training of DNN for speech enhancement based on perceptually-motivated black-box cost function. In ICASSP 2020-2020 ",(0,r.kt)("em",{parentName:"li"},"IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)")," (pp. 7524-7528). IEEE.")))}u.isMDXComponent=!0},6347:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/clarity_schematic_for_blog-09-6f015d990f75f14a4068ca7ab8295f69.png"},87779:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/clarity_schematic_for_blog-10-7ee2dc89faab4e5ffb0ccb240d682e86.png"}}]);