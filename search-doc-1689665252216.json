[{"title":"Announcement of ICASSP 2023 Grand Challenge","type":0,"sectionRef":"#","url":"blog/Announcement of ICASSP 2023 Grand Challenge","content":"We are pleased to announce that registration for the ICASSP 2023 Clarity Grand Challenge is now open. To register please complete the simple Google form found on the registration page. The remaining important dates for the challenge are as follows: 28th Nov 2022: Challenge launch: Release training/dev data; tools; baseline; rules &amp; documentation.2nd Feb 2023: Release of evaluation data.10th Feb 2023: Teams submit processed signals and technical reports.14th Feb 2023: Results released. Top 5 ranked teams invited to submit papers to ICASSP-202320th Feb 2023: Invited papers submitted to ICASSP-20234-9th June 2023: Overview paper and invited papers presented at dedicated ICASSP session The challenge training, dev data and initial tools are now fully from the Github repository. If you have any questions please do not hesitate to contact us at claritychallengecontact@gmail.com.","keywords":""},{"title":"Baseline speech intelligibility model in round one","type":0,"sectionRef":"#","url":"blog/baseline","content":"","keywords":""},{"title":"Some comments on signal alignment and level-insensitivity​","type":1,"pageTitle":"Baseline speech intelligibility model in round one","url":"blog/baseline#some-comments-on-signal-alignment-and-level-insensitivity","content":"Our baseline binaural speech intelligibility measure in round one is the Modified Binaural Short-Time Objective Intelligibility measure, or MBSTOI. This short post outlines the importance of correcting for delays that your hearing aid processing algorithm introduces into the audio signals to allow MBSTOI to estimate the speech intelligibility accurately. It also discusses the importance of considering the audibility of signals before evaluation with MBSTOI. "},{"title":"Evaluation​","type":1,"pageTitle":"Baseline speech intelligibility model in round one","url":"blog/baseline#evaluation","content":"In stage one, entries will be ranked according to the average MBSTOI score across all samples in the evaluation test set. In the second stage, entries will be evaluated by the listening panel. There will be prizes for both stages. See this page for more information. "},{"title":"Signal alignment in time and frequency​","type":1,"pageTitle":"Baseline speech intelligibility model in round one","url":"blog/baseline#signal-alignment-in-time-and-frequency","content":"If the signal processed by the hearing aid introduces a significant delay, you should correct for this delay before submitting your entry. This is necessary because MBSTOI requires alignment of the clean speech “reference” with the processed signal in time and frequency. This needs to be done for both ear signals. MBSTOI downsamples signals to 10 kHz, uses a Discrete Fourier Transform to decompose the signal into one-third octave bands, and performs envelope extraction and short-time segmentation into 386 ms regions. Each region consists of 30 frames. These approaches are motivated by what is know about which frequencies and modulation frequencies are most important for intelligibility. For each frequency band and frame (over the region of which it is the last frame), an intermediate correlation coefficient is calculated between the clean reference and processed power envelopes for each ear. These are averaged to obtain the MBSTOI index. Thus is usually between 0 and 1, and rises monotonically with measured intelligibility scores, such that higher values indicate greater speech intelligibility. Alignment is therefore required at the level of the one-third octave bands and short-time regions. Our baseline corrects for broadband delay per ear due to the hearing loss model. (The delay is measured by running a kronnecker delta function through the model for each ear.) However, the baseline software will not correct for delays created by your hearing aid processing. Consequently, when submitting your hearing aid output signals, you are responsible for correcting for any delays introduced by your hearing aid. Note that this must be done blindly; the clean reference signals will not be supplied for the test/evaluation set. "},{"title":"Level insensitivity​","type":1,"pageTitle":"Baseline speech intelligibility model in round one","url":"blog/baseline#level-insensitivity","content":"MBSTOI is level-independent, i.e., MBSTOI is broadly insensitive to the level of the processed signal because it is calculated using a cross-correlation method. This could be a problem because sounds that are below the auditory thresholds of the hearing impaired listener may appear to MBSTOI to be highly intelligible. To overcome this, the baseline experimental code mbstoi_beta, in conjunction with the baseline hearing loss model, can be used to approximate hearing-impaired auditory thresholds. Specifically, mbstoi_beta adds internal noise that can be used to approximate normal hearing auditory thresholds. This noise, in combination with the attenuation of signals by the hearing loss model to simulate raised auditory thresholds, makes MBSTOI level-sensitive. The noise is created by filtering white noise using pure tone threshold filter coefficients with one-third octave weighting, approximating the shape of a typical auditory filter (from Moore 2012, based on Patterson’s method, 1976). This noise is added to the processed signal. Note, the standard MBSTOI in the equalisation-cancellation stage adds internal noise to parameters, but this is an independent process. "},{"title":"MBSTOI​","type":1,"pageTitle":"Baseline speech intelligibility model in round one","url":"blog/baseline#mbstoi","content":"The method was developed by Asger Heidemann Andersen, Jan Mark de Haan, Zheng-Hua Tan and Jesper Jensen (Andersen et al., 2018). It builds on the Short-Time Objective Intelligibility (STOI) metric created by Cees H. Taal, Richard C. Hendriks, Richard Heusdens, and Jesper Jensen (Taal et al., 2011). MBSTOI includes a better ear stage and an equalisation-cancellation stage. For simplicity, the latter stage is not discussed here; see Andersen et al. (2018) for details. "},{"title":"References​","type":1,"pageTitle":"Baseline speech intelligibility model in round one","url":"blog/baseline#references","content":"Andersen, A. H., de Haan, J. M., Tan, Z. H., &amp; Jensen, J. (2018). Refinement and validation of the binaural short time objective intelligibility measure for spatially diverse conditions. Speech Communication, 102, 1-13.Moore, B. C. (2012). An introduction to the psychology of hearing. Brill.Patterson, R. D. (1976). Auditory filter shapes derived with noise stimuli. The Journal of the Acoustical Society of America, 59(3), 640-654.Taal, C. H., Hendriks, R. C., Heusdens, R., &amp; Jensen, J. (2011). An algorithm for intelligibility prediction of time–frequency weighted noisy speech. IEEE Transactions on Audio, Speech, and Language Processing, 19(7), 2125-2136. "},{"title":"CEC1 eval data released","type":0,"sectionRef":"#","url":"blog/CEC1 eval data released","content":"The evaluation dataset is now available to download from the myairbridge download site. The evaluation data filename is clarity_CEC1_data.scenes_eval.v1_1.tgz. Full details of how to prepare your submission are now available on this site. Please read them carefully. Registration: Teams must register via the Google form on the How To Submit page of this site. (Please complete this even if you have already completed a pre-registration form). Only one person from each team should register. Only those who have registered will be eligible to proceed to the evaluation. Once you have registered you will receive a confirmation email, a team ID and a link to a Google Drive to which you can upload your signals. Submission deadline: The deadline for submission is the 15th June. The submission consists of two components: i) a technical document of up to 2 pages describing the system/model and any external data and pre-existing tools, software and models used. This should be prepared as a Clarity-2021 workshop abstract and submitted to the workshop. ii) the set of processed signals that we will evaluate using the MBSTOI metric. Details of how to name and package your signals for upload can be found on the How To Submit page. Listening Tests: Teams that do well in the MBSTOI evaluation will be notified on 22nd June and invited to submit further signals for the second stage Listening Test evaluation. For any questions please contact us at claritychallengecontact@gmail.com or by posting to the Clarity challenge google group.","keywords":""},{"title":"CEC1 submissions received","type":0,"sectionRef":"#","url":"blog/CEC1 submissions received","content":"The CEC1 submission deadline has now passed. Thank you to all the teams who sent us signals. Please remember to submit your finalised system descriptions by June 22nd to the Clarity workshop following the instructions provided on the workshop website. We are currently busy evaluating the submissions using the MBSTOI metric. We will be contacting teams on the 22nd with details of how to prepare signals for the listening panel evaluation. If you have been working on the challenge but missed the submission deadline then please do get in contact. We will still be happy to receive your signals and system descriptions. Although late entries will not be eligible for the official challenge ranking, we will be happy to compute the eval set MBSTOI score for you and may even be able to arrange listening test evaluation through our panel. For any questions please contact us at claritychallengecontact@gmail.com or by posting to the Clarity challenge google group.","keywords":""},{"title":"CEC2 registration open","type":0,"sectionRef":"#","url":"blog/CEC2 registration open","content":"We are pleased to announce that registration for the 2nd Clarity Enhancement Challenge (CEC2) is now open. To register please complete the simple Google form found on the registration page. The remaining important dates for the challenge are as follows: 25th July 2022: Evaluation data released1st Sept 2022: 1st round submission deadline for evaluation by objective measure15th Sept 2022: 2nd round submission deadline for listening testsSept-Nov 2022: Listening test evaluation period.2nd Dec 2022: Results announced at a Clarity Challenge Workshop; prizes awarded. The challenge training, dev data and initial tools are now fully from the Github repository. If you have any questions please do not hesitate to contact us at claritychallengecontact@gmail.com.","keywords":""},{"title":"Clarity Challenge pre-announcement","type":0,"sectionRef":"#","url":"blog/Clarity Challenge pre-announcement","content":"","keywords":""},{"title":"The Task​","type":1,"pageTitle":"Clarity Challenge pre-announcement","url":"blog/Clarity Challenge pre-announcement#the-task","content":"You will be provided with simulated scenes, each including a target speaker and interfering noise. For each scene, there will be signals that simulate those captured by a behind-the-ear hearing aid with three channels at each ear and those captured at the eardrum without a hearing aid present. The target speech will be a short sentence and the interfering noise will be either speech or domestic appliance noise. The task will be to deliver a hearing aid signal processing algorithm that can improve the intelligibility of the target speaker for a specified hearing-impaired listener. Initially, entries will be evaluated using an objective speech intelligibility measure we will provide. Subsequently, up to twenty of the most promising systems will be evaluated by a panel of listeners. We will provide a baseline system so that teams can choose to focus on individual components or to develop their own complete pipelines. "},{"title":"What will be provided​","type":1,"pageTitle":"Clarity Challenge pre-announcement","url":"blog/Clarity Challenge pre-announcement#what-will-be-provided","content":"Evaluation of the best entries by a panel of hearing-impaired listeners.Speech + interferer scenes for training and evaluation.An entirely new database of 10,000 spoken sentencesListener characterisations including audiograms and speech-in-noise testing.Software including tools for generating training data, a baseline hearing aid algorithm, a baseline model of hearing impairment, and a binaural objective intelligibility measure. "},{"title":"Important Dates​","type":1,"pageTitle":"Clarity Challenge pre-announcement","url":"blog/Clarity Challenge pre-announcement#important-dates","content":"January 2021 – Challenge launch and release of software and dataApril 2021 – Evaluation data releasedMay 2021 – Submission deadlineJune-August 2021 – Listening test evaluation periodSeptember 2021 – Results announced at a Clarity Challenge Workshop in conjunction with Interspeech 2021 Challenge and workshop participants will be invited to contribute to a journal Special Issue on the topic of Machine Learning for Hearing Aid Processing that will be announced next year. "},{"title":"Further information​","type":1,"pageTitle":"Clarity Challenge pre-announcement","url":"blog/Clarity Challenge pre-announcement#further-information","content":"If you are interested in participating and wish to receive further information, please sign up. If you have questions, contact us directly at contact@claritychallenge.org "},{"title":"Organisers​","type":1,"pageTitle":"Clarity Challenge pre-announcement","url":"blog/Clarity Challenge pre-announcement#organisers","content":"Prof. Jon P. Barker, Department of Computer Science, University of SheffieldProf. Michael A. Akeroyd, Hearing Sciences, School of Medicine, University of NottinghamProf. Trevor J. Cox, Acoustics Research Centre, University of SalfordProf. John F. Culling, School of Psychology, Cardiff UniversityProf. Graham Naylor, Hearing Sciences, School of Medicine, University of NottinghamDr Simone Graetzer, Acoustics Research Centre, University of SalfordDr Rhoddy Viveros Muñoz, School of Psychology, Cardiff UniversityEszter Porter, Hearing Sciences, School of Medicine, University of Nottingham Funded by the Engineering and Physical Sciences Research Council (EPSRC), UK. Supported by RNID (formerly Action on Hearing Loss), Hearing Industry Research Consortium, Amazon TTS Research, Honda Research Institute Europe. "},{"title":"Acknowledgement​","type":1,"pageTitle":"Clarity Challenge pre-announcement","url":"blog/Clarity Challenge pre-announcement#acknowledgement","content":"The image copyright is owned by the University of Nottingham. "},{"title":"CPC1 results and prizes","type":0,"sectionRef":"#","url":"blog/CPC1 results and prizes","content":"The 1st Clarity Prediction Challenge is now complete. Thank you to all who took part! The full results can be found on the Clarity-2022 workshop website where you will also find links to system papers and the overview presentation. Many of the systems have led to successful Interspeech 2022 papers and will be contributing to the Interspeech 2022 special session on Speech Intelligibility Prediction for Hearing-Impaired Listeners. We hope to see many of you in Korea! In the meantime, please be sure to check out the onging 2nd Clarity Enhancement Challenge. The deadline for submitting enhanced signals is 1st September 2022, so there is still time to participate. To register a team please use the form here.","keywords":""},{"title":"ICASSP 2023 evaluation data released","type":0,"sectionRef":"#","url":"blog/ICASSP 2023 evaluation data released","content":"We are pleased to announce that the evaluation dataset for the ICASSP Clarity Challenge is now available for download. https://www.myairbridge.com/en/#!/folder/EkthOZZeBW33aaDBWSDadTgpOkbgaFxO For instructions on preparing your submission please visit: https://claritychallenge.org/docs/icassp2023/taking_part/icassp2023_submission If you have not yet registered it is not too late to do so. Please use the form at the link below and we will then send you a Team ID and a personalised upload link for your submission. https://claritychallenge.org/docs/icassp2023/taking_part/icassp2023_registration Note, we have extended the deadline for submission until Friday 10th February so that teams have a full week to process the signals. The remaining schedule is as follows, 2nd Feb 2023: Release of evaluation data.10th Feb 2023: Teams submit processed signals and technical reports.14th Feb 2023: Results released. Top 5 ranked teams invited to submit papers to ICASSP-202320th Feb 2023: Invited papers submitted to ICASSP-20234-9th June 2023: Overview paper and invited papers presented at dedicated ICASSP session","keywords":""},{"title":"Launch of CEC2","type":0,"sectionRef":"#","url":"blog/launch of CEC2","content":"We are pleased to announce the launch of the 2nd Clarity Enhancement Challenge (CEC2). The website has been fully updated to provide you with all the information you will need to participate in the challenge. The schedule for the challenge is as follows: 13th April 2022: Release of training and development data; initial tools.30th April 2022: Release of full toolset and baseline system.1st May 2022: Registration for challenge entrants opens.25th July 2022: Evaluation data released1st Sept 2022: 1st round submission deadline for evaluation by objective measure15th Sept 2022: 2nd round submission deadline for listening testsSept-Nov 2022: Listening test evaluation period.2nd Dec 2022: Results announced at a Clarity Challenge Workshop; prizes awarded. The challenge training, dev data and initial tools will be available from 13th April. In the meantime, please visit the CEC2 Intro page to learn more about the task. If you have any questions please do not hesitate to contact us at claritychallengecontact@gmail.com.","keywords":""},{"title":"Hearing loss simulation","type":0,"sectionRef":"#","url":"blog/Hearing loss simulation","content":"","keywords":""},{"title":"Audio examples of hearing loss​","type":1,"pageTitle":"Hearing loss simulation","url":"blog/Hearing loss simulation#audio-examples-of-hearing-loss","content":"Here are two samples of speech in noise processed through the simulator. In each audio example there are three versions of the same sentence: Unimpaired hearingMild hearing impairmentModerate to severe hearing impairment 0 dB signal to noise ratio Your browser does not support the audio element. And here is an example where the noise is louder: Your browser does not support the audio element. Noisier: -10dB signal to noise ratio "},{"title":"Acknowledgements​","type":1,"pageTitle":"Hearing loss simulation","url":"blog/Hearing loss simulation#acknowledgements","content":"The hearing loss model we’re using was generously supplied by Michael Stone at the University of Manchester as MATLAB code and translated by us into Python. The original code was written by members of the Auditory Perception Group at the University of Cambridge, ca. 1991-2013, including Michael Stone, Brian Moore, Brian Glasberg and Thomas Baer. Information about the model can be found primarily in Nejime and Moore (1997), but also in Nejime and Moore (1998), Baer and Moore (1993 and 1994), and Moore and Glasberg (1993). The original speech recordings come from the ARU corpus, University of Liverpool (Hopkins et al. 2019). This corpus is freely available at the link in the reference below. "},{"title":"References​","type":1,"pageTitle":"Hearing loss simulation","url":"blog/Hearing loss simulation#references","content":"Baer, T., &amp; Moore, B. C. (1993). Effects of spectral smearing on the intelligibility of sentences in noise. The Journal of the Acoustical Society of America, 94(3), 1229-1241.Baer, T., &amp; Moore, B. C. (1994). Effects of spectral smearing on the intelligibility of sentences in the presence of interfering speech. The Journal of the Acoustical Society of America, 95(4), 2277-2280.Hopkins, C., Graetzer, S., &amp; Seiffert, G. (2019). ARU adult British English speaker corpus of IEEE sentences (ARU speech corpus) version 1.0 [data collection]. Acoustics Research Unit, School of Architecture, University of Liverpool, United Kingdom. DOI: 10.17638/datacat.liverpool.ac.uk/681. Retrieved from http://datacat.liverpool.ac.uk/681/.Moore, B. C., &amp; Glasberg, B. R. (1993). Simulation of the effects of loudness recruitment and threshold elevation on the intelligibility of speech in quiet and in a background of speech. The Journal of the Acoustical Society of America, 94(4), 2050-2062.Moore, B. C., Glasberg, B. R., &amp; Vickers, D. A. (1996). Factors influencing loudness perception in people with cochlear hearing loss. B. Kollmeier, World Scientific, Singapore, 7-18.Nejime, Y., &amp; Moore, B. C. (1997). Simulation of the effect of threshold elevation and loudness recruitment combined with reduced frequency selectivity on the intelligibility of speech in noise. The Journal of the Acoustical Society of America, 102(1), 603-615.Nejime, Y., &amp; Moore, B. C. (1998). Evaluation of the effect of speech-rate slowing on speech intelligibility in noise using a simulation of cochlear hearing loss. The Journal of the Acoustical Society of America, 103(1), 572-576. "},{"title":"Latency, computation time and real-time operation","type":0,"sectionRef":"#","url":"blog/Latency, computation time and real-time operation","content":"","keywords":""},{"title":"The 1st Clarity Enhancement Challenge​","type":1,"pageTitle":"Latency, computation time and real-time operation","url":"blog/Latency, computation time and real-time operation#the-1st-clarity-enhancement-challenge","content":"For a hearing aid to work well for users, the processing needs to be quick. The output of the hearing aid should be produced with a delay of less than about 10 ms. Many audio processing techniques are non-causal, i.e., the output of the system depends on samples from the future. Such processing is useless for hearing aids and therefore our rules include a restriction on the use of future samples. The rules state the following: Systems must be causal; the output at time t must not use any information from input samples more than 5 ms into the future (i.e., no information from input samples &gt;t+5ms).There is no limit on computational cost. Mathematically this is: yn=f(xm , xm+1 ... xn+N–1 , xn+N , L ) where yn is the output from your hearing aid for sample n.x is the audio input signal from a hearing aid microphone.N = 0.005 fs where fs is the sampling frequency.m is a sample number where m &lt;= n.L is the listener characteristics.f() is the hearing aid function. There is no limitation on how long this takes to compute.You can use multiple microphones; only a single input signal x is shown here just for simplicity. Here it is illustrated as a diagram.  Figure. Example of how the limit of 5 ms is applied to a hearing aid input and output signal. We have a chosen a limit of 5 ms because in a real hearing aid there will be other sources of delay (e.g., analogue-to-digital, digital-to-analogue conversion). "},{"title":"Why is there no limitation of how long f() takes to compute?​","type":1,"pageTitle":"Latency, computation time and real-time operation","url":"blog/Latency, computation time and real-time operation#why-is-there-no-limitation-of-how-long-f-takes-to-compute","content":"We’re trying to foster new approaches to hearing aid processing and decided that at this stage we will drive more innovation if we don’t restrict computation time for round one. Such restrictions will be considered in future rounds. "},{"title":"Why haven’t you talked about latency?​","type":1,"pageTitle":"Latency, computation time and real-time operation","url":"blog/Latency, computation time and real-time operation#why-havent-you-talked-about-latency","content":"In discussions, it is apparent that this term is used in different ways by different people, so to avoid confusion we’re not using it! "},{"title":"Do algorithms have to be real-time?​","type":1,"pageTitle":"Latency, computation time and real-time operation","url":"blog/Latency, computation time and real-time operation#do-algorithms-have-to-be-real-time","content":"The above limitations mean that the algorithms could in theory be made real-time if a powerful enough computer was available, but your entry can take as long as it needs to process the signals. "},{"title":"Live events in January","type":0,"sectionRef":"#","url":"blog/Jan-2-live-events","content":"","keywords":""},{"title":"Webinar - Challenge Overview​","type":1,"pageTitle":"Live events in January","url":"blog/Jan-2-live-events#webinar---challenge-overview","content":""},{"title":"Friday 14th January​","type":1,"pageTitle":"Live events in January","url":"blog/Jan-2-live-events#friday-14th-january","content":"9:00 GMT | 17:00 CST (GMT+8) "},{"title":"Click here to join the webinar​","type":1,"pageTitle":"Live events in January","url":"blog/Jan-2-live-events#click-here-to-join-the-webinar","content":"An introduction to the aims of the challenge and some background to the problem of speech intelligibility prediction for hearing aids: Welcome, introduction to Clarity.Speech intelligibility models: Overview and why are they needed.Hearing impairment speech intelligibility prediction.The prediction challenge - details and how you can sign up to participate.Audience questions / discussion. The presentations will be recorded and made available online shortly after the event. The Q&amp;A discussion will not be recorded. You are welcome to join slightly later if you are only interested in joining for the Q&amp;A section (presentations should finish around 9:40 GMT). "},{"title":"Live Q&A session​","type":1,"pageTitle":"Live events in January","url":"blog/Jan-2-live-events#live-qa-session","content":""},{"title":"Monday 17th January​","type":1,"pageTitle":"Live events in January","url":"blog/Jan-2-live-events#monday-17th-january","content":"17:00 GMT | 12:00 EST (GMT-5) | 9:00 PST (GMT-8) "},{"title":"Click here to join the Q&A​","type":1,"pageTitle":"Live events in January","url":"blog/Jan-2-live-events#click-here-to-join-the-qa","content":"A chance to ask the team questions about the Clarity Prediction Challenge - for anyone that could not attend the webinar on Friday 14th due to time zone differences. Please note there will be no presentations in this session. The talks from Friday’s webinar will be uploaded to the Clarity project YouTube channel later in the day so you are invited to watch those before joining this live Q&amp;A. "},{"title":"Release of CEC2 baseline","type":0,"sectionRef":"#","url":"blog/release of CEC2 baseline","content":"We are pleased to announce the release of the 2nd Clarity Enhancement Challenge (CEC2) baseline system code. The baseline code has been released in the latest commit to the Clarity GitHub repository. The baseline system perform NAL-R amplification according to the audiogram of the target listener, followed by a simple gain control and output of the signals to 16-bit stereo wav format. The system has been kept deliberately simple with no microphone array processing or attempt at noise cancellation. HASPI scores for the dev set have been measured. The scores are as follows. System\tHASPIUnprocessed\t0.1615 NAL-R baseline\t0.2493 See here for further details. If you have any problems using the baseline code please do not hesitate to contact us at claritychallengecontact@gmail.com, or post questions on the Google group.","keywords":""},{"title":"One approach to our enhancement challenge","type":0,"sectionRef":"#","url":"blog/One approach to our enhancement challenge","content":"","keywords":""},{"title":"References​","type":1,"pageTitle":"One approach to our enhancement challenge","url":"blog/One approach to our enhancement challenge#references","content":"[1] Andersen, A.H., Haan, J.M.D., Tan, Z.H. and Jensen, J., 2015. A binaural short time objective intelligibility measure for noisy and enhanced speech. In the Sixteenth Annual Conference of the International Speech Communication Association.[2] Li, H., Fu, S.W., Tsao, Y. and Yamagishi, J., 2020. iMetricGAN: Intelligibility Enhancement for Speech-in-Noise using Generative Adversarial Network-based Metric Learning. arXiv preprint arXiv:2004.00932.[3] Gillhofer, M., Ramsauer, H., Brandstetter, J., Schäfl, B. and Hochreiter, S., 2019. A GAN based solver of black-box inverse problems. Proceedings of the NeurIPS 2019 Workshop.[4] Kawanaka, M., Koizumi, Y., Miyazaki, R. and Yatabe, K., 2020, May. Stable training of DNN for speech enhancement based on perceptually-motivated black-box cost function. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 7524-7528). IEEE. "},{"title":"Sounds for round one","type":0,"sectionRef":"#","url":"blog/Sounds","content":"","keywords":""},{"title":"Everyday background noises that interfere with understanding of speech​","type":1,"pageTitle":"Sounds for round one","url":"blog/Sounds#everyday-background-noises-that-interfere-with-understanding-of-speech","content":"A long and varied list of sounds cause problems. These lists are in no particular order. Living room or spaceClocks tickingCrisp packets rustlingTaps runningKettles boilingDishwasherMicrowaveWashing machineTV, music, radioPhone ringing (or receiving texts – unknown beeps/tones)Newspapers rustlingAir-conditioning and oven extractor fansVacuum cleanerDoorbell ringingDog barkingRain on window "},{"title":"Family and friends​","type":1,"pageTitle":"Sounds for round one","url":"blog/Sounds#family-and-friends","content":"Cutlery/crockery banging/clangingDoors opening/closing (to rooms and cupboards)MusicPeople walking around the roomChildren playing with toysLaughingPeople talking from another roomSpeakers from a different conversation in close proximity (i.e. beside you) when you are trying to converseTraffic outsideChewing/chompingSteam pipes/ coffee machinesChairs being moved "},{"title":"Outside​","type":1,"pageTitle":"Sounds for round one","url":"blog/Sounds#outside","content":"Church bellsMarket noiseFootsteps on different types of ground, i.e. heels on hard floors but also wellingtons in mudClothes rustling (such as waterproof coats or hat on hearing aid)Wind (even with HA on ‘wind setting’)Pigeons/birdsSirensTraffic noise (especially at junctions)MusicLaughterPhones ringingTillsChildren playing outside or running around (in shops, on the street and at parks)Beeping signal at crossingsGarden centres – high glass ceilings, open plan, trolleysRoad/ tyre and traffic noise when in a car or on the busAlso mentioned how people you speak to in the car may be in front or behind youTrains and the tubeAeroplanes and airports (suitcases rolling)Tannoys "},{"title":"Characteristics of processed speech to consider​","type":1,"pageTitle":"Sounds for round one","url":"blog/Sounds#characteristics-of-processed-speech-to-consider","content":"Clarity (clearness) or qualityRhythm of speech‘Inflection’ (intonation)Similarity to original speakerAgreed that in situations where the voice would not be processed clearly, i.e. outside with many noise sources, not sounding like the original - speaker is fine. "},{"title":"Other comments​","type":1,"pageTitle":"Sounds for round one","url":"blog/Sounds#other-comments","content":"Speed of speech; it was suggested that we have sentences read at different speeds as faster talkers are often harder to understand.Stated that emphasis on key words is useful for following conversation; perhaps key words in the sentence when marked should be given higher value.Lots of comments on room acoustics, i.e., ceiling heights, furnishings, floorings, windows etc., which has a big impact on how difficult it is to have a conversation with background noise.Different accents of talkers can make conversation more difficult; including speakers with different accents in the background.We’re now working out what sounds to use. But are there other sounds we should consider? "},{"title":"Credits​","type":1,"pageTitle":"Sounds for round one","url":"blog/Sounds#credits","content":"Thank you to the patient and public involvement representatives who participated.Clarity Organiser: Eszter Porter .Facilitators: Adele Horobin, Erin Dawe-Lane.This discussion group was supported by the National Institute for Health Research Nottingham Biomedical Research Centre. "},{"title":"The baseline","type":0,"sectionRef":"#","url":"blog/The baseline","content":"An overview of the current state of the baseline we’re developing for the machine learning challenges We’re currently developing the baseline processing that challenge entrants will need. This takes a random listener and a random audio sample of speech in noise (SPIN) and passes that through a simulated hearing aid (the Enhancement Model). This improves the speech in noise. We then have an algorithm (the Prediction Model) to estimate the Speech Intelligibility that the listener would perceive (SI score). This score can then be used to drive machine learning to improve the hearing aid. A talk through the baseline model we’re developing. The first machine learning challenge is to improve the enhancement model, in other words, to produce a better processing algorithm for the hearing aid. The second challenge is to improve the prediction model using perceptual data we’ll provide.","keywords":""},{"title":"Welcome","type":0,"sectionRef":"#","url":"blog/welcome","content":"Welcome to the new Clarity blog. We will be using this blog to post regular updates about our Challenges and Workshop, as well as posts discussing the tools and techniques that we are using in our baseline systems.","keywords":""},{"title":"Welcome to CPC1","type":0,"sectionRef":"#","url":"blog/welcome to CPC1","content":"Welcome to the new Clarity CPC1 site for the first prediction challenge launching in autumn 2021. Feel free to look around. At the moment we're still doing listening tests and preparing the data, so the download links don't work. If anything is unclear or you've got questions, please contact us through the Google group.","keywords":""},{"title":"Why use machine learning challenges for hearing aids?","type":0,"sectionRef":"#","url":"blog/Why use machine learning challenges for hearing aids","content":"","keywords":""},{"title":"Components of a challenge​","type":1,"pageTitle":"Why use machine learning challenges for hearing aids?","url":"blog/Why use machine learning challenges for hearing aids#components-of-a-challenge","content":"There needs to be a common task based on a target application scenario to allow communities to gain from benchmarking and collaboration. Clarity project’s first enhancement challenge will be about hearing speech from a single talker in a typical living room, where there is one source of noise and a little reverberation. We’re currently working on developing simulation tools to allow us to generate our living room data. The room acoustic will be simulated using RAVEN and the Hearing Device Head-related Transfer Functions will come from Denk’s work. We’re working on getting better, more ecologically valid speech than is often used in speech intelligibility work.  Entrants are then given training data and development (dev) test data along with a baseline system that represents the current state-of-the-art. You can find a post and video on the current thinking on the baseline here. We’re still working on the rules stipulating what is and what is not allowed (for example, will entrants be allowed to use data from outside the challenge). Clarity’s first enhancement challenge is focussed on maximising the speech intelligibility (SI) score. We will evaluate this first through a prediciton model that is based on a hearing loss simulation and an objective metric for speech intellibility. Simulation has been hugely important for generating training data in the CHIME challenges and so we intend to use that approach in Clarity. But results from simulated test sets cannot be trusted and hence a second evaluation will come through perceptual tests on hearing impaired subjects. However, one of our current problems is that we can’t bring listeners into our labs because of COVID-19. We’ll actually be running two challenges in roughly parallel, because we’re also going to task the community to improve our prediction model for speech intelligibility. We’re running a series of challenges over five years. What other scenarios should we consider? What speech? What noise? What environment? Please comment below. "},{"title":"Acknowledgements​","type":1,"pageTitle":"Why use machine learning challenges for hearing aids?","url":"blog/Why use machine learning challenges for hearing aids#acknowledgements","content":"Much of this text is based on Jon Barker’s 2020 SPIN keynote "},{"title":"The speech-in-noise problem","type":0,"sectionRef":"#","url":"blog/The speech-in-noise problem","content":"","keywords":""},{"title":"References​","type":1,"pageTitle":"The speech-in-noise problem","url":"blog/The speech-in-noise problem#references","content":"Akeroyd, M. A. (2008). Are individual differences in speech reception related to individual differences in cognitive ability? A survey of twenty experimental studies with normal and hearing-impaired adults. International Journal of Audiology, 47(sup2), S53-S71.Cherry, E. C. (1953). Some experiments on the recognition of speech, with one and with two ears. The Journal of the Acoustical Society of America, 25(5), 975-979.Heinrich, A., Henshaw, H., and Ferguson, M. A. (2015). The relationship of speech intelligibility with hearing sensitivity, cognition, and perceived hearing difficulties varies for different speech perception tests. Frontiers in Psychology, 6, 782.Vestergaard Knudsen, L., Öberg, M., Nielsen, C., Naylor, G., and Kramer, S. E. (2010). Factors influencing help seeking, hearing aid uptake, hearing aid use and satisfaction with hearing aids: A review of the literature. Trends in Amplification, 14(3), 127-154.Kochkin, S. (2000). MarkeTrak V: “Why my hearing aids are in the drawer” The consumers’ perspective. The Hearing Journal, 53(2), 34-36. "},{"title":"Credits​","type":1,"pageTitle":"The speech-in-noise problem","url":"blog/The speech-in-noise problem#credits","content":"Photo of Cocktail party by Ross CC BY-NC-SA 2.0Ronan, N., &amp; Barrett, G. (2014). A 68 year old woman with deteriorating hearing. BMJ, 348, g2984. https://www.bmj.com/content/348/bmj.g2984 "},{"title":"Introduction Webinar - Recording Available","type":0,"sectionRef":"#","url":"blog/webinar-1-link","content":"The Clarity team recently hosted a webinar to introduce the Prediction Challenge. The recording is now available to view online: Slides The slides are available to download: 1 Welcome and Overview 2 Speech Intelligibility Models 3 Hearing Impariment and SI Prediction 4 Clarity Prediction Challenge Details Note that we did not record the Q&amp;A session at the end, but if you have questions about taking part in the challenge you can contact us at claritychallengecontact@gmail.com","keywords":""},{"title":"The speech-in-noise problem part two","type":0,"sectionRef":"#","url":"blog/The speech-in-noise problem part two","content":"","keywords":""},{"title":"Machine learning​","type":1,"pageTitle":"The speech-in-noise problem part two","url":"blog/The speech-in-noise problem part two#machine-learning","content":"In recent years, there has been increasing interest in what machine learning methods can do for hearing aids. Machine learning is a branch of artificial intelligence where computers learn directly from example data. One machine learning method is the neural network. This is an algorithm formed from layers of simple computational units connected to each other in a way that is inspired by connections between neurons in the brain. Deep (3+ layer) neural networks are able to learn complex, non-linear mapping functions, which makes them ideal candidates for noise reduction tasks. We anticipate that machine learning can help tackle the challenge of speech in noise for hearing aids, providing a tailored solution for each individual and listening situation. For example, one thing machine learning could do is to sense the acoustic environment the listener is in, and choose the most suitable processing settings.  Image via www.vpnsrus.com In recent years, a machine learning approach for noise reduction has become popular. Neural networks are used to estimate time-frequency masks (a set of gains for each time-frequency unit that, when multiplied by the signal, produce less noisy speech; see, e.g., Zhao et al., 2018). Machine learning systems for noise reduction are trained on artificially mixed speech and noise. Some operate on a single channel, i.e., using spectral cues, and some work with multiple channels using spatial cues. We expect that future hearing aids built on machine learning will perform best if they combine the left and right microphones to work binaurally. Most of these noise reduction systems have been designed and evaluated in an off-line mode where they process pre-recorded signals. This isn’t much use for hearing aids that need to work in real-time with low latency (i.e., short delays). One challenge for hearing aids is to redesign off-line approaches to work quickly enough without too much loss of performance. The potential for machine learning to produce better approaches to hearing aid processing is what motivated the Clarity Project. If you’re interested in hearing more as the challenges develop, please sign up. "},{"title":"References​","type":1,"pageTitle":"The speech-in-noise problem part two","url":"blog/The speech-in-noise problem part two#references","content":"Brons, I., Houben, R., and Dreschler, W. A. (2014). Effects of noise reduction on speech intelligibility, perceived listening effort, and personal preference in hearing-impaired listeners. Trends in hearing, 18, 1-10.Van den Bogaert, T., Doclo, S., Wouters, J., and Moonen, M. (2009). Speech enhancement with multichannel Wiener filter techniques in multimicrophone binaural hearing aids. The Journal of the Acoustical Society of America, 125(1), 360-371.Zhao, Y., Wang, D., Johnson, E. M., and Healy, E. W. (2018). A deep learning based segregation algorithm to increase speech intelligibility for hearing-impaired listeners in reverberant-noisy conditions. The Journal of the Acoustical Society of America, 144(3), 1627-1637. "},{"title":"Credits​","type":1,"pageTitle":"The speech-in-noise problem part two","url":"blog/The speech-in-noise problem part two#credits","content":"Photograph of hearing aid wearer, copyright University of Nottingham. Image of brain with overlaid circuity made available by www.vpnsrus.com. "},{"title":"Download","type":0,"sectionRef":"#","url":"docs/cec1/cec1_download","content":"Download The baseline code is available on Github. The github repository includes instructions for obtaining the data.","keywords":""},{"title":"Baseline System","type":0,"sectionRef":"#","url":"docs/cec1/cec1_baseline","content":"Baseline System Figure 1 shows a simplified schematic of the baseline system. For simplicity, not all signal paths are shown. A scene generator (blue box) creates the speech in noise (SPIN) that the hearing aid model then enhances (yellow box). This enhancement is individualised for each listener, hence there is also a system to select a random listener (white ellipse) with a particular set of pure tone air-conduction thresholds or audiograms. The speech in noise that has been improved by the hearing aid is then passed to the prediction stage (orange box). This includes: (i) a simulation of hearing loss and (ii) a binaural model of intelligibility that estimates the speech intelligibility. Figure 1 Simplified overview of the baseline. As stated in the rules of the first Enhancement Challenge, you are free to choose which parts of the baseline are useful to your approach, and reconfigure the system as you feel fit. More details of the different parts of the baseline appear on the software page, see, Scene GeneratorHearing aid modelHearing loss modelSpeech intelligibility model Download baseline software and data.","keywords":""},{"title":"The 1st Clarity Enhancement Challenge","type":0,"sectionRef":"#","url":"docs/cec1/cec1_intro","content":"The 1st Clarity Enhancement Challenge info The 1st Clarity Enhancement Challenge has now finished. For the details of the systems that were submitted and to see the table of results, please visit the Clarity-2021 Workshsop website. For details on our current challenge see here. Scenario - a description of the listening scenario and how it has been simulated. Baseline System - a description of the baseline hearing aid model. CEC1 Data - the data that can be used to train and evaluate your system during development. CEC1 Software - the software tools that we are providing to help you build and evaluate a challenge entry. Challenge Rules - the rules to which all challenge entries must adhere. Listening Tests - information about the listening tests which will be used to evaluate the best systems. Submission - information about how to prepare your submission. Prizes - information about our prizes. Download - where to go to download the software and challenge data.","keywords":""},{"title":"Prizes","type":0,"sectionRef":"#","url":"docs/cec1/cec1_prizes","content":"","keywords":""},{"title":"The Team Prize​","type":1,"pageTitle":"Prizes","url":"docs/cec1/cec1_prizes#the-team-prize","content":"Team prizes have been made available by the generosity of the Hearing Industry Research ConsortiumThere will be separate MBSTOI and listening test prizes for the top systems.  MBSTOI prize emoji_events 1st Place $1000 emoji_events 2nd Place $500 emoji_events 3rd Place $250 Listening Test prize emoji_events 1st Place $1000 emoji_events 2nd Place $500 emoji_events 3rd Place $250 "},{"title":"The Amazon Student Prize​","type":1,"pageTitle":"Prizes","url":"docs/cec1/cec1_prizes#the-amazon-student-prize","content":"Student prizes have been made available by the generosity of Amazon TTS Research  Amazon prize for top student contribution emoji_events 1st Place $1000 emoji_events 2nd Place $500 emoji_events 3rd Place $250  - The award will be judged by a panel formed from members of the Clarity-2021 workshop scientific committee. - There will be a lightweight nomination process. Details to be announced. info Anonymous entries and those with direct links to the Clarity project team are ineligible for cash prizes, sorry. "},{"title":"Listening Tests","type":0,"sectionRef":"#","url":"docs/cec1/cec1_listening_tests","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"Listening Tests","url":"docs/cec1/cec1_listening_tests#overview","content":"Our panel members will be provided with a tablet preloaded with our Listen@Home software and headphones to complete the listening experiment. They will listen to an entrant’s sentence, respond verbally with what they think was said by the target talker, and then move on to the next sentence. Their response will be recorded by the tablet microphone(s) and then processed using automatic speech recognition. Intelligibility will be evaluated as the number of words identified correctly in the sentence. Our plan is that each listener will evaluate 1,200 sentences, which is about 4 hours of listening, and that every listener will evaluate sentences from every entrant. We will use a combinatorial design to equate this as far as possible. Should a listener drop out from the panel, we will endeavour to replace them with someone with a similar hearing loss, but should that prove impractical we will reduce the size of the panel, and inform entrants which listener has withdrawn. "},{"title":"Listen@Home hardware​","type":1,"pageTitle":"Listening Tests","url":"docs/cec1/cec1_listening_tests#listenhome-hardware","content":"We will be using a Lenovo 10e Chromebook running Android 81.0 and Sennheiser PC-8 headsets to play the sounds to our participants. We will allow participants to set the volume so that the sounds are not so loud to be uncomfortable. Without loudness-recruitment measures for our listeners, we cannot be sure just what loudnesses every participant will hear, so we need to allow them to make the choice here. We have measurements on the output capability of a system in the laboratory: A 1 kHz pure tone set to be the most powerful it can be (i.e., an amplitude range of +/-1 = RMS amplitude of 0.707, and the volume controls at 100%) gave 99 dB(A) SPL on the PC-8 headphones.An ICRA speech-shaped noise [1], unmodulated in time, and scaled to an RMS of 0.3, gave 90 dB(A) at the same volume level. With this RMS, the noise had 0.1% of its samples clipped at +/- 1. It is important to note that there is a convention for the prediction model that a +/-1 square wave has RMS = 0 dB FS and corresponds to 120 dB, while for listening tests, 0 dB FS corresponds to approximately 100 dB, given the above capabilities of the reproduction equipment. For the listening tests, we will require the signals to be provided as 16-bit WAV files with a 32 kHz sampling rate (see this page). We will play the signals as is using a HTML/PHP audio player coded on a webpage. The responsibility for the final signal level is therefore yours. It’s worth bearing in mind that should your signals overall seem too loud to be comfortable to a participant, they may well turn down the volume themselves.  "},{"title":"References​","type":1,"pageTitle":"Listening Tests","url":"docs/cec1/cec1_listening_tests#references","content":"[1] ICRA standard noises, https://icra-audiology.org/Repository/icra-noise. We used track #1. "},{"title":"CEC2 Schedule","type":0,"sectionRef":"#","url":"docs/cec2/cec2_dates","content":"CEC2 Schedule Key dates are as follows 30th March 2022: Challenge website launch 14th April 2022: Release of training and development data, plus core software. 30th April 2022: Release of full toolset and baseline system. 1st May 2022: Registration for challenge entrants opens. 25th July 2022: Evaluation data released 1st Sept 2022: 1st round submission deadline for evaluation by objective measure 15th Sept 2022: 2nd round submission deadline for listening tests Sept-Nov 2022: Listening test evaluation period. 12th Dec 2022: Results announced at a Clarity Challenge Workshop; prizes awarded. Workshop likely to be a one-day virtual event","keywords":""},{"title":"Rules","type":0,"sectionRef":"#","url":"docs/cec1/cec1_rules","content":"","keywords":""},{"title":"Teams​","type":1,"pageTitle":"Rules","url":"docs/cec1/cec1_rules#teams","content":"Teams must have pre-registered and nominated a contact person.Teams can be from one or more institutions.The organisers may enter the challenge themselves but will not be eligible to win the cash prizes. "},{"title":"Transparency​","type":1,"pageTitle":"Rules","url":"docs/cec1/cec1_rules#transparency","content":"Teams must provide a technical document of up to 2 pages describing the system/model and any external data and pre-existing tools, software and models used.We will publish all technical documents (anonymous or otherwise).Teams are encouraged – but not required – to provide us with access to the system/model and to make their code open source.Anonymous entries are allowed but will not be eligible for cash prizes.All teams will be referred to using anonymous codenames in rank ordering. "},{"title":"Intellectual property​","type":1,"pageTitle":"Rules","url":"docs/cec1/cec1_rules#intellectual-property","content":"The following terms apply to participation in this machine learning challenge (“Challenge”). Entrants may create original solutions, prototypes, datasets, scripts, or other content, materials, discoveries or inventions (a “Submission”). The Challenge is organised by the Challenge Organiser. Entrants retain ownership of all intellectual and industrial property rights (including moral rights) in and to Submissions. As a condition of submission, Entrant grants the Challenge Organiser, its subsidiaries, agents and partner companies, a perpetual, irrevocable, worldwide, royalty-free, and non-exclusive license to use, reproduce, adapt, modify, publish, distribute, publicly perform, create a derivative work from, and publicly display the Submission. Entrants provide Submissions on an “AS IS” BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. "},{"title":"What information can I use?​","type":1,"pageTitle":"Rules","url":"docs/cec1/cec1_rules#what-information-can-i-use","content":""},{"title":"Training and development​","type":1,"pageTitle":"Rules","url":"docs/cec1/cec1_rules#training-and-development","content":"There is no limit on the amount of training data that can be generated using our tools. Teams can also use their own data for training or expand the training data through simple automated modifications. However, teams that do this must make a second submission using only the official audio files and signal generation tool. Any audio or metadata can be used during training and development, but during evaluation the proposed simulated hearing aid or Enhancement Processor will not have access to all of the data (see next section). "},{"title":"Evaluation​","type":1,"pageTitle":"Rules","url":"docs/cec1/cec1_rules#evaluation","content":"The only data that can be used by the Enhancement Processor during evaluation are The audio input signals (the sum of the target and interferer for each hearing aid microphone), andThe listener characterisation (pure tone air-conduction audiograms). "},{"title":"Computational restrictions​","type":1,"pageTitle":"Rules","url":"docs/cec1/cec1_rules#computational-restrictions","content":"Teams may choose to use all or some of the provided baseline models.Systems must be causal; the output at time t must not use any information from input samples more than 5 ms into the future (i.e., no information from input samples &gt;t+5 ms).There is no limit on computational cost. Please see this blog post for further explanation of these last two rules about latency and computation time. "},{"title":"Submitting multiple entries​","type":1,"pageTitle":"Rules","url":"docs/cec1/cec1_rules#submitting-multiple-entries","content":"If you wish to submit two entries, where one is optimised for MBSTOI and the other, for listening tests, Both systems must be submitted for MBSTOI evaluation.You must register two teams, submitting each entry as a different team.In your documentation, you must make it clear which has been optimised for listening tests and the relationship between the two entries. We will assume that if only one of these systems is to go forward to listening tests, your preference is to use the one optimised for listening tests. "},{"title":"Evaluation of systems​","type":1,"pageTitle":"Rules","url":"docs/cec1/cec1_rules#evaluation-of-systems","content":""},{"title":"Stage 1: Objective evaluation​","type":1,"pageTitle":"Rules","url":"docs/cec1/cec1_rules#stage-1-objective-evaluation","content":"Entries will be ranked according to average Modified Binaural Short-Time Objective Intelligibility (MBSTOI) score across all samples in the evaluation/test dataset (i.e., all signals submitted for the MBSTOI evaluation). "},{"title":"Stage 2: Listening test evaluation​","type":1,"pageTitle":"Rules","url":"docs/cec1/cec1_rules#stage-2-listening-test-evaluation","content":"There is a limit on how many systems can be evaluated by the listener panel.A maximum of two entries can go through to the listener panel from any individual entrant. Furthermore, a second will only be allowed if it is judged by us to use significantly different signal processing approaches.We will choose which will go to the listener panel based on The top N scored using the objective evaluation method.A sample of M others that use contrasting and promising approaches. "},{"title":"CEC1 Data","type":0,"sectionRef":"#","url":"docs/cec1/cec1_data","content":"","keywords":""},{"title":"A. Training, development, evaluation data​","type":1,"pageTitle":"CEC1 Data","url":"docs/cec1/cec1_data#a-training-development-evaluation-data","content":"The dataset is split into these three subsets: training (train), development (dev) and evaluation (eval). You should only train on the training set.The system submitted should be chosen on the evidence provided by the development set.The final listening and ranking will be performed with the (held-out) evaluation set.For more information on supplementing the training data, please see the rules. The evaluation dataset will be made available one month before the challenge submission deadline. "},{"title":"B. The scene dataset​","type":1,"pageTitle":"CEC1 Data","url":"docs/cec1/cec1_data#b-the-scene-dataset","content":"The complete dataset is composed of 10,000 scenes split into the following sets: Training (6000 scenes, 24 speakers);Development (2500 scenes, 10 speakers);Evaluation (1500 scenes, 6 speakers). Each scene corresponds to a unique target utterance and a unique segment of noise from an interferer. The training, development and evaluation sets are disjoint for target speaker. The three sets are balanced for target speaker gender. Binaural Room Impulse Responses (BRIRs) are used to model how the sound is altered as it propagates through the room and interacts with the head. The audio signals for the scenes are generated by convolving source signals with the BRIRs and summing. See the page on modelling the scenario for more details. Randomised room dimensions, target and interferer locations are used. The BRIRs are generated for: A hearing aid with 3 microphone inputs (front, mid, rear). The hearing aid has a Behind-The-Ear (BTE) form factor; see Figure 1. The distance between microphones is approx. 7.6 mm. The properties of the tube and ear mould are not considered.Close to the eardrum.The anechoic target reference (front microphone).  Figure 1. Front (Fr), Middle (Mid) and Rear microphones on a BTE hearing aid form. Head Related Impulse Responses (HRIRs) are used to model how sound is altered as it propagates in a free-field and interacts with the head (i.e., no room is included). These are taken from the OlHeadHRTF database with permission. These include HRIRs for human heads and for three types of head-and-torso simulator/mannekin. The eardrum HRIRs (labelled ED) are for a position close to the eardrum of the open ear. rpf files are specification files for the geometric room acoustic model that include a complete description of the room. "},{"title":"B.1 Training data​","type":1,"pageTitle":"CEC1 Data","url":"docs/cec1/cec1_data#b1-training-data","content":"For each scene in the training data the following signals and metadata are available: The target and interferer BRIRs (4 pairs: front, mid, rear and eardrum for left and right ears).HRIRs including those corresponding to the target azimuth.The mono target and interferer signals (pre-convolution).For each hearing aid microphone (channels 1-3 where channel 1 is front, channel 2 is mid and channel 3 is rear) and a position close to the eardrum (channel 0): The target convolved with the appropriate BRIR;The interferer convolved with the appropriate BRIR;The sum of the target and interferer convolved. The target convolved with the anechoic BRIR (channel 1) for each ear (‘target_anechoic’).Metadata describing the scene: a JSON file containing, e.g., the filenames of the sources, the location of the sources, the viewvector of the target source, the location and viewvector of the receiver, the room dimensions (see specification below), and the room number, which corresponds to the RAVEN BRIR, rpf and ac files. Software for generating more training data is also available. "},{"title":"B.2 Development data​","type":1,"pageTitle":"CEC1 Data","url":"docs/cec1/cec1_data#b2-development-data","content":"The same data as for the training will be made available to allow you to fully examine the performance of your system. Note, that the data available for the evaluation will be much more limited (see B.3). For each scene, during development, your hearing aid enhancement model must only use the following input signals/data: The sum of the target and interferer – mixed at the SNR specified in the scene metadata – at one or more hearing aid microphones (CH1, CH2 and/or CH3).The IDs of the listeners assigned to the scene in the metadata provided.The audiograms of these listeners. "},{"title":"B.3 Evaluation scene data​","type":1,"pageTitle":"CEC1 Data","url":"docs/cec1/cec1_data#b3-evaluation-scene-data","content":"For each scene in the evaluation data only the following will be available: The sum of the target and interferer for each hearing aid microphone.The ID of the evaluation panel members/listeners who will be listening to the processed scene.The audiograms of these listeners. "},{"title":"C Listener data​","type":1,"pageTitle":"CEC1 Data","url":"docs/cec1/cec1_data#c-listener-data","content":""},{"title":"C.1 Training and development data​","type":1,"pageTitle":"CEC1 Data","url":"docs/cec1/cec1_data#c1-training-and-development-data","content":"A sample of pure tone air-conduction audiograms that characterise the hearing impairment of potential listeners, split into training and development sets. "},{"title":"C.2 Evaluation data​","type":1,"pageTitle":"CEC1 Data","url":"docs/cec1/cec1_data#c2-evaluation-data","content":"You will be given the left and right pure tone air-conduction audiograms for the listening panel, so the signals you generate for evaluation can be individualised to the listeners. A panel of 50 hearing-aided listeners will be recruited for the evaluation panel. We plan that they will be experienced bilateral hearing-aid users (they use two hearing aids but the hearing loss may be asymmetrical) with an averaged hearing loss as measured by pure tone air-conduction of between 25 and about 60 dB in the better ear, with fluent speaking of (and listening to) British English. "},{"title":"D Data file formats and naming conventions​","type":1,"pageTitle":"CEC1 Data","url":"docs/cec1/cec1_data#d-data-file-formats-and-naming-conventions","content":""},{"title":"D.1 Abbreviations in Filenames​","type":1,"pageTitle":"CEC1 Data","url":"docs/cec1/cec1_data#d1-abbreviations-in-filenames","content":"R – “room”: e.g., “R02678” # Room ID linking to RAVEN rpf fileS – “scene”: e.g., S00121 # Scene ID for a particular setup in a room I.e., room + choice of target and interferer signalsBNC – BNC sentence identifier e.g. BNC_A06_01702CH – CH0 – eardrum signalCH1 – front signal, hearing aid channelCH2 – middle signal, hearing aid channelCH3 – rear signal, hearing aid channel I/i1 – Interferer, i.e., noise or sentence ID for the interferer/maskerT – talker who produced the target speech sentencesL – listenerE – entrant (identifying a team participating in the challenge)t – target (used in BRIRs and RAVEN project ‘rpf’ files) "},{"title":"D.2 General​","type":1,"pageTitle":"CEC1 Data","url":"docs/cec1/cec1_data#d2-general","content":"Audio and BRIRs will be 44.1 kHz 32 bit wav files in either mono or stereo as appropriate.Where stereo signals are provided the two channels represent the left and right signals of the ear or hearing aid microphones.HRIRs have a sampling rate of 48 kHz.Metadata will be stored in JSON format wherever possible.Room descriptions are stored as RAVEN project ‘rpf’ configuration files.Signals are saved within the Python code as 32-bit floating point by default. "},{"title":"D.3 Prompt and transcription data​","type":1,"pageTitle":"CEC1 Data","url":"docs/cec1/cec1_data#d3-prompt-and-transcription-data","content":"The following text is available for the target speech: Prompts are the text that was supposed to be spoken as presented to the readers.‘Dot’ transcriptions contain the text as it was spoken in a form more suitable for scoring tools.These are stored in the master json metadata file. "},{"title":"D.4 Source audio files​","type":1,"pageTitle":"CEC1 Data","url":"docs/cec1/cec1_data#d4-source-audio-files","content":"Wav files containing the original source materials.Original target sentence recordings:  &lt;Talker ID&gt;_&lt;BNC sentence identifier&gt;.wav  "},{"title":"D.5 Preprocessed scene signals​","type":1,"pageTitle":"CEC1 Data","url":"docs/cec1/cec1_data#d5-preprocessed-scene-signals","content":"Audio files storing the signals picked up by the hearing aid microphone ready for processing. Separate signals are generated for each hearing aid microphone pair or ‘channel’. &lt;Scene ID&gt;_target_&lt;Channel ID&gt;.wav &lt;Scene ID&gt;_interferer_&lt;Channel ID&gt;.wav &lt;Scene ID&gt;_mixed_&lt;Channel ID&gt;.wav &lt;Scene ID&gt;_target_anechoic.wav  Scene ID – S00001 to S10000 S followed by 5 digit integer with 0 pre-padding Channel ID CH0 – Eardrum signalCH1 – Hearing aid front microphoneCH2 – Hearing aid middle microphoneCH3 – Hearing aid rear microphone "},{"title":"D.6 Enhanced signals​","type":1,"pageTitle":"CEC1 Data","url":"docs/cec1/cec1_data#d6-enhanced-signals","content":"The signals that are output by the enhancement (hearing aid) model. &lt;Scene ID&gt;_&lt;Listener ID&gt;_HA-output.wav #HA output signal (i.e., as submitted by the challenge entrants) Listener ID – ID of the listener panel member, e.g., L001 to L100 for initial ‘pseudo-listeners’, etc. We are no longer providing the script for post-processing signals in preparation for the listener panel. "},{"title":"D.7 Enhanced signals processed by the hearing loss model​","type":1,"pageTitle":"CEC1 Data","url":"docs/cec1/cec1_data#d7-enhanced-signals-processed-by-the-hearing-loss-model","content":"The signals that are produced by the hearing loss (HL) model. &lt;Scene ID&gt;_&lt;Listener ID&gt;_HL-output.wav HL output signal&lt;Scene ID&gt;_&lt;Listener ID&gt;_HL-mixoutput.wav HL-processed CH0 signal, bypassing HA processing, for comparison&lt;Scene ID&gt;_&lt;Listener ID&gt;_flat0dB_HL-output HL-output for flat 0 dB audiogram processed signal for comparison&lt;Scene ID&gt;_&lt;Listener ID&gt;_HLddf-output unit impulse signal output by HL model for time-alignment of signals before processing by the baseline speech intelligibility model "},{"title":"D.8 Scene metadata​","type":1,"pageTitle":"CEC1 Data","url":"docs/cec1/cec1_data#d8-scene-metadata","content":"JSON file containing a description of the scene and assigns the scene to a specific member of the listening panel. It is a hierarchical dictionary, with the top level being scenes indexed by unique scene ID, and each scene described by a second-level dictionary. Here, viewvector indicates the direction vector or line of sight. [ { scene&quot;: &quot;S00001&quot;, &quot;room&quot;: { &quot;name&quot;: &quot;R00001&quot;, &quot;dimensions&quot;: &quot;5.9x3.4186x2.9&quot; # Room dimensions in metres }, &quot;SNR&quot;: 3.8356, &quot;hrirfilename&quot;: &quot;VP_N5-ED&quot;, # HRIR filename &quot;target&quot;: { # target positions (x,y,z) and view vectors (look directions, x,y,z) &quot;Positions&quot;: [ -0.5, 3.4, 1.2 ], &quot;ViewVectors&quot;: [ 0.291, -0.957, 0 ], &quot;name&quot;: &quot;T022_HCS_00002&quot;, # target speaker code and BNCid &quot;nsamples&quot;: 153468, # length of target speech in samples }, &quot;listener&quot;: { &quot;Positions&quot;: [ 0.2, 1.1, 1.2 ], &quot;ViewVectors&quot;: [ -0.414, 0.91, 0 ] }, &quot;interferer&quot;: { &quot;Positions&quot;: [ 0.4, 3.2, 1.2 ], &quot;name&quot;: &quot;CIN_dishwasher_012&quot;, # interferer name &quot;nsamples&quot;: 1190700, # interferer length in samples &quot;duration&quot;: 27, # interferer duration in seconds &quot;type&quot;: &quot;noise&quot;, # interferer type: noise or speech &quot;offset&quot;: 182115, # interferer segment starts at n samples from beginning of recording }, &quot;azimuth_target_listener&quot;: -7.55, # angle azimuth in degrees of target for receiver &quot;azimuth_interferer_listener&quot;: -29.92, # angle azimuth in degrees of interferer for receiver &quot;dataset&quot;: &quot;train&quot;, # dataset: train, dev or eval/test &quot;pre_samples&quot;: 88200, # number of samples of interferer before target onset &quot;post_samples&quot;: 44100 # number of samples of interferer after target offset }, { etc. } ]  There are JSON files containing the scene specifications per dataset, e.g., scenes.train.json.- Note, that the scene ID and room ID might have a one-to-one mapping in the challenge, but are not necessarily the same. Multiple scenes can be made by changing the target and masker choices for a given room. E.g., participants wanting to expand the training data could remix multiple scenes from the same room.A scene is completely described by the room ID and target and interferer source IDs, as all other information, e.g., source + target geometry are already in the RAVEN project rpf files. Only the room ID is needed to identify the BRIR files.The listener ID is not stored in the scene metadata; this information is stored separately in a scenes_listeners.json file.Non-speech interferers are labelled CIN_&lt;noise type&gt;_XXX, while speech interferers are labelled &lt;three letter code including dialect and talker gender&gt;_XXXXX . "},{"title":"D.9 Listener metadata​","type":1,"pageTitle":"CEC1 Data","url":"docs/cec1/cec1_data#d9-listener-metadata","content":"Listener data stored in a single JSON file with the following format. {“L0001”: { “name”: “L0001”, &quot;audiogram_cfs&quot;: [250, 500, 1000, 2000, 3000, 4000, 6000, 8000], “audiogram_levels_l”: [10, 10, 20, 30, 40, 55, 55, 60], “audiogram_levels_r”: [ … ], }, “L0002”: { }, ... }  "},{"title":"D.10 Scene-Listener map​","type":1,"pageTitle":"CEC1 Data","url":"docs/cec1/cec1_data#d10-scene-listener-map","content":"JSON file named scenes_listeners.json dictates which scenes are to be processed by which listeners. {“S00001”: [“L0001”, “L0002”, “L0003”], “S00002”: [“L0003”. “L0005”, “L0007”], etc }  "},{"title":"CEC2 FAQ","type":0,"sectionRef":"#","url":"docs/cec2/cec2_faq","content":"","keywords":""},{"title":"Speech Intelligibility​","type":1,"pageTitle":"CEC2 FAQ","url":"docs/cec2/cec2_faq#speech-intelligibility","content":""},{"title":"What is Speech Intelligibility?​","type":1,"pageTitle":"CEC2 FAQ","url":"docs/cec2/cec2_faq#what-is-speech-intelligibility","content":"The term Speech Intelligibility is generally used in two different ways. It can refer to how much speech is understood by a listener, or to the number of words correctly identified by a listener as a proportion or percentage of the total number of words. In the Clarity project, we are using the latter definition, i.e., the percentage of words in a sentence that a listener identified correctly. This percentage is the target for your prediction models. Speech intelligibility captures how a listener's ability to participate in conversation is changed when the speech signal is degraded, e.g., by background noise and room reverberation, or is processed, e.g., by a hearing aid. Your prediction model will need to incorporate a model of the hearing abilities of each listener. "},{"title":"How is Speech Intelligibility measured with listeners?​","type":1,"pageTitle":"CEC2 FAQ","url":"docs/cec2/cec2_faq#how-is-speech-intelligibility-measured-with-listeners","content":"In the Clarity project, a set of listeners listen to a sentence and then say what words they heard. In this project, speech intelligibility is measured as the number of words identified correctly as a percentage of the total number of words in a sentence. You might consider looking at other metrics, such as Word Error Rate (WER), which picks up on, e.g., where listeners insert words not in the original sentence. You might do this if you think that an estimate of WER or other metrics would help your system to estimate speech intelligibility, as defined in the Clarity project. "},{"title":"How is Speech Intelligibility objectively measured by a computer?​","type":1,"pageTitle":"CEC2 FAQ","url":"docs/cec2/cec2_faq#how-is-speech-intelligibility-objectively-measured-by-a-computer","content":"When fitting a hearing aid, it would be beneficial for an audiologist to be able to use an objective measure of speech intelligibility to determine what signal processing algorithm(s) should be used to compensate for the listener's hearing impairment. Objective measures are also useful when measured speech intelligibility scores are unavailable, such as when developing a machine learning-based hearing aid algorithm or some other speech enhancement method. Another advantage of non-intrusive measures is that they do not require time-alignment of processed and reference signals. Objective measures - or metrics - of speech intelligibility are used to allow a computer to estimate the likely performance of humans in listening tests. The main goal of entries to the prediction challenge is to produce one of these measures that performs well for listeners with hearing loss. There are two broad classes of speech intelligibility models: Intrusive metrics (also known as double-ended) are most common. This is where the intelligibility is estimated by comparing the degraded or processed speech signal with the original clean speech signal.Non-intrusive metrics (also known as single-ended or blind) are less well developed. This is where intelligibility is estimated from the degraded or processed speech signal alone. In the Clarity project, both types of metrics are of interest. Intrusive metrics will be more accurate in many cases. However, there are hearing aid processes where the speech content is shifted in frequency, which will defeat most current intrusive speech intelligibility metrics. We also hypothesise that there might be issues with intrusive metrics and machine learning approaches in hearing aids that revoice the original speech. "},{"title":"What speech intelligibility models already exist and what are they used for?​","type":1,"pageTitle":"CEC2 FAQ","url":"docs/cec2/cec2_faq#what-speech-intelligibility-models-already-exist-and-what-are-they-used-for","content":"There aren't many speech intelligibility models that consider hearing impairment, but one that does is HASPI by Kates and Arehart. In this seminar from the first Clarity workshop, James Kates discusses speech intelligibility models with a focus on the ones he has developed. He also discusses the speech quality metric HASQI. If you're interested in using HASPI or HASQI for the challenge, James Kates has kindly made the MATLAB code and user guide available for download.  Click arrow to see synopsis. Signal degradations, such as additive noise and nonlinear distortion, can reduce the intelligibility and quality of a speech signal. Predicting intelligibility and quality for hearing aids is especially difficult since these devices may contain intentional nonlinear distortion designed to make speech more audible to a hearing-impaired listener. This speech processing often takes the form of time-varying multichannel gain adjustments. Intelligibility and quality metrics used for hearing aids and hearing-impaired listeners must therefore consider the trade-offs between audibility and distortion introduced by hearing-aid speech envelope modifications. This presentation uses the Hearing Aid Speech Perception Index (HASPI) and the Hearing Aid Speech Quality Index (HASQI) to predict intelligibility and quality, respectively. These indices incorporate a model of the auditory periphery that can be adjusted to reflect hearing loss. They have been trained on intelligibility scores and quality ratings from both normal-hearing and hearing-impaired listeners for a wide variety of signal and processing conditions. The basics of the metrics are explained, and the metrics are then used to analyse the effects of additive noise on speech, to evaluate noise suppression algorithms, and to measure differences among commercial hearing aids. "},{"title":"Hearing Loss​","type":1,"pageTitle":"CEC2 FAQ","url":"docs/cec2/cec2_faq#hearing-loss","content":"There are many types of hearing loss, but the focus of the Clarity project is the hearing loss that happens with ageing. This is a form of sensorineural hearing loss. "},{"title":"How does hearing loss affect the perception of audio signals, and how do modern hearing aids process sound to help with this?​","type":1,"pageTitle":"CEC2 FAQ","url":"docs/cec2/cec2_faq#how-does-hearing-loss-affect-the-perception-of-audio-signals-and-how-do-modern-hearing-aids-process-sound-to-help-with-this","content":"In this seminar from the first Clarity workshop, Karolina Smeds from ORCA Europe and WS Audiology discusses the effects of hearing loss and the hearing aid processing strategies that are typically used to counter the sensory deficits.  Click arrow to see synopsis. Hearing loss leads to several unwanted effects. Loss of audibility for soft sounds is one effect, but also when amplification is used to create audibility for soft sounds, many [suprathreshold](https://www.lexico.com/en/definition/suprathreshold) deficits remain. The most common type of hearing loss is a [cochlear](https://www.lexico.com/definition/cochlear) hearing loss, where haircells or nerve synapses in the cochlea are damaged. Ageing and noise exposure are the most common causes of cochlear hearing loss. This type of hearing loss is associated with atypical loudness perception and difficulties in noisy situations. Background noise masks for instance speech to a higher degree than for a person with healthy hair cells. This explains why listening to speech-in-noise (SPIN) is such an important topic to work on. A brief introduction to signal processing in hearing aids will be presented. With the use of frequency-specific amplification and compression (automatic gain control, AGC), hearing aids are usually doing a good job in compensating for reduced audibility and for atypical suprathreshold loudness perception. However, it is more difficult to compensate for the increased masking effect. Some examples of strategies will be presented. Finally, natural conversations in noise will be discussed. The balance between being able to have a conversation with a specific communication partner in a group of people and being able to switch attention if someone else starts to talk will be touched upon. "},{"title":"The 2nd Clarity Enhancement Challenge","type":0,"sectionRef":"#","url":"docs/cec2/cec2_intro","content":"","keywords":""},{"title":"Overview of challenge​","type":1,"pageTitle":"The 2nd Clarity Enhancement Challenge","url":"docs/cec2/cec2_intro#overview-of-challenge","content":"We want you to improve speech in the presence of background noise - see Figure 1. On the left there is a person with a quantified hearing loss. They are listening to speech from the target talker on the right. They are both in a living room. There is interfering noise from a number of sources (TV and washing machine in this case). The speech and noise is sensed by microphones on the hearing aids of the listener. Your task is to take these microphone feeds and the listener’s hearing characteristics, to produce signals where the speech is more intelligible. We will evaluate the success of your processing using an objective speech intelligibility metric. Some entrants will also be evaluated by a panel of listeners with a hearing impairment. Figure 1. The scenario involves one talker, a listener who rotates their head, and at least two common sources of unwanted sound. The scenario has been made more difficult than the first Clarity Enhancement Challenge by having: More noise sourcesMore varied noise sources (speech, music, appliances)The listener turns their head during the talking.Less predictable target onset timing. For more details use the contents pane on the left to navigate the CEC2 site. "},{"title":"Submission","type":0,"sectionRef":"#","url":"docs/cec1/cec1_submission","content":"","keywords":""},{"title":"Registration​","type":1,"pageTitle":"Submission","url":"docs/cec1/cec1_submission#registration","content":"Teams are required to register using the form below. Please submit one form per team, i.e., providing a single contact email address. Once you have registered, you will receive an email confirmation with a team ID and an individualised link to a Google Drive for submitting materials. Loading… info It is important that all teams who are intending to submit an entry complete the registration form no later than 11th June. "},{"title":"What evaluation data is provided?​","type":1,"pageTitle":"Submission","url":"docs/cec1/cec1_submission#what-evaluation-data-is-provided","content":"The evaluation data consists of 1500 scenes. For each scene you are provided with the signals received at each of the three microphones on the left and right hearing aid device. You will also be provided with JSON formatted metadata consisting of the audiograms of a set of listeners anda mapping of which listeners will listen to which scenes. For the MBSTOI evaluation, there will be one listener per scene and the scene-listener mapping will be the same for all teams. For the listening test evaluation, there will be five listeners per scene and each team will have a separate scene-listener mapping. The file formats will be the same as used for the development data; for details see the CEC1 Data page. "},{"title":"What audio do I need to submit?​","type":1,"pageTitle":"Submission","url":"docs/cec1/cec1_submission#what-audio-do-i-need-to-submit","content":"You must submit the audio signals produced at the output of your simulated hearing aid for the evaluation datasets. You will be asked to provide two sets of signals: the first for the MBSTOI evaluation (due 15th June) and the second for the listening tests (due 29th June). MBSTOI evaluation. Signals should be submitted in floating point WAV format with a 44.1 kHz sampling rate. For levels, we will follow the convention in the baseline hearing aid (at the output) and hearing loss models. That is, a +/-1 square wave has RMS = 0 dB FS and corresponds to 120 dB. Listening tests. Signals should be submitted as 16-bit WAV files with a 32 kHz sampling rate (due to hardware limitations). You should ensure that any samples that are &gt;+1 or &lt;-1 have been hard-clipped at +/-1 before submission. Here, 0 dB FS corresponds to approximately 100 dB, given the capabilities of the reproduction equipment. These signals will be played as is to the listener panel. We also encourage you to submit your simulated hearing aid code. See the page on listening tests for more information about the levels that can be reproduced by the listening test equipment. When playing signals to listeners we will then play them as is. The responsibility for the final signal level is therefore yours. It’s worth bearing in mind that should your signals overall seem too loud to be comfortable to a participant, they may well turn down the volume themselves. "},{"title":"Naming and packaging signals​","type":1,"pageTitle":"Submission","url":"docs/cec1/cec1_submission#naming-and-packaging-signals","content":"Your processed signals should be named using the conventions used by the baseline system, i.e., &lt;Scene ID&gt;_&lt;Listener ID&gt;_HA-output.wav and explained on the CEC1 data page. These should be placed in a directory whose name is the unique team ID that you will be sent, e.g., E001 and then packaged using zip or tar or any standard packaging tool. The resulting file should be about 2 GB for the first round. "},{"title":"Technical report​","type":1,"pageTitle":"Submission","url":"docs/cec1/cec1_submission#technical-report","content":"The two page technical report must be submitted as a paper to the Clarity-2021 Workshop. Deadline 22nd June. An author kit and submission instructions are available at the workshop website. A draft of the report needs to be uploaded to the Google Drive along with your MBSTOI signals by 15th June. The draft needs to be sufficiently complete for us to judge whether your system is compliant with the challenge rules. Your report should include an abstract and introduction and sections on experimental setup/methodology including system information and model/network architecture, evaluation/results, discussion, conclusion and references. Please provide an estimation of the computational resources needed. You must describe any external data and pre-existing tools, software and models used. Your report should cite the following document, which provides an overview of the challenge and the baseline system: S. Graetzer, J. Barker, T. J. Cox, M. Akeroyd, J. F. Culling, G. Naylor, E. Porter, and R. Viveros Muñoz, “Clarity-2021 challenges: Machine learning challenges for advancing hearing aid processing,” in Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH 2021, Brno, Czech Republic, 2021. The document can be accessed here. "},{"title":"How will intellectual property be handled?​","type":1,"pageTitle":"Submission","url":"docs/cec1/cec1_submission#how-will-intellectual-property-be-handled","content":"See here under Intellectual Property. "},{"title":"Where do I submit the signals?​","type":1,"pageTitle":"Submission","url":"docs/cec1/cec1_submission#where-do-i-submit-the-signals","content":"When you have registered you will receive a link to a Google Drive to which you will be able to securely upload your signals. You will be able to use the same link to upload materials for both the 1st submission, and the 2nd submission if you are selected for the 2nd round. We also encourage you to submit your simulated hearing aid code via this link. Materials uploaded will be visible to the Clarity Team but not to other entrants. danger Note, in order to use the Google Drive you will need to have a Google account. If you anticipate problems using Google then please make arrangements to send us the materials by other means, e.g., via a service such as WeTransfer or similar. "},{"title":"Find collaborators","type":0,"sectionRef":"#","url":"docs/cec2/cec2_find_a_team","content":"Find collaborators If you'd like to team up with someone else to compete in the challenges, we can help. Please complete this Google form to let us know your own expertise, and what you're looking for in a collaborator. We'll then put people in contact with possible collaborators. We encourage everyone to join the Clarity Challenge’s Google group to stay updated with project news and announcements. We post in there when we have new people seeking team members (we don't share any personally-identifying details to the group). You are welcome to contact us if you have any questions about forming a team or participating in the challenge: Email the Clarity Team","keywords":""},{"title":"CEC2 Download","type":0,"sectionRef":"#","url":"docs/cec2/cec2_download","content":"","keywords":""},{"title":"Software​","type":1,"pageTitle":"CEC2 Download","url":"docs/cec2/cec2_download#software","content":"All the necessary software tools are available as a single GitHub repository. We recommend installing the software first and then following the instructions in the repositorys README for downloading and unpacking the data. "},{"title":"Data​","type":1,"pageTitle":"CEC2 Download","url":"docs/cec2/cec2_download#data","content":"The data is available for download here. On the download site you will see three data packages are available, clarity_CEC2_core.v1_1.tgz [28 GB] - metadata and dev setclarity_CEC2_train.v1_1.tgz [69 GB] - scenes for training systemsclarity_CEC2_hoairs.v1_0.tgz [144 GB] - impulse responses for generating extended training data All participants will require the core data package. Participants using machine learning approaches will additionally require the train data package. Participants wishing to extend the training set by using our provided scene rendering tools will also require the high order ambisonic impulse responses (i.e., the hoairs package). To unpack the data we recommend you follow the instructions in the Clarity Challenge GitHub repository. Attention If you previously downloaded v1_0 of the core and train data, please replace your data with v1_1: an error was found in the head rotations for the initial data release. "},{"title":"Software","type":0,"sectionRef":"#","url":"docs/cec1/cec1_software","content":"","keywords":""},{"title":"A. Scene generator​","type":1,"pageTitle":"Software","url":"docs/cec1/cec1_software#a-scene-generator","content":"Fully open-source python code for generating hearing aid inputs for each scene Inputs: target and interferer signals, BRIRs, RAVEN project (rpf) files, scene description JSON filesOutputs: Mixed target+interferer signals for each hearing aid channel, direct path (simulating a measurement close to the eardrum). Reverberated pre-mixed signals can also be optionally generated. "},{"title":"B. Baseline hearing aid processor​","type":1,"pageTitle":"Software","url":"docs/cec1/cec1_software#b-baseline-hearing-aid-processor","content":"The baseline hearing aid processor is based on openMHA. The python code configures openMHA with a Camfit compressive fitting for a specific listener’s audiogram. This includes a python implementation of the Camfit compressive prescription and python code for driving openMHA. This configuration of openMHA includes multiband dynamic compression and non-adaptive differential processing. The intention was to produce a basic hearing aid without various aspects of signal processing that are common in high-end hearing aids, but tend to be implemented in proprietary forms so cannot be replicated exactly. The main inputs and outputs for the processor are as follows: Inputs: Mixed scene signals for each hearing aid channel, a listener ID drawn from scene-listener pairs identified in ‘scenes_listeners.json’ and an entry in the listener metadata json file ‘listeners.json’ for that IDOutputs: The stereo hearing aid output signal, &lt;scene&gt;_&lt;listener&gt;_HA-output.wav "},{"title":"C. Hearing Loss model​","type":1,"pageTitle":"Software","url":"docs/cec1/cec1_software#c-hearing-loss-model","content":"Open-source python implementation of the Cambridge Auditory Group Moore/Stone/Baer/Glasberg hearing loss model. Inputs: A stereo wav audio signal, e.g., the output of the baseline hearing aid processor, and a set of audiograms (both L and R ears).Outputs: The signal after simulating the hearing loss as specified by the set of audiograms (stereo wav file), &lt;scene&gt;_&lt;listener&gt;_HL-output.wav "},{"title":"D. Speech Intelligibility model​","type":1,"pageTitle":"Software","url":"docs/cec1/cec1_software#d-speech-intelligibility-model","content":"Python implementation of a binaural intelligibility model, Modified Binaural Short-Time Objective Intelligibility (MBSTOI). This is an experimental baseline tool that will be used in the stage 1 evaluation of entrants (see Rules). Note that MBSTOI requires signal time-alignment (and alignment within one-third octave bands). Inputs: HL-model output signals, audiogram, reference target signal (i.e., the premixed target signal convolved with the BRIR with the reflections “turned off”, specified as ‘target_anechoic’), (scene metadata)Outputs: predicted intelligibility score "},{"title":"Scene Generation","type":0,"sectionRef":"#","url":"docs/cec2/data/cec2_scene_generation","content":"","keywords":""},{"title":"References​","type":1,"pageTitle":"Scene Generation","url":"docs/cec2/data/cec2_scene_generation#references","content":" Schröder, D. and Vorländer, M., 2011, January. RAVEN: A real-time framework for the auralization of interactive virtual environments. In Proceedings of Forum Acusticum 2011 (pp. 1541-1546). Denmark: Aalborg. "},{"title":"Additional Tools","type":0,"sectionRef":"#","url":"docs/cec2/software/cec2_additional_tools","content":"","keywords":""},{"title":"Hearing loss model​","type":1,"pageTitle":"Additional Tools","url":"docs/cec2/software/cec2_additional_tools#hearing-loss-model","content":"This is an open-source python implementation of a hearing loss model developed by Brian Moore, Michael Stone and other members of the Auditory Perception Group, University of Cambridge [1, 2]. Inputs: A stereo wav audio signal, e.g., the output of the hearing aid model and audiograms for left and right ear.Outputs: The signal after simulating the hearing loss as specified by the set of audiograms (stereo wav file), &lt;scene&gt;_&lt;listener&gt;_HL-output.wav "},{"title":"Differentiable source separation and hearing aid amplification modules​","type":1,"pageTitle":"Additional Tools","url":"docs/cec2/software/cec2_additional_tools#differentiable-source-separation-and-hearing-aid-amplification-modules","content":"The modules are from the Sheffield E009 system in CEC1. The source separation module is a multi-channel Conv-TasNet optimised with a SNR objective. The hearing aid amplification module is an FIR filter optimised with an objective, which is the combination of a differentiable approximation to the hearing loss model and a STOI loss. Inputs: six channels of mixed signals, i.e., mixed_CH1.wav, mixed_CH2.wav, and mixed_CH3.wavOutputs: a single channel enhanced signal, therefore two source separation and amplification modules for left and right ears need to be optimised for the enhanced binaural signal. "},{"title":"Speech intelligibility model (MBSTOI)​","type":1,"pageTitle":"Additional Tools","url":"docs/cec2/software/cec2_additional_tools#speech-intelligibility-model-mbstoi","content":"Python implementation of a binaural intelligibility model, Modified Binaural Short-Time Objective Intelligibility (MBSTOI) [3]. Note that MBSTOI requires signal time-alignment (and alignment within one-third octave bands). Inputs: HL-model output signals, audiogram, reference target signal (i.e., the premixed target signal convolved with the BRIR with the reflections 'turned off', specified as 'target_anechoic'), (scene metadata)Outputs: predicted intelligibility score "},{"title":"References​","type":1,"pageTitle":"Additional Tools","url":"docs/cec2/software/cec2_additional_tools#references","content":" Moore, B. C. J., Alcantara, J. I., Stone, M. and Glasberg, B. R., 1999. Use of a loudness model for hearing aid fitting: II. Hearing aids with multi-channel compression. British Journal of Audiology, 33(3), pp. 157-170.Nejime, Y. and Moore, B. C., 1997. Simulation of the effect of threshold elevation and loudness recruitment combined with reduced frequency selectivity on the intelligibility of speech in noise. Journal of the Acoustical Society of America, 102(1), pp. 603-615.Andersen, A. H., de Haan, J. M., Tan, Z. H. and Jensen, J., 2018. Refinement and validation of the binaural short-time objective intelligibility measure for spatially diverse conditions. Speech Communication, 102, pp. 1-13. "},{"title":"Baseline System","type":0,"sectionRef":"#","url":"docs/cec2/software/cec2_baseline","content":"","keywords":""},{"title":"Baseline performance​","type":1,"pageTitle":"Baseline System","url":"docs/cec2/software/cec2_baseline#baseline-performance","content":"The average speech intelligibility (HASPI) score for the unprocessed development test set is 0.1615. When processed with the simple baseline hearing aid (i.e., NALR amplification followed by a simple automatic gain compressor) the average HASPI score increases to 0.2493. These results are summarised in the table below. Your task is to improve on the 0.2493 baseline HASPI score. System\tHASPIUnprocessed\t0.1615 NAL-R baseline\t0.2493 "},{"title":"References​","type":1,"pageTitle":"Baseline System","url":"docs/cec2/software/cec2_baseline#references","content":" Kates, J.M. and Arehart, K.H., 2021. The hearing-aid speech perception index (haspi) version 2. Speech Communication, 131, pp.35-46. "},{"title":"Core Software","type":0,"sectionRef":"#","url":"docs/cec2/software/cec2_core_software","content":"","keywords":""},{"title":"A. Scene generator​","type":1,"pageTitle":"Core Software","url":"docs/cec2/software/cec2_core_software#a-scene-generator","content":"Fully open-source python code for generating hearing aid inputs for each scene Inputs: target and interferer signals, HOA-IRs, RAVEN project (rpf) files, scene description JSON filesOutputs: Mixed target+interferer signals for each hearing aid channel, direct path (simulating a measurement close to the eardrum). Reverberated pre-mixed signals can also be optionally generated. "},{"title":"B. Baseline hearing aid processor​","type":1,"pageTitle":"Core Software","url":"docs/cec2/software/cec2_core_software#b-baseline-hearing-aid-processor","content":"The baseline hearing aid consists of a NAL-R fitting amplification stage [1] followed by a simple automatic gain compressor. It produces output signals in 16-bit wav format ready for HASPI or listening test evaluation. Inputs: Inputs for each hearing aid channel and audiograms to characterise the listeners.Outputs: Stereo hearing aid (HA) outputs signals. "},{"title":"C. HASPI Speech Intelligibility model​","type":1,"pageTitle":"Core Software","url":"docs/cec2/software/cec2_core_software#c-haspi-speech-intelligibility-model","content":"Python implementation of the Hearing Aid Speech Perception Index (HASPI) model which is used for objective intelligibility estimation. This will be used in the stage 1 evaluation of entrants (see Rules). Inputs: reference target signal (i.e., the premixed target signal convolved with the BRIR with the reflections “turned off”, specified as ‘target_anechoic’), HA output signals, audiogram, level reference (level in dB SPL which corresponds to 0 dB FS)Outputs: predicted intelligibility score It is important to remember that both reference target and HA output signals have to be calibrated to the same dB SPL level before calculating HASPI. "},{"title":"References​","type":1,"pageTitle":"Core Software","url":"docs/cec2/software/cec2_core_software#references","content":" [1] Byrne, Denis, and Harvey Dillon. &quot;The National Acoustic Laboratories'(NAL) new procedure for selecting the gain and frequency response of a hearing aid.&quot; Ear and hearing 7.4 (1986): 257-265. "},{"title":"Listening Tests","type":0,"sectionRef":"#","url":"docs/cec2/taking_part/cec2_listening_tests","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"Listening Tests","url":"docs/cec2/taking_part/cec2_listening_tests#overview","content":"The listeners will be provided with a USB stereo headset to complete the listening experiment. The experiment will be run by our “Listen@Home” web software running on either the participant’s own computer or a tablet we supply. The software plays each sentence once, then the participant speaks aloud what they think was said by the target talker. Their response is recorded by the headset’s microphone for offline scoring. We ask that the participant uses a quiet room for the experiment. Intelligibility will be evaluated as the number of words identified correctly in the sentence. Our plan is that each listener will undertake a few hours of listening and evaluate sentences from every entrant. We will use a combinatorial design to equate this as far as possible. Should a listener drop out from the panel, we will endeavour to replace them with someone with a similar hearing loss, but should that prove impractical we will reduce the size of the panel, and inform entrants which listener has withdrawn. "},{"title":"Listen@Home hardware​","type":1,"pageTitle":"Listening Tests","url":"docs/cec2/taking_part/cec2_listening_tests#listenhome-hardware","content":"We will be using Sennheiser PC-8 headsets to play the sounds to our participants. We will allow participants to set the volume so that the sounds are not so loud to be uncomfortable. Without loudness-recruitment measures for our listeners, we cannot be sure just what loudnesses every participant will hear, so we need to allow them to make the choice here. We have measurements of the output capability of a system in the laboratory: A 1 kHz pure tone set to be the most powerful it can be (i.e., an amplitude range of +/-1 = RMS amplitude of 0.707, and the volume controls at 100%) gave 99 dB(A) SPL on the PC-8 headphones.An ICRA speech-shaped noise [1], unmodulated in time, and scaled to an RMS of 0.3, gave 90 dB(A) at the same volume level. With this RMS, the noise had 0.1% of its samples clipped at +/- 1. Due to the above capabilities of the reproduction equipment, in the submitted signals, 0 dB FS should correspond to 100 dB SPL. We will also require the signals to be provided as 16-bit WAV files with a 32 kHz sampling rate. We will play the signals as is using an HTML/PHP audio player coded on a webpage. The responsibility for the final signal level is therefore yours. It’s worth bearing in mind that should your signals overall seem too loud to be comfortable to a participant, they may well turn down the volume themselves. "},{"title":"References​","type":1,"pageTitle":"Listening Tests","url":"docs/cec2/taking_part/cec2_listening_tests#references","content":" ICRA standard noises, https://icra-audiology.org/Repository/icra-noise. We used track #1. "},{"title":"CEC2 Rules","type":0,"sectionRef":"#","url":"docs/cec2/taking_part/cec2_rules","content":"","keywords":""},{"title":"Teams​","type":1,"pageTitle":"CEC2 Rules","url":"docs/cec2/taking_part/cec2_rules#teams","content":"Teams must have pre-registered and nominated a contact person.Teams can be from one or more institutions.The organisers may enter the challenge themselves but will not be eligible to win prizes. "},{"title":"Transparency​","type":1,"pageTitle":"CEC2 Rules","url":"docs/cec2/taking_part/cec2_rules#transparency","content":"Teams must provide a technical document of up to 2 pages describing the system/model and any external data and pre-existing tools, software and models used.We will publish all technical documents (anonymous or otherwise).Teams are encouraged – but not required – to provide us with access to the system/model and to make their code open source.Anonymous entries are allowed but will not be eligible for prizes.Teams may reserve the right to be referred to using anonymous code names in the published rank ordering. "},{"title":"What information can I use?​","type":1,"pageTitle":"CEC2 Rules","url":"docs/cec2/taking_part/cec2_rules#what-information-can-i-use","content":""},{"title":"Training and development​","type":1,"pageTitle":"CEC2 Rules","url":"docs/cec2/taking_part/cec2_rules#training-and-development","content":"There is no limit on the amount of training data that can be generated using our tools and training data sets. Teams can also use their own data for training or expand the training data through simple automated modifications. However, teams that do this must make a second submission using only the official audio files and signal generation tool. Any audio or metadata can be used during training and development, but during evaluation, the proposed simulated hearing aid or Enhancement Processor will not have access to all of the data (see next section). "},{"title":"Evaluation​","type":1,"pageTitle":"CEC2 Rules","url":"docs/cec2/taking_part/cec2_rules#evaluation","content":"The only data that can be used by the Enhancement Processor during evaluation are The audio input signals (the sum of the target and interferers for each hearing aid microphone).The listener characterisation (pure tone air-conduction audiograms and/or digit triple test results).The provided clean audio examples for the target talker (these will not be the same as any of the target utterances.)The head-rotation signal (but if used, a version of the system that does not use it should also be prepared for comparison.) "},{"title":"Computational restrictions​","type":1,"pageTitle":"CEC2 Rules","url":"docs/cec2/taking_part/cec2_rules#computational-restrictions","content":"Teams may choose to use all, some or none of the parts of the baseline model.Systems must be causal; the output from the hearing aid at time t must not use any information from input samples more than 5 ms into the future (i.e., no information from input samples &gt;t+5 ms).There is no limit on computational cost. Please see this blog post for further explanation of these last two rules about latency and computation time. "},{"title":"Submitting multiple entries​","type":1,"pageTitle":"CEC2 Rules","url":"docs/cec2/taking_part/cec2_rules#submitting-multiple-entries","content":"You can submit two entries, where one is optimised for HASPI and the other for listening tests if you wish. In this case: Both systems must be submitted for HASPI evaluation.You must register two teams, submitting each entry as a different team.In your documentation, you must make it clear which has been optimised for listening tests and the relationship between the two entries.head-rotation: if the head-rotation signal is used then a second entry must be submitted that does not use it and allows the benefit to be measured. We will assume that if only one of these systems is to go forward to listening tests, your preference is to use the one optimised for listening tests. "},{"title":"Evaluation of systems​","type":1,"pageTitle":"CEC2 Rules","url":"docs/cec2/taking_part/cec2_rules#evaluation-of-systems","content":""},{"title":"Stage 1: Objective evaluation​","type":1,"pageTitle":"CEC2 Rules","url":"docs/cec2/taking_part/cec2_rules#stage-1-objective-evaluation","content":"Entries will be ranked according to average HASPI score across all signals in the evaluation dataset. We will use the HASPI implementation in the baseline system. "},{"title":"Stage 2: Listening test evaluation​","type":1,"pageTitle":"CEC2 Rules","url":"docs/cec2/taking_part/cec2_rules#stage-2-listening-test-evaluation","content":"There is a limit on how many systems can be evaluated by the listener panel.A maximum of two entries can go through to the listener panel from any individual entrant. Furthermore, a second will only be allowed if it is judged by us to use significantly different signal processing approaches.We will choose which will go to the listener panel based on The top N scored using the objective metric HASPI.A sample of M others that use contrasting and promising approaches. "},{"title":"Intellectual property​","type":1,"pageTitle":"CEC2 Rules","url":"docs/cec2/taking_part/cec2_rules#intellectual-property","content":"The following terms apply to participation in this machine learning challenge (“Challenge”). Entrants may create original solutions, prototypes, datasets, scripts, or other content, materials, discoveries or inventions (a “Submission”). The Challenge is organised by the Challenge Organiser. Entrants retain ownership of all intellectual and industrial property rights (including moral rights) in and to Submissions. As a condition of submission, Entrant grants the Challenge Organiser, its subsidiaries, agents and partner companies, a perpetual, irrevocable, worldwide, royalty-free, and non-exclusive licence to use, reproduce, adapt, modify, publish, distribute, publicly perform, create a derivative work from, and publicly display the Submission. Entrants provide Submissions on an “AS IS” BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. "},{"title":"Modelling the scenario","type":0,"sectionRef":"#","url":"docs/cec1/cec1_scenario","content":"","keywords":""},{"title":"Simulating the audio signals received by the hearing aid​","type":1,"pageTitle":"Modelling the scenario","url":"docs/cec1/cec1_scenario#simulating-the-audio-signals-received-by-the-hearing-aid","content":"A listener – or receiver – is sitting or standing in a small room that has low to moderate reverberation. The person is listening to a target talker, who is selected from our set of 40 speakers. The target talker is producing one of our unique 7-10 word Clarity utterances. Simultaneously, an interferer sound is playing; this is either a competing talker or a continuous noise source (e.g., a washing machine). The target and interferer are at the same height as the listener. The room dimensions, boundary materials, and the locations of the listener, target and interferer are randomised (discussed below). An example scenario is shown in Figure 1. The room geometry showing origin location is defined in Figure 2. Example SceneRoom Geometry Figure 1. Example overview. Figure 3, below, shows the basic scene generator. The sound at the receiver is generated first by convolving the source signals with Binaural Room Impulse Responses (BRIRs). The reverberated speech and noise signals are then summed after appropriate gains are applied. The gains are set to achieve a Signal-to-Noise Ratio (SNR), which is chosen pseudo-randomly between limits. The BRIRs are generated using the RAVEN Geometric Room Acoustic Model [1]. There are additional signal paths and outputs generated that have been omitted from Figure 3 for clarity. In addition to the reverberated signals associated with the hearing aid microphones, the signal close to the eardrum is also generated. You can also access the reverberated speech and noise signals before they are mixed.  Figure 3. Simplified diagram of the scene generator. RIR refers to Room Impulse Response, HRTFs refers to Head Related Transfer Functions, SNRs are signal-to-noise ratios, and gain calc. indicates gain calculation. Dry here means anechoic. The outputs are noisy speech signals. "},{"title":"Room Geometry​","type":1,"pageTitle":"Modelling the scenario","url":"docs/cec1/cec1_scenario#room-geometry","content":"Cuboid rooms with dimensions length, LLL, by width, WWW, by height, HHH.Length LLL set using a uniform probability distribution random number generator with 3≤L(m)≤83 \\le L (m) \\le 83≤L(m)≤8.Height HHH set using a Gaussian distribution random number generator with a mean of 2.7m2.7 m2.7m and standard deviation of 0.8m0.8 m0.8m.Area L×WL \\times WL×W set using a Gaussian distribution random number generator with mean 17.7m217.7 m^217.7m2 and standard deviation of 5.5m25.5 m^25.5m2. "},{"title":"Room Materials​","type":1,"pageTitle":"Modelling the scenario","url":"docs/cec1/cec1_scenario#room-materials","content":"One of the walls of the room is randomly selected for the location of the door. The door can be at any position with the constraint of being at least at 20 cm from the corner of the wall. A window is placed on one of the other three walls. The window could be at any position of the wall but at 1.9 m height and at 0.4 m from any corner. The curtains are simulated to the side of the window. For larger rooms, a second window and curtains are simulated following a similar methodology. A sofa is simulated at a random position as a layer on the wall and the floor. Finally, a rug is simulated at a random location on the floor. "},{"title":"The receiver​","type":1,"pageTitle":"Modelling the scenario","url":"docs/cec1/cec1_scenario#the-receiver","content":"The receiver has position, r⃗=(xr,yr,zr)\\vec{r} = (x_r,y_r,z_r)r=(xr​,yr​,zr​) This is positioned within the room using uniform probability distribution random number generators for the x and y coordinates (see Figure 2 for origin location). There are constraints to ensure that the receiver is not too close to the wall: −W/2+1≤xr≤W/2−1-W/2+1 \\le x_r \\le W/2-1−W/2+1≤xr​≤W/2−11≤yr≤L−11 \\le y_r \\le L-11≤yr​≤L−1zrz_rzr​ either 1.2m1.2 m1.2m (sitting) or 1.6m1.6 m1.6m (standing). The receiver is positioned so as to be roughly facing the target talker. That is to say, within ±30\\pm 30±30 degrees of target. The angle = 7.5n7.5n7.5n where nnn is an integer and ∣n∣≤4|n| \\le 4∣n∣≤4. "},{"title":"The target talker​","type":1,"pageTitle":"Modelling the scenario","url":"docs/cec1/cec1_scenario#the-target-talker","content":"The target talker has position t⃗=(xt,yt,zt)\\vec{t} = (x_t,y_t,z_t)t=(xt​,yt​,zt​) The target talker is positioned within the room using uniform probability distribution random number generators for the coordinates. Constraints ensure the target is not too close to the wall or receiver. It is set to have the same height as the receiver. −W/2+1≤xt≤W/2−1-W/2+1 \\le x_t \\le W/2-1−W/2+1≤xt​≤W/2−11≤yt≤L−11 \\le y_t \\le L-11≤yt​≤L−1∣r−t∣&gt;1|r-t| &gt; 1∣r−t∣&gt;1zt=zrz_t=z_rzt​=zr​ A speech directivity pattern is used, which is directed at the listener. "},{"title":"The interferer​","type":1,"pageTitle":"Modelling the scenario","url":"docs/cec1/cec1_scenario#the-interferer","content":"The interferer has position i⃗=(xi,yi,zi)\\vec{i} = (x_i,y_i,z_i)i=(xi​,yi​,zi​) The interferer is a single point source radiating speech or non-speech noise omnidirectionally. It is placed within the room using uniform probability distribution random number generators for the coordinates. These constraints ensure the interferer is not too close to the wall or receiver. It is set to be at the same height as the receiver. Note, this means that the interferer can be at any angle relative to the receiver. −W/2+1≤xi≤W/2−1-W/2+1 \\le x_i \\le W/2-1−W/2+1≤xi​≤W/2−11≤yi≤L−11 \\le y_i \\le L-11≤yi​≤L−1∣r−i∣&gt;1|r-i| \\gt 1∣r−i∣&gt;1zi=zrz_i = z_rzi​=zr​ "},{"title":"Timing​","type":1,"pageTitle":"Modelling the scenario","url":"docs/cec1/cec1_scenario#timing","content":"The target sound starts 2 seconds after the start of the interferer. This is so the target is clear and unambiguously identifiable for listening tests. This also gives the hearing aid algorithms some time to adjust to the background noise.The interferer continues 1 second after the target has finished, so that all words in the target utterance can be masked. "},{"title":"Signal-to-Noise Ratio (SNR)​","type":1,"pageTitle":"Modelling the scenario","url":"docs/cec1/cec1_scenario#signal-to-noise-ratio-snr","content":"The mixtures are engineered such that the target utterances are at an appropriate level of intelligibility when processed by the default hearing aid software. This is achieved by scaling the interferer. Pilot tests have been conducted to get this approximately correct. Scaling is done this way because it does not require recomputing the BRIRs. Note that the interferer can be at any azimuth from the point of view of the listener/receiver. A desired signal-to-noise ratio, SNRD (dB), is chosen using a uniform probability distribution random number generator between the limits of ranges specified for the speech and non-speech interferers. The better ear SNR, here termed BE_SNR, which models the better ear effect in binaural listening, is calculated for the reference channel (channel 1, which corresponds to the front microphone of the hearing aid). This value is used to scale all interferer channels. The procedure is described below. For the reference channel, The segment of the interferer that overlaps with the target (without padding) , i‘, and the target (without padding), t‘, are extractedSpeech-weighted SNRs are calculated for each ear, SNRL and SNRR: Signals i‘ and t’ are separately convolved with a speech-weighting filter, h (specified below).The rms is calculated for each convolved signal.SNRL and SNRR are calculated as the ratio of these rms values. The BE_SNR is selected as the maximum of the two SNRs: BE_SNR = max(SNRL and SNRR). Then per channel, The whole interferer signal, i, is scaled by the BE_SNR i=i∗BESNRi = i*BE_{SNR}i=i∗BESNR​ Finally, i is scaled as follows: i=i∗10((−SNRD)/20)i = i*10^{((-SNR_D)/20)}i=i∗10((−SNRD​)/20) The speech-weighting filter is an FIR designed using the host window method [2, 3]. The specification is: Frequency (Hz) = [0, 150, 250, 350, 450, 4000, 4800, 5800, 7000, 8500, 9500, 22050];Magnitude of transfer function at each frequency = [0.0001, 0.0103, 0.0261, 0.0419, 0.0577, 0.0577, 0.046, 0.0343, 0.0226, 0.0110, 0.0001, 0.0001];   Figure 4, Speech weighting filter transfer function graph. "},{"title":"References​","type":1,"pageTitle":"Modelling the scenario","url":"docs/cec1/cec1_scenario#references","content":"Schröder, D. and Vorländer, M., 2011, January. RAVEN: A real-time framework for the auralization of interactive virtual environments. In Proceedings of Forum Acusticum 2011 (pp. 1541-1546). Denmark: Aalborg.Abed, A.H.M. and Cain, G.D., 1978. Low-pass digital filtering with the host windowing design technique. Radio and Electronic Engineer, 48(6), pp.293-300.Abed, A.E. and Cain, G., 1984. The host windowing technique for FIR digital filter design. IEEE transactions on acoustics, speech, and signal processing, 32(4), pp.683-694. "},{"title":"Contact","type":0,"sectionRef":"#","url":"docs/cpc1/cpc1_contact","content":"Contact Join the Clarity Challenge’s Google group Email the Clarity Team","keywords":""},{"title":"CEC2 Prizes","type":0,"sectionRef":"#","url":"docs/cec2/taking_part/cec2_prizes","content":"","keywords":""},{"title":"The Team Prize​","type":1,"pageTitle":"CEC2 Prizes","url":"docs/cec2/taking_part/cec2_prizes#the-team-prize","content":"Team prizes have been made available by the generosity of the Hearing Industry Research ConsortiumThere will be separate HASPI and listening test prizes for the top systems.  HASPI prize emoji_events 1st Place $1000 emoji_events 2nd Place $500 emoji_events 3rd Place $250 Listening Test prize emoji_events 1st Place $1000 emoji_events 2nd Place $500 emoji_events 3rd Place $250   info Anonymous entries and those with direct links to the Clarity project team are ineligible for cash prizes, sorry. "},{"title":"CEC2 Registration","type":0,"sectionRef":"#","url":"docs/cec2/taking_part/cec2_registration","content":"CEC2 Registration Teams are required to register using the form below. Please submit one form per team, providing a single contact email address. Once you have registered, you will receive an email confirmation with a team ID. When the submission date approaches, you will be sent an individualised link to a Google Drive for submitting materials. Loading… Registration closes on August 28th, but earlier registration will help us to plan for the listening tests.","keywords":""},{"title":"Contact Us","type":0,"sectionRef":"#","url":"docs/contact","content":"","keywords":""},{"title":"Send us an email​","type":1,"pageTitle":"Contact Us","url":"docs/contact#send-us-an-email","content":"You can contact the Clarity Team by email at claritychallengecontact@gmail.com "},{"title":"Join the Google group​","type":1,"pageTitle":"Contact Us","url":"docs/contact#join-the-google-group","content":"If you wish to stay updated with Clarity Challenges please sign up the Clarity Challenge’s Google group "},{"title":"Baseline System","type":0,"sectionRef":"#","url":"docs/cpc1/cpc1_baseline","content":"Baseline System Figure 1 is a simplified schematic of the baseline system, where not all signal paths are shown. A scene generator (blue box) creates the speech in noise (SPIN) that the hearing aid model then enhances (yellow box). This enhancement is individualised for each listener; hence, there is also a system to select a random listener (white ellipse) with a particular set of characteristics (e.g., audiograms). The SPIN that has been improved by the hearing aid is then passed to the prediction stage (orange box). This comprises two models: a hearing loss model, anda binaural speech intelligibility model. This prediction stage (orange box) is what we want you to improve on in this challenge. Figure 1 Simplified overview of the baseline. You are free to choose which parts of the baseline you use and reconfigure the system as you see fit. You can use our hearing loss model as part of your entry, or produce a single model that combines the hearing loss and speech intelligibility models. For an introduction to elements of the prediction model, please see our FAQ, which includes an overview of Speech intelligibility, andHearing loss and what hearing aids do. For the prediction challenge, most examples of the improved SPIN shown in the centre of the diagram come from hearing aid models created by the entrants to the first Enhancement Challenge. Therefore, most audio signals in the prediction challenge data were not processed by the baseline hearing aid model. More details of the different parts of the baseline appear on the software page. See the following sections: Scene GeneratorHearing aid modelHearing loss modelSpeech intelligibility model Download baseline software and data.","keywords":""},{"title":"Important dates","type":0,"sectionRef":"#","url":"docs/cpc1/cpc1_dates","content":"Important dates 16th November 2021: Launch of challenge, release of data.23rd November 2021: Webinar to introduce the challenge 15:00-17:00 UK time.1st March 2022: Release of evaluation data.21st March 2022: Submission deadline. All entrants submit their predictions plus a draft of their technical report. Scores will be returned with 24 hours of submission.28th March 2022: Deadline for Interspeech paper submission.25th April 2022: Deadline by which all entrants must submit two page technical reports to Clarity Prediction Challenge 2022 workshop.29th June 2022: Clarity Prediction Challenge 2022 workshop.Sept 18-22, 2022: Interspeech 2022 Special Session.","keywords":""},{"title":"CEC2 Submission","type":0,"sectionRef":"#","url":"docs/cec2/taking_part/cec2_submission","content":"","keywords":""},{"title":"What evaluation data is provided?​","type":1,"pageTitle":"CEC2 Submission","url":"docs/cec2/taking_part/cec2_submission#what-evaluation-data-is-provided","content":"The evaluation data consists of 1500 scenes. For each scene, you are provided with the signals received at each of the three microphones on the left and right hearing aid device. You will also be provided with JSON or csv formatted metadata consisting of the audiograms and DTT results for a set of listeners anda mapping of which listeners will listen to which scenes. There will also be some clean audio examples for the target talker, that are not the same as the target utterance. For the stage 1 HASPI evaluation, there will be one listener per scene and the scene-listener mapping will be the same for all teams. For the stage 2 listening test evaluation, there will be five listeners per scene and each team will have a separate scene-listener mapping. The file formats will be the same as used for the development data; for details see the CEC2 Data page. "},{"title":"What audio do I need to submit?​","type":1,"pageTitle":"CEC2 Submission","url":"docs/cec2/taking_part/cec2_submission#what-audio-do-i-need-to-submit","content":"You must submit the audio signals produced at the output of your simulated hearing aid for the evaluation datasets. You will be asked to provide two sets of signals: the first for the HASPI evaluation and the second for the listening tests (see submission dates above). Signals should be submitted as 16-bit WAV files with a 32 kHz sampling rate, and 0 dB FS corresponds to 100 dB SPL, given the capabilities of the listening test reproduction equipment. The format of signals submitted for HASPI evaluation and for the listening tests is the same. We also encourage you to submit your simulated hearing aid code. See the page on listening tests for more information about the levels that can be reproduced by the listening test equipment. When playing signals to listeners we will then play them as is. The responsibility for the final signal level is therefore yours. It’s worth bearing in mind that should your signals overall seem too loud to be comfortable to a participant, they may well turn down the volume themselves. "},{"title":"Naming and packaging signals​","type":1,"pageTitle":"CEC2 Submission","url":"docs/cec2/taking_part/cec2_submission#naming-and-packaging-signals","content":"Your processed signals should be named using the conventions used by the baseline system, i.e., &lt;Scene ID&gt;_&lt;Listener ID&gt;_HA-output.wav and explained on the CEC2 data page. These should be placed in a directory whose name is the unique team ID that you will be sent, e.g., E001 and then packaged using zip or tar or any standard packaging tool. The resulting file should be about 2 GB for the first round. "},{"title":"Technical report​","type":1,"pageTitle":"CEC2 Submission","url":"docs/cec2/taking_part/cec2_submission#technical-report","content":"The two page technical report must be submitted as a paper to the Clarity-2022 Workshop. Deadline - see date above. An author kit and submission instructions will be made available.A draft of the report needs to be uploaded to the Google Drive along with your HASPI signals - see above for deadline. The draft needs to be sufficiently complete for us to judge whether your system is compliant with the challenge rules.Your report should include an abstract and introduction and sections on experimental setup/methodology including system information and model/network architecture, evaluation/results, discussion, conclusion and references. Please provide an estimation of the computational resources needed. You must describe any external data and pre-existing tools, software and models used. "},{"title":"How will intellectual property be handled?​","type":1,"pageTitle":"CEC2 Submission","url":"docs/cec2/taking_part/cec2_submission#how-will-intellectual-property-be-handled","content":"See here under Intellectual Property. "},{"title":"Where do I submit the signals?​","type":1,"pageTitle":"CEC2 Submission","url":"docs/cec2/taking_part/cec2_submission#where-do-i-submit-the-signals","content":"When you have registered you will receive a link to a Google Drive to which you will be able to securely upload your signals. You will be able to use the same link to upload materials for both the 1st submission and the 2nd submission if you are selected for the 2nd round. We also encourage you to submit your simulated hearing aid code via this link. Materials uploaded will be visible to the Clarity Team but not to other entrants. danger Note, in order to use the Google Drive you will need to have a Google account. If you anticipate problems using Google then please make arrangements to send us the materials by other means, e.g., via a service such as WeTransfer or similar. "},{"title":"Prizes","type":0,"sectionRef":"#","url":"docs/cpc1/cpc1_prizes","content":"","keywords":""},{"title":"The Team Prize​","type":1,"pageTitle":"Prizes","url":"docs/cpc1/cpc1_prizes#the-team-prize","content":"There will be separate prizes for the top contributions by students and non-students.There will be a separate prize for the best performing non-intrusive model.Students eligible for the prize are expected to have made a significant contribution and be first author on the workshop paper.Team prizes have been made available by the generosity of the Hearing Industry Research Consortium.  General prize emoji_events 1st Place $1000 emoji_events 2nd Place $500 emoji_events 3rd Place $250 Student prize emoji_events 1st Place $1000 emoji_events 2nd Place $500 emoji_events 3rd Place $250  info The 1st Clarity Prediction Challenge has now finished. For the details of the systems submitted, results and prize winners, please visit the Clarity-2022 Workshsop website. "},{"title":"The 1st Clarity Prediction Challenge","type":0,"sectionRef":"#","url":"docs/cpc1/cpc1_intro","content":"","keywords":""},{"title":"Key dates (updated 14/01/22)​","type":1,"pageTitle":"The 1st Clarity Prediction Challenge","url":"docs/cpc1/cpc1_intro#key-dates-updated-140122","content":"16th November 2021: Launch of challenge, release of data.23rd November 2021: Webinar to introduce the challenge 15:00-17:00 UK time.1st March 2022: Release of evaluation data.21st March 2022: Submission deadline. All entrants submit their predictions plus a draft of their technical report (details below). Scores will be returned with 24 hours of submission.28th March 2022: Deadline for Interspeech paper submission.25th April 2022: Deadline by which all entrants must submit two page technical reports to Clarity Prediction Challenge 2022 workshop.29th June 2022: Clarity Prediction Challenge 2022 workshop.Sept 18-22, 2022: Interspeech 2022 Special Session. "},{"title":"More details​","type":1,"pageTitle":"The 1st Clarity Prediction Challenge","url":"docs/cpc1/cpc1_intro#more-details","content":"Scenario - a description of the listening scenario and how it has been simulated. Baseline System - a description of the baseline software model. Data - the data that can be used to train and evaluate your system during development. Software - the software tools that we are providing to help you build and evaluate a challenge entry. Challenge Rules - the rules to which all challenge entries must adhere. Submission - information about how to prepare your submission. Prizes - information about our prizes. Download - where to go to download the software and challenge data. Find a team - if you'd like to find collaborators to help you compete. FAQ - an extensive FAQ answering key questions and providing background knowledge to help you compete. "},{"title":"Download","type":0,"sectionRef":"#","url":"docs/cpc1/cpc1_download","content":"Download The following challenge data are available for download: The challenge training data is available for download as a single 13 GB file, clarity_CPC1_data.v1_1.tgzThe evaluation data in now available (1st March) for download as a single 6 GB file, clarity_CPC1_data.test.v1.tgz. The evaluation data should be untarred into the same root as the training data. The Github repository containing the baseline code is here. The repository contains code for CPC1 and also for the earlier enhancement challenge CEC1. You will find all the necessary instructions for installing the data and setting up the baseline system: i.e. running the MSBG hearing loss model and MBSTOI intelligibility prediction stage. We will be making a further small release in early December to specify the final evaluation metrics that we will be using to rank entries. info The Challenge is now closed but the data is still available for anyone to use. If using the data please cite the following paper Jon Barker and Michael Akeroyd and Trevor J. Cox and John F. Culling and Jennifer Firth and Simone Graetzer and Holly Griffiths and Lara Harris and Graham Naylor and Zuzanna Podwinska and Eszter Porter and Rhoddy Viveros Munoz, “The 1st Clarity Prediction Challenge: A machine learning challenge for hearing aid intelligibility prediction,” in Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH 2022, Incheon, South Korea, 2022.","keywords":""},{"title":"FAQ","type":0,"sectionRef":"#","url":"docs/cpc1/cpc1_faq","content":"","keywords":""},{"title":"Speech Intelligibility​","type":1,"pageTitle":"FAQ","url":"docs/cpc1/cpc1_faq#speech-intelligibility","content":""},{"title":"What is Speech Intelligibility?​","type":1,"pageTitle":"FAQ","url":"docs/cpc1/cpc1_faq#what-is-speech-intelligibility","content":"The term Speech Intelligibility is generally used in two different ways. It can refer to how much speech is understood by a listener, or to the number of words correctly identified by a listener as a proportion or percentage of the total number of words. In the Clarity project, we are using the latter definition, i.e., the percentage of words in a sentence that a listener identified correctly. This percentage is the target for your prediction models. Speech intelligibility captures how a listener's ability to participate in conversation is changed when the speech signal is degraded, e.g., by background noise and room reverberation, or is processed, e.g., by a hearing aid. Your prediction model will need to incorporate a model of the hearing abilities of each listener. "},{"title":"How is Speech Intelligibility measured with listeners?​","type":1,"pageTitle":"FAQ","url":"docs/cpc1/cpc1_faq#how-is-speech-intelligibility-measured-with-listeners","content":"In the Clarity project, a set of listeners listen to a sentence and then say what words they heard. In this project, speech intelligibility is measured as the number of words identified correctly as a percentage of the total number of words in a sentence. You might consider looking at other metrics, such as Word Error Rate (WER), which picks up on, e.g., where listeners insert words not in the original sentence. You might do this if you think that an estimate of WER or other metrics would help your system to estimate speech intelligibility, as defined in the Clarity project. "},{"title":"How is Speech Intelligibility objectively measured by a computer?​","type":1,"pageTitle":"FAQ","url":"docs/cpc1/cpc1_faq#how-is-speech-intelligibility-objectively-measured-by-a-computer","content":"When fitting a hearing aid, it would be beneficial for an audiologist to be able to use an objective measure of speech intelligibility to determine what signal processing algorithm(s) should be used to compensate for the listener's hearing impairment. Objective measures are also useful when measured speech intelligibility scores are unavailable, such as when developing a machine learning-based hearing aid algorithm or some other speech enhancement method. Another advantage of non-intrusive measures is that they do not require time-alignment of processed and reference signals. Objective measures - or metrics - of speech intelligibility are used to allow a computer to estimate the likely performance of humans in listening tests. The main goal of entries to the prediction challenge is to produce one of these measures that performs well for listeners with hearing loss. There are two broad classes of speech intelligibility models: Intrusive metrics (also known as double-ended) are most common. This is where the intelligibility is estimated by comparing the degraded or processed speech signal with the original clean speech signal.Non-intrusive metrics (also known as single-ended or blind) are less well developed. This is where intelligibility is estimated from the degraded or processed speech signal alone. In the Clarity project, both types of metrics are of interest. Intrusive metrics will be more accurate in many cases. However, there are hearing aid processes where the speech content is shifted in frequency, which will defeat most current intrusive speech intelligibility metrics. We also hypothesise that there might be issues with intrusive metrics and machine learning approaches in hearing aids that revoice the original speech. "},{"title":"What speech intelligibility models already exist and what are they used for?​","type":1,"pageTitle":"FAQ","url":"docs/cpc1/cpc1_faq#what-speech-intelligibility-models-already-exist-and-what-are-they-used-for","content":"There aren't many speech intelligibility models that consider hearing impairment, but one that does is HASPI by Kates and Arehart. In this seminar from the first Clarity workshop, James Kates discusses speech intelligibility models with a focus on the ones he has developed. He also discusses the speech quality metric HASQI. If you're interested in using HASPI or HASQI for the challenge, James Kates has kindly made the MATLAB code and user guide available for download.  Click arrow to see synopsis. Signal degradations, such as additive noise and nonlinear distortion, can reduce the intelligibility and quality of a speech signal. Predicting intelligibility and quality for hearing aids is especially difficult since these devices may contain intentional nonlinear distortion designed to make speech more audible to a hearing-impaired listener. This speech processing often takes the form of time-varying multichannel gain adjustments. Intelligibility and quality metrics used for hearing aids and hearing-impaired listeners must therefore consider the trade-offs between audibility and distortion introduced by hearing-aid speech envelope modifications. This presentation uses the Hearing Aid Speech Perception Index (HASPI) and the Hearing Aid Speech Quality Index (HASQI) to predict intelligibility and quality, respectively. These indices incorporate a model of the auditory periphery that can be adjusted to reflect hearing loss. They have been trained on intelligibility scores and quality ratings from both normal-hearing and hearing-impaired listeners for a wide variety of signal and processing conditions. The basics of the metrics are explained, and the metrics are then used to analyse the effects of additive noise on speech, to evaluate noise suppression algorithms, and to measure differences among commercial hearing aids. "},{"title":"Hearing Loss​","type":1,"pageTitle":"FAQ","url":"docs/cpc1/cpc1_faq#hearing-loss","content":"There are many types of hearing loss, but the focus of the Clarity project is the hearing loss that happens with ageing. This is a form of sensorineural hearing loss. "},{"title":"How does hearing loss affect the perception of audio signals, and how do modern hearing aids process sound to help with this?​","type":1,"pageTitle":"FAQ","url":"docs/cpc1/cpc1_faq#how-does-hearing-loss-affect-the-perception-of-audio-signals-and-how-do-modern-hearing-aids-process-sound-to-help-with-this","content":"In this seminar from the first Clarity workshop, Karolina Smeds from ORCA Europe and WS Audiology discusses the effects of hearing loss and the hearing aid processing strategies that are typically used to counter the sensory deficits.  Click arrow to see synopsis. Hearing loss leads to several unwanted effects. Loss of audibility for soft sounds is one effect, but also when amplification is used to create audibility for soft sounds, many [suprathreshold](https://www.lexico.com/en/definition/suprathreshold) deficits remain. The most common type of hearing loss is a [cochlear](https://www.lexico.com/definition/cochlear) hearing loss, where haircells or nerve synapses in the cochlea are damaged. Ageing and noise exposure are the most common causes of cochlear hearing loss. This type of hearing loss is associated with atypical loudness perception and difficulties in noisy situations. Background noise masks for instance speech to a higher degree than for a person with healthy hair cells. This explains why listening to speech-in-noise (SPIN) is such an important topic to work on. A brief introduction to signal processing in hearing aids will be presented. With the use of frequency-specific amplification and compression (automatic gain control, AGC), hearing aids are usually doing a good job in compensating for reduced audibility and for atypical suprathreshold loudness perception. However, it is more difficult to compensate for the increased masking effect. Some examples of strategies will be presented. Finally, natural conversations in noise will be discussed. The balance between being able to have a conversation with a specific communication partner in a group of people and being able to switch attention if someone else starts to talk will be touched upon. "},{"title":"Prediction model​","type":1,"pageTitle":"FAQ","url":"docs/cpc1/cpc1_faq#prediction-model","content":""},{"title":"Do I have to use a separate hearing loss model?​","type":1,"pageTitle":"FAQ","url":"docs/cpc1/cpc1_faq#do-i-have-to-use-a-separate-hearing-loss-model","content":"No is the short answer! In the baseline, we've used the Cambridge hearing loss model and a separate binaural speech intelligibility model. Another approach would be to create a single model that encapsulates the combined effects of hearing loss and speech perception. "},{"title":"What should the output of my prediction model be?​","type":1,"pageTitle":"FAQ","url":"docs/cpc1/cpc1_faq#what-should-the-output-of-my-prediction-model-be","content":"The output should include a predicted speech intelligibility score per input signal, specifically, an estimate of the number of words correct as a percentage of the total number of words in the signal. "},{"title":"Data​","type":1,"pageTitle":"FAQ","url":"docs/cpc1/cpc1_faq#data","content":""},{"title":"Do you have suggestions for expanding the training data?​","type":1,"pageTitle":"FAQ","url":"docs/cpc1/cpc1_faq#do-you-have-suggestions-for-expanding-the-training-data","content":"The prediction challenge data is limited by having to get the ground truth from listening tests on people with a hearing loss. We look forward to seeing what approaches teams use to help overcome this limitation, such as using unsurpervised models, data augmentation or generating additional ground truth data using a pre-existing model. The baseline model includes a hearing loss and speech intelligibility model that could be used for creating additional pre-training data. There are other models that you might consider where code is available. None has been checked by the Clarity team. Katerina Zmolikova has made her Pytorch version of the baseline hearing impairment and speech intelligibility model available. Both model fit a neural network framework, are faster but more approximate (see graphs on github).HASQI and HASPI are quality and speech intelligibility metrics designed to work for people with a hearing impairment. James Kates explains more about these above. MATLAB code HASPI v2 and HASQI v2 are available, along with the user guide.STOI-Net: A Deep Learning based Non-Intrusive Speech Intelligibility Assessment Model by Ryandhimas Zezario et al. is monaural and non-intrusive using Python, Keras and TensorFlow. It doesn't model the effect of hearing loss. An alternative is Asger Heidemann Andersen's MATLAB code. "},{"title":"Missing data​","type":1,"pageTitle":"FAQ","url":"docs/cpc1/cpc1_faq#missing-data","content":"We have audiograms for all our listening panel. But for other characterisations of hearing, only some of the panel have provided data. Therefore there is missing data that has to be dealt with. 1) One approach to the missing data is to just ignore it and just use the audiograms. The problem with this approach is that audiograms only quantifies the hearing threshold, and our speech in noise audio samples were not played that quietly. Nevertheless, audiograms are the most common way of characterising hearing loss. 2) Alternatively, a method to use the partial data could be developed, and we expect this would help with speech intelligibility prediction. You will find plenty of data science blog posts, e.g. towards data science discussing different approaches. A key question is whether the missing data is 'missing at random' i.e. is the distribution of the missing data expected to be the same as that of the present data? For the prediction challenge, this would mean the missing triple-digit-test values are coming from some random sample of the listeners, who'd be no different from the listeners who did complete the triple-digital-test. Unfortunately, this might not be true, because the failure to complete the triple-digit-tests could well correlate with hearing loss (e.g. maybe older people with more hearing loss were less likely to do the test). The Clarity data is probably 'missing not at random'. One simple solution is to delete examples with missing data, but the loss of so much data probably makes this undesirable. A more sophisticated approach is to fill gaps in data via imputation i.e. first estimate values for the missing data and then treat the dataset as complete. A couple of simple approaches for imputation are: (i) use the mean value from the dataset for missing values, and (ii) create a model to estimate the missing data from the audiograms. There are other approaches in data science that could be exploited such as coding the missing values into a 'N/A' category within the input data. "},{"title":"Submission","type":0,"sectionRef":"#","url":"docs/cpc1/cpc1_submission","content":"","keywords":""},{"title":"Registration​","type":1,"pageTitle":"Submission","url":"docs/cpc1/cpc1_submission#registration","content":"Teams are required to register to help us organise the challenge. Registered teams will be assigned a unique team ID. "},{"title":"What evaluation data is provided?​","type":1,"pageTitle":"Submission","url":"docs/cpc1/cpc1_submission#what-evaluation-data-is-provided","content":"The evaluation data consists of audio signals processed by hearing aid systems, clean reference signals, listener metadata, and a mapping of which listeners listened to which scenes/hearing aid systems. The evaluation data is available for download here clarity_CPC1_data.test.v1.tgz. See the download page for more details. Note, the evaluation data does not contain the listener responses. We will score your submission for you and return your score (we aim to do this within 24 hours for of submission). We will then release the true listener responses the day after the submission deadline to allow teams to perform analysis of their results. "},{"title":"What do I need to submit?​","type":1,"pageTitle":"Submission","url":"docs/cpc1/cpc1_submission#what-do-i-need-to-submit","content":"All teams must submit Their predicted intelligibility scoresAn Interspeech paper describing their work (encouraged)A two page technical report (mandatory) "},{"title":"The predicted intellgibility scores​","type":1,"pageTitle":"Submission","url":"docs/cpc1/cpc1_submission#the-predicted-intellgibility-scores","content":"You must submit your predicted intelligibility scores for the signals provided. The predictions should be sent in CSV format files with two columns: signal_ID, intelligibility_score  Where the signal_ID is the unique signal identifier used for the wav file name (e.g., S08510_L0239_E001) and intelligibility_score is the predicted intelligibilty given in terms of the percentage words recognised correctly for the signal (i.e., for 0 to 100). Your CSV files should be named as follows CPC1_&lt;TEAM_ID&gt;.test.csv and CPC1_&lt;TEAM_ID&gt;.test_indep.csv for closed set and open set evaluations respectively, where &lt;TEAM_ID&gt; is your individual team ID, e.g. 'E001'. The files should be sent as email attachments to the email address: claritychallengecontact@gmail.com Please use &quot;CPC1 Submission &lt;TEAM_ID&gt;&quot; as the subject line. We also encourage you to submit your prediction model(s) code. info All registered teams will be emailed with their unique team ID shortly before the submission deadline. If you plan to submit please register before the submission deadline. "},{"title":"Interspeech paper submission​","type":1,"pageTitle":"Submission","url":"docs/cpc1/cpc1_submission#interspeech-paper-submission","content":"All teams are strongly encouraged to submit a paper describing their work to the Interspeech 2022 Special Session &quot;Speech Intelligibility Prediction for Hearing-Impaired Listeners&quot;. Interspeech submission instructions are here https://interspeech2022.org/forauthor/submissions.php The Interspeech papers need to be initially submitted by March 21st (title and abstract), with the full paper due on March 28th. "},{"title":"The technical report​","type":1,"pageTitle":"Submission","url":"docs/cpc1/cpc1_submission#the-technical-report","content":"The two page technical report must be submitted as a paper to the Clarity-CPC1-2022 Workshop. Deadline 25th April 2022. An author kit and submission instructions will be made available. A draft of the report needs to be submitted along with your predictions by 21st March. The draft needs to be sufficiently complete for us to judge whether your system(s)/model(s) is compliant with the challenge rules. You can find a list of key challenge dates here. Your report should include an abstract and introduction and sections on experimental setup/methodology including system/model information and model/network architecture, evaluation/results, discussion, conclusion and references. Please provide an estimation of the computational resources needed. You must describe any external data and pre-existing tools, software and models used. Please make it clear how your system(s)/model(s) meets the challenge rules. Your report should cite the following document, which provides an overview of the challenge and the baseline system: Jon Barker and Michael Akeroyd and Trevor J. Cox and John F. Culling and Jennifer Firth and Simone Graetzer and Holly Griffiths and Lara Harris and Graham Naylor and Zuzanna Podwinska and Eszter Porter and Rhoddy Viveros Munoz, “The 1st Clarity Prediction Challenge: A machine learning challenge for hearing aid intelligibility prediction,” in Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH 2022, Incheon, South Korea, 2022. The document can be accessed here. "},{"title":"How will intellectual property be handled?​","type":1,"pageTitle":"Submission","url":"docs/cpc1/cpc1_submission#how-will-intellectual-property-be-handled","content":"See here under Intellectual Property. "},{"title":"Registration","type":0,"sectionRef":"#","url":"docs/cpc1/cpc1_registration","content":"","keywords":""},{"title":"Registration​","type":1,"pageTitle":"Registration","url":"docs/cpc1/cpc1_registration#registration","content":"Please use this Google form to register. Please submit one form per team, i.e., providing a single contact email address. Once you have registered, you will receive an email confirmation with a team ID. Please register early to help us organise the challenge. "},{"title":"Google group​","type":1,"pageTitle":"Registration","url":"docs/cpc1/cpc1_registration#google-group","content":"If you haven't done so already, please sign up to Clarity's Google group to keep up to date with the challenges. "},{"title":"Rules","type":0,"sectionRef":"#","url":"docs/cpc1/cpc1_rules","content":"","keywords":""},{"title":"Teams​","type":1,"pageTitle":"Rules","url":"docs/cpc1/cpc1_rules#teams","content":"Teams must have registered and nominated a contact person.Teams can be from one or more institutions.Teams can comprise up to 10 persons.The organisers - and any person forming a team with one or more organisers - may enter the challenge themselves but will not be eligible to win the cash prizes. "},{"title":"Transparency​","type":1,"pageTitle":"Rules","url":"docs/cpc1/cpc1_rules#transparency","content":"Teams must provide a technical document of up to 2 pages describing the system/model and any external data and pre-existing tools, software and models used.We will publish all technical documents (anonymous or otherwise).Teams are encouraged – but not required – to provide us with access to the system(s)/model(s) and to make their code open source.Anonymous entries are allowed but will not be eligible for cash prizes.If a group of people submits multiple entries, they cannot win more than one prize in a given category.All teams will be referred to using anonymous codenames if the rank ordering is published before the final results are announced. "},{"title":"Intellectual property​","type":1,"pageTitle":"Rules","url":"docs/cpc1/cpc1_rules#intellectual-property","content":"The following terms apply to participation in this machine learning challenge (“Challenge”). Entrants may create original solutions, prototypes, datasets, scripts, or other content, materials, discoveries or inventions (a “Submission”). The Challenge is organised by the Challenge Organiser. Entrants retain ownership of all intellectual and industrial property rights (including moral rights) in and to Submissions. As a condition of submission, Entrant grants the Challenge Organiser, its subsidiaries, agents and partner companies, a perpetual, irrevocable, worldwide, royalty-free, and non-exclusive license to use, reproduce, adapt, modify, publish, distribute, publicly perform, create a derivative work from, and publicly display the Submission. Entrants provide Submissions on an “AS IS” BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. "},{"title":"What information can I use?​","type":1,"pageTitle":"Rules","url":"docs/cpc1/cpc1_rules#what-information-can-i-use","content":""},{"title":"Training and development​","type":1,"pageTitle":"Rules","url":"docs/cpc1/cpc1_rules#training-and-development","content":"For Track 1 (closed-set), teams should use the signals and listener responses provided in the `CPC1.train.json` file.Track 2 (open-set), teams should use the signals and listener responses provided in the smaller `CPC1.train_indep.json`. In addition, teams can use their own data for training or expand the training data through simple automated modifications. Additional pre-training data could be generated by existing speech intelligibility and hearing loss models. The FAQ gives links to some models that might be used for this. Any audio or metadata can be used during training and development, but during evaluation the prediction model(s) will not have access to all of the data (see next section). "},{"title":"Evaluation​","type":1,"pageTitle":"Rules","url":"docs/cpc1/cpc1_rules#evaluation","content":"The only data that can be used by the prediction model(s) during evaluation are The output of the hearing aid processor/system.The target convolved with the anechoic BRIR (channel 1) for each ear (‘target_anechoic’).The IDs of the listeners assigned to the scene/hearing aid system in the metadata provided.The listener metadata.The prompt for the utterances (the text the actors were given to read) If you use text from the speech prompts as part of evaluating the systems, we will classify that as an intrusive method for the purpose of awarding prizes. "},{"title":"Baseline models and computational restrictions​","type":1,"pageTitle":"Rules","url":"docs/cpc1/cpc1_rules#baseline-models-and-computational-restrictions","content":"Teams may choose to use all or some of the provided baseline models.There is no limit on computational cost.Models can be non-causal. "},{"title":"What sort of model do I create?​","type":1,"pageTitle":"Rules","url":"docs/cpc1/cpc1_rules#what-sort-of-model-do-i-create","content":"You can create either a single prediction model that calculates speech intelligibility given a listener's hearing characteristics (that is, the metadata provided), or you can submit separate models of hearing loss and speech intelligibility.You should report the speech intelligibility for the whole sentence for each audio sample/listener combination. "},{"title":"Submitting multiple entries​","type":1,"pageTitle":"Rules","url":"docs/cpc1/cpc1_rules#submitting-multiple-entries","content":"If you wish to submit multiple entries, All systems/models must be submitted for evaluation.Your systems must have significant differences in their approach.You must register multiple teams, submitting each entry as a different team.In your documentation, you must make it clear how the submissions differ. "},{"title":"Evaluation of systems​","type":1,"pageTitle":"Rules","url":"docs/cpc1/cpc1_rules#evaluation-of-systems","content":"Entries will be ranked according to their performance in predicting measured intelligibility scores. "},{"title":"Baseline system","type":0,"sectionRef":"#","url":"docs/cpc2/cpc2_baseline","content":"","keywords":""},{"title":"References​","type":1,"pageTitle":"Baseline system","url":"docs/cpc2/cpc2_baseline#references","content":" Kates, J.M. and Arehart, K.H., 2021. The hearing-aid speech perception index (haspi) version 2. Speech Communication, 131, pp.35-46. "},{"title":"Software","type":0,"sectionRef":"#","url":"docs/cpc1/cpc1_software","content":"","keywords":""},{"title":"A. Scene generator​","type":1,"pageTitle":"Software","url":"docs/cpc1/cpc1_software#a-scene-generator","content":"The scene generator is fully open-source python code for generating hearing aid inputs for each scene Inputs: target and interferer signals, BRIRs, RAVEN project (rpf) files, scene description JSON filesOutputs: Mixed target+interferer signals for each hearing aid channel, direct path (simulating a measurement close to the eardrum). Reverberated pre-mixed signals can also be optionally generated. "},{"title":"B. Baseline hearing aid processor​","type":1,"pageTitle":"Software","url":"docs/cpc1/cpc1_software#b-baseline-hearing-aid-processor","content":"The baseline hearing aid processor is based on openMHA [1] but with a Python wrapper. The python code configures openMHA with a Camfit compressive fitting [2] for a specific listener’s audiogram. This configuration of openMHA includes multiband dynamic compression, non-adaptive differential processing and a softclip plugin. The intention was to produce a basic hearing aid without various aspects of signal processing that are common in high-end hearing aids, but tend to be implemented in proprietary forms so cannot be replicated exactly. The main inputs and outputs for the processor are as follows: Inputs: Mixed scene signals for each hearing aid channel, a listener ID drawn from scene-listener pairs identified in ‘scenes_listeners.json’ and an entry in the listener metadata json file ‘listeners.json’ for that IDOutputs: The stereo hearing aid output signal, &lt;scene&gt;_&lt;listener&gt;_HA-output.wav "},{"title":"C. Hearing Loss model​","type":1,"pageTitle":"Software","url":"docs/cpc1/cpc1_software#c-hearing-loss-model","content":"Open-source python implementation of a hearing loss model developed by Brian Moore, Michael Stone and other members of the Auditory Perception Group, University of Cambridge (e.g., [3]). Inputs: A stereo wav audio signal, e.g., the output of the baseline hearing aid processor, and a set of audiograms (both L and R ears).Outputs: The signal after simulating the hearing loss as specified by the set of audiograms (stereo wav file), &lt;scene&gt;_&lt;listener&gt;_HL-output.wav "},{"title":"D. Speech Intelligibility model​","type":1,"pageTitle":"Software","url":"docs/cpc1/cpc1_software#d-speech-intelligibility-model","content":"Python implementation of a binaural intelligibility model, Modified Binaural Short-Time Objective Intelligibility (MBSTOI; [4]). This is an experimental baseline tool that is level-independent. Note that MBSTOI requires signal time-alignment (and alignment within one-third octave bands). Inputs: HL-model output signals, audiogram, reference target signal (i.e., the premixed target signal convolved with the BRIR with the reflections “turned off”, specified as ‘target_anechoic’), (scene metadata)Outputs: predicted intelligibility score  "},{"title":"References​","type":1,"pageTitle":"Software","url":"docs/cpc1/cpc1_software#references","content":"Kayser, H., Herzke, T., Maanen, P., Pavlovic, C. and Hohmann, V., 2019. Open Master Hearing Aid (openMHA): An integrated platform for hearing aid research. Journal of the Acoustical Society of America, 146(4), pp. 2879-2879.Moore, B. C. J., Alcantara, J. I., Stone, M. and Glasberg, B. R., 1999. Use of a loudness model for hearing aid fitting: II. Hearing aids with multi-channel compression. British Journal of Audiology, 33(3), pp. 157-170.Nejime, Y. and Moore, B. C., 1997. Simulation of the effect of threshold elevation and loudness recruitment combined with reduced frequency selectivity on the intelligibility of speech in noise. Journal of the Acoustical Society of America, 102(1), pp. 603-615.Andersen, A. H., de Haan, J. M., Tan, Z. H. and Jensen, J., 2018. Refinement and validation of the binaural short-time objective intelligibility measure for spatially diverse conditions. Speech Communication, 102, pp. 1-13. "},{"title":"Obtaining the data","type":0,"sectionRef":"#","url":"docs/cpc2/taking_part/cpc2_download","content":"Obtaining the data The following challenge data are available for download: The challenge data is available for download as a single 11 GB file, clarity_CPC2_data.v1_1.tgz.The evaluation data is now available for download as a single 478 MB file, clarity_CPC2_data.test.v1_0.tgz. The evaluation data should be untarred into the same root as the training data. The Github repository containing the baseline code is here. The repository contains code for CPC2 and also for the earlier enhancement and prediction challenges, i.e., CEC1, CEC2 and CPC1. You will find all the necessary instructions for installing the data and running the baseline system.","keywords":""},{"title":"FAQ for CPC2","type":0,"sectionRef":"#","url":"docs/cpc2/cpc2_faq","content":"","keywords":""},{"title":"Speech Intelligibility​","type":1,"pageTitle":"FAQ for CPC2","url":"docs/cpc2/cpc2_faq#speech-intelligibility","content":""},{"title":"What is Speech Intelligibility?​","type":1,"pageTitle":"FAQ for CPC2","url":"docs/cpc2/cpc2_faq#what-is-speech-intelligibility","content":"The term Speech Intelligibility is generally used in two different ways. It can refer to how much speech is understood by a listener, or to the number of words correctly identified by a listener as a proportion or percentage of the total number of words. In the Clarity project, we are using the latter definition, i.e., the percentage of words in a sentence that a listener identified correctly. This percentage is the target for your prediction models. Speech intelligibility captures how a listener's ability to participate in conversation is changed when the speech signal is degraded, e.g., by background noise and room reverberation, or is processed, e.g., by a hearing aid. Your prediction model will need to incorporate a model of the hearing abilities of each listener. "},{"title":"How is Speech Intelligibility measured with listeners?​","type":1,"pageTitle":"FAQ for CPC2","url":"docs/cpc2/cpc2_faq#how-is-speech-intelligibility-measured-with-listeners","content":"In the Clarity project, a set of listeners listen to a sentence and then say what words they heard. In this project, speech intelligibility is measured as the number of words identified correctly as a percentage of the total number of words in a sentence. You might consider looking at other metrics, such as Word Error Rate (WER), which picks up on, e.g., where listeners insert words not in the original sentence. You might do this if you think that an estimate of WER or other metrics would help your system to estimate speech intelligibility, as defined in the Clarity project. "},{"title":"How is Speech Intelligibility objectively measured by a computer?​","type":1,"pageTitle":"FAQ for CPC2","url":"docs/cpc2/cpc2_faq#how-is-speech-intelligibility-objectively-measured-by-a-computer","content":"When fitting a hearing aid, it would be beneficial for an audiologist to be able to use an objective measure of speech intelligibility to determine what signal processing algorithm(s) should be used to compensate for the listener's hearing impairment. Objective measures are also useful when measured speech intelligibility scores are unavailable, such as when developing a machine learning-based hearing aid algorithm or some other speech enhancement method. Another advantage of non-intrusive measures is that they do not require time-alignment of processed and reference signals. Objective measures - or metrics - of speech intelligibility are used to allow a computer to estimate the likely performance of humans in listening tests. The main goal of entries to the prediction challenge is to produce one of these measures that performs well for listeners with hearing loss. There are two broad classes of speech intelligibility models: Intrusive metrics (also known as double-ended) are most common. This is where the intelligibility is estimated by comparing the degraded or processed speech signal with the original clean speech signal.Non-intrusive metrics (also known as single-ended or blind) are less well developed. This is where intelligibility is estimated from the degraded or processed speech signal alone. In the Clarity project, both types of metrics are of interest. Intrusive metrics will be more accurate in many cases. However, there are hearing aid processes where the speech content is shifted in frequency, which will defeat most current intrusive speech intelligibility metrics. We also hypothesise that there might be issues with intrusive metrics and machine learning approaches in hearing aids that revoice the original speech. "},{"title":"What speech intelligibility models already exist and what are they used for?​","type":1,"pageTitle":"FAQ for CPC2","url":"docs/cpc2/cpc2_faq#what-speech-intelligibility-models-already-exist-and-what-are-they-used-for","content":"There aren't many speech intelligibility models that consider hearing impairment, but one that does is HASPI by Kates and Arehart. In this seminar from the first Clarity workshop, James Kates discusses speech intelligibility models with a focus on the ones he has developed. He also discusses the speech quality metric HASQI. If you're interested in using HASPI or HASQI for the challenge, James Kates has kindly made the MATLAB code and user guide available for download.  Click arrow to see synopsis. Signal degradations, such as additive noise and nonlinear distortion, can reduce the intelligibility and quality of a speech signal. Predicting intelligibility and quality for hearing aids is especially difficult since these devices may contain intentional nonlinear distortion designed to make speech more audible to a hearing-impaired listener. This speech processing often takes the form of time-varying multichannel gain adjustments. Intelligibility and quality metrics used for hearing aids and hearing-impaired listeners must therefore consider the trade-offs between audibility and distortion introduced by hearing-aid speech envelope modifications. This presentation uses the Hearing Aid Speech Perception Index (HASPI) and the Hearing Aid Speech Quality Index (HASQI) to predict intelligibility and quality, respectively. These indices incorporate a model of the auditory periphery that can be adjusted to reflect hearing loss. They have been trained on intelligibility scores and quality ratings from both normal-hearing and hearing-impaired listeners for a wide variety of signal and processing conditions. The basics of the metrics are explained, and the metrics are then used to analyse the effects of additive noise on speech, to evaluate noise suppression algorithms, and to measure differences among commercial hearing aids. "},{"title":"Hearing Loss​","type":1,"pageTitle":"FAQ for CPC2","url":"docs/cpc2/cpc2_faq#hearing-loss","content":"There are many types of hearing loss, but the focus of the Clarity project is the hearing loss that happens with ageing. This is a form of sensorineural hearing loss. "},{"title":"How does hearing loss affect the perception of audio signals, and how do modern hearing aids process sound to help with this?​","type":1,"pageTitle":"FAQ for CPC2","url":"docs/cpc2/cpc2_faq#how-does-hearing-loss-affect-the-perception-of-audio-signals-and-how-do-modern-hearing-aids-process-sound-to-help-with-this","content":"In this seminar from the first Clarity workshop, Karolina Smeds from ORCA Europe and WS Audiology discusses the effects of hearing loss and the hearing aid processing strategies that are typically used to counter the sensory deficits.  Click arrow to see synopsis. Hearing loss leads to several unwanted effects. Loss of audibility for soft sounds is one effect, but also when amplification is used to create audibility for soft sounds, many [suprathreshold](https://www.lexico.com/en/definition/suprathreshold) deficits remain. The most common type of hearing loss is a [cochlear](https://www.lexico.com/definition/cochlear) hearing loss, where haircells or nerve synapses in the cochlea are damaged. Ageing and noise exposure are the most common causes of cochlear hearing loss. This type of hearing loss is associated with atypical loudness perception and difficulties in noisy situations. Background noise masks for instance speech to a higher degree than for a person with healthy hair cells. This explains why listening to speech-in-noise (SPIN) is such an important topic to work on. A brief introduction to signal processing in hearing aids will be presented. With the use of frequency-specific amplification and compression (automatic gain control, AGC), hearing aids are usually doing a good job in compensating for reduced audibility and for atypical suprathreshold loudness perception. However, it is more difficult to compensate for the increased masking effect. Some examples of strategies will be presented. Finally, natural conversations in noise will be discussed. The balance between being able to have a conversation with a specific communication partner in a group of people and being able to switch attention if someone else starts to talk will be touched upon. "},{"title":"Prediction model​","type":1,"pageTitle":"FAQ for CPC2","url":"docs/cpc2/cpc2_faq#prediction-model","content":""},{"title":"Do I have to use a separate hearing loss model?​","type":1,"pageTitle":"FAQ for CPC2","url":"docs/cpc2/cpc2_faq#do-i-have-to-use-a-separate-hearing-loss-model","content":"No is the short answer! In the baseline, we've used the Cambridge hearing loss model and a separate binaural speech intelligibility model. Another approach would be to create a single model that encapsulates the combined effects of hearing loss and speech perception. "},{"title":"What should the output of my prediction model be?​","type":1,"pageTitle":"FAQ for CPC2","url":"docs/cpc2/cpc2_faq#what-should-the-output-of-my-prediction-model-be","content":"The output should include a predicted speech intelligibility score per input signal, specifically, an estimate of the number of words correct as a percentage of the total number of words in the signal. "},{"title":"Data​","type":1,"pageTitle":"FAQ for CPC2","url":"docs/cpc2/cpc2_faq#data","content":""},{"title":"Do you have suggestions for expanding the training data?​","type":1,"pageTitle":"FAQ for CPC2","url":"docs/cpc2/cpc2_faq#do-you-have-suggestions-for-expanding-the-training-data","content":"The prediction challenge data is limited by having to get the ground truth from listening tests on people with a hearing loss. We look forward to seeing what approaches teams use to help overcome this limitation, such as using unsupervised models, data augmentation or generating additional ground truth data using a pre-existing model. The baseline model includes a hearing loss and speech intelligibility model that could be used for creating additional pre-training data. There are other models that you might consider where code is available. None has been checked by the Clarity team. Katerina Zmolikova has made her Pytorch version of the baseline hearing impairment and speech intelligibility model available. Both model fit a neural network framework, are faster but more approximate (see graphs on github).HASQI and HASPI are quality and speech intelligibility metrics designed to work for people with a hearing impairment. James Kates explains more about these above. MATLAB code HASPI v2 and HASQI v2 are available, along with the user guide.STOI-Net: A Deep Learning based Non-Intrusive Speech Intelligibility Assessment Model by Ryandhimas Zezario et al. is monaural and non-intrusive using Python, Keras and TensorFlow. It doesn't model the effect of hearing loss. An alternative is Asger Heidemann Andersen's MATLAB code. "},{"title":"Missing data​","type":1,"pageTitle":"FAQ for CPC2","url":"docs/cpc2/cpc2_faq#missing-data","content":"We have audiograms for all our listening panel. But for other characterisations of hearing, only some of the panel have provided data. Therefore there is missing data that has to be dealt with. One approach to the missing data is to just ignore it and just use the audiograms. The problem with this approach is that audiograms only quantify the hearing threshold, and our speech in noise audio samples were not played that quietly. Nevertheless, audiograms are the most common way of characterising hearing loss. Alternatively, a method to use the partial data could be developed, and we expect this would help with speech intelligibility prediction. You will find plenty of data science blog posts, e.g. towards data science discussing different approaches. A key question is whether the missing data is 'missing at random' i.e. is the distribution of the missing data expected to be the same as that of the present data? For the prediction challenge, this would mean the missing triple-digit-test values are coming from some random sample of the listeners, who'd be no different from the listeners who did complete the triple-digital-test. Unfortunately, this might not be true, because the failure to complete the triple-digit-tests could well correlate with hearing loss (e.g. maybe older people with more hearing loss were less likely to do the test). The Clarity data is probably 'missing not at random'. One simple solution is to delete examples with missing data, but the loss of so much data probably makes this undesirable. A more sophisticated approach is to fill gaps in data via imputation i.e. first estimate values for the missing data and then treat the dataset as complete. A couple of simple approaches for imputation are: (i) use the mean value from the dataset for missing values, and (ii) create a model to estimate the missing data from the audiograms. There are other approaches in data science that could be exploited such as coding the missing values into a 'N/A' category within the input data. "},{"title":"Modelling the scenario","type":0,"sectionRef":"#","url":"docs/cec2/data/cec2_scenario","content":"","keywords":""},{"title":"Brief overview of random scenario generation​","type":1,"pageTitle":"Modelling the scenario","url":"docs/cec2/data/cec2_scenario#brief-overview-of-random-scenario-generation","content":"The scenarios are for: Small rooms that have low to moderate reverberation with randomized dimensions and locations of materials such as carpets.The locations of the listener, target and interferer are randomized.The target talker is selected from our set of 40 speakers.The target talker produces a randomly chosen 7-10 word utterance.There are two or three interferer sounds running throughout the audio. This can be a: stream of competing speech;continuous domestic noise source (e.g., a washing machine); ormusic source. The target speech source will onset about one second into the scene.The listener starts not looking at the target talker, but around the time the target speech starts, the listener rotates their head to approximately face towards the target. An example scenario is shown in Figure 1. It also defines the coordinate system and origin for the room generation. Figure 1, An example scenario with two noise interferers. "},{"title":"Room geometry​","type":1,"pageTitle":"Modelling the scenario","url":"docs/cec2/data/cec2_scenario#room-geometry","content":"Cuboid rooms with dimensions length LLL by width WWW by height HHH.Length LLL set using a uniform probability distribution random number generator with 3 &lt;&lt;&lt; LLL (m) ≤≤≤ 8.Height HHH set using a Gaussian distribution random number generator with a mean of 2.7 m and standard deviation of 0.8 m.Area L×WL×WL×W set using a Gaussian distribution random number generator with mean 17.7 m2^22 and standard deviation of 5.5 m2^22 "},{"title":"Room materials​","type":1,"pageTitle":"Modelling the scenario","url":"docs/cec2/data/cec2_scenario#room-materials","content":"One of the walls of the room is randomly selected for the location of the door. The door can be at any position with the constraint of being at least 20 cm from the corner of the wall. A window is placed on one of the other three walls. The window could be at any position of the wall but at 1.9 m height and at 0.4 m from any corner. The curtains are simulated to the side of the window. For larger rooms, a second window and curtains are simulated following a similar methodology. A sofa is simulated at a random position as a layer on the wall and the floor. Finally, a rug is simulated at a random location on the floor. "},{"title":"The listener (receiver)​","type":1,"pageTitle":"Modelling the scenario","url":"docs/cec2/data/cec2_scenario#the-listener-receiver","content":"The listener has position, r⃗=(xr,yr,zr)\\vec{r} = (x_r,y_r,z_r)r=(xr​,yr​,zr​) This is positioned within the room using uniform probability distribution random number generators for the x and y coordinates (see Figure 2 for origin location). There are constraints to ensure that the receiver is not too close to the wall: −W/2+1≤xr≤W/2−1-W/2+1 \\le x_r \\le W/2-1−W/2+1≤xr​≤W/2−11≤yr≤L−11 \\le y_r \\le L-11≤yr​≤L−1zrz_rzr​ either 1.2 m (sitting) or 1.6 m (standing). "},{"title":"Head rotation​","type":1,"pageTitle":"Modelling the scenario","url":"docs/cec2/data/cec2_scenario#head-rotation","content":"The listener is initially oriented away from the target and will turn to be roughly facing the target talker around the time when the target speech starts Orientation of listener at start of the sample ~25° from facing the target (standard deviation = 5°), limited to +-2 standard deviations.Start of rotation is between -0.635 s to 0.865s (rectangular probability)The rotation lasts for 200 ms (standard deviation =10 ms)Orientation after rotation is 0-10° (random with rectangular probability distribution). "},{"title":"The target talker​","type":1,"pageTitle":"Modelling the scenario","url":"docs/cec2/data/cec2_scenario#the-target-talker","content":"​​The target talker has position t⃗=(xt,yt,zt)\\vec{t} = (x_t,y_t,z_t)t=(xt​,yt​,zt​) The target talker is positioned within the room using uniform probability distribution random number generators for the coordinates. Constraints ensure the target is not too close to the wall or receiver. It is set to have the same height as the receiver. −W/2+1≤xt≤W/2−1-W/2+1 \\le x_t \\le W/2-1−W/2+1≤xt​≤W/2−11≤yt≤L−11 \\le y_t \\le L-11≤yt​≤L−1∣r−t∣&gt;1|r-t| &gt; 1∣r−t∣&gt;1zt=zrz_t=z_rzt​=zr​ A speech directivity pattern is used, which is directed at the listener. The target speech starts between 1.0 and 1.5 seconds into the mixed sound files (rectangular probability distribution). "},{"title":"The interferers​","type":1,"pageTitle":"Modelling the scenario","url":"docs/cec2/data/cec2_scenario#the-interferers","content":"The interferers have position i1,2,3⃗=(xi,yi,zi)\\vec{i_{1,2,3}} = (x_i,y_i,z_i)i1,2,3​​=(xi​,yi​,zi​) Each interferer is modelled as an omnidirectional point source. They will be radiating: speech, noise or music. They are placed within the room using uniform probability distribution random number generators for the coordinates. The following constraints ensure the interferer is not too close to the wall or listener. However, interferers are independently positioned with no constraint on their position relative to each other. They are set to be at the same height as the listener. Note, this means that the interferers can be at any angle relative to the listener. −W/2+1≤xi≤W/2−1-W/2+1 \\le x_i \\le W/2-1−W/2+1≤xi​≤W/2−11≤yi≤L−11 \\le y_i \\le L-11≤yi​≤L−1∣r−i∣&gt;1|r-i| \\gt 1∣r−i∣&gt;1zi=zrz_i = z_rzi​=zr​ The interferers are present over the whole mixed sound file. "},{"title":"Signal-to-noise ratio (SNR)​","type":1,"pageTitle":"Modelling the scenario","url":"docs/cec2/data/cec2_scenario#signal-to-noise-ratio-snr","content":"The SNR of the mixtures are engineered to achieve a suitable range of speech intelligibility values. A desired signal-to-noise ratio, SNRD (dB), is chosen at random. This is generated with a uniform probability distribution between limits determined by pilot listening tests. The better ear SNR (BE_SNR) models the better ear effect in binaural listening. It is calculated for the reference channel (channel 1, which corresponds to the front microphone of the hearing aid). This value is used to scale all interferer channels. The procedure is described below. For the reference channel, The segment of the summed interferers that overlaps with the target (without padding), i′i'i′, and the target (without padding), t′t't′, are extractedSpeech-weighted SNRs are calculated for each ear, SNRL_LL​ and SNRR_RR​: Signals i′i'i′ and t′t't′ are separately convolved with a speech-weighting filter, h (specified below).The rms is calculated for each convolved signal.SNRL_LL​ and SNRR_RR​ are calculated as the ratio of these rms values. The BE_SNR is selected as the maximum of the two SNRs: BE_SNR = max(SNRL_LL​ and SNRR_RR​). Then per channel, The summed interferer signal, i, is scaled by the BE_SNR i=i×i = i \\timesi=i× BE_SNR Finally, i is scaled as follows: i=i×10((−SNRD)/20)i = i \\times 10^{((-SNR_D)/20)}i=i×10((−SNRD​)/20) The speech-weighting filter is an FIR designed using the host window method [2, 3]. The frequency response is shown in Figure 2. The specification is: Frequency (Hz) = [0, 150, 250, 350, 450, 4000, 4800, 5800, 7000, 8500, 9500, 22050]Magnitude of transfer function at each frequency = [0.0001, 0.0103, 0.0261, 0.0419, 0.0577, 0.0577, 0.046, 0.0343, 0.0226, 0.0110, 0.0001, 0.0001] Figure 2, Speech weighting filter transfer function graph. "},{"title":"References​","type":1,"pageTitle":"Modelling the scenario","url":"docs/cec2/data/cec2_scenario#references","content":" Schröder, D. and Vorländer, M., 2011, January. RAVEN: A real-time framework for the auralization of interactive virtual environments. In Proceedings of Forum Acusticum 2011 (pp. 1541-1546). Denmark: Aalborg.Abed, A.H.M. and Cain, G.D., 1978. Low-pass digital filtering with the host windowing design technique. Radio and Electronic Engineer, 48(6), pp.293-300.Abed, A.E. and Cain, G., 1984. The host windowing technique for FIR digital filter design. IEEE transactions on acoustics, speech, and signal processing, 32(4), pp.683-694. "},{"title":"CEC2 Data","type":0,"sectionRef":"#","url":"docs/cec2/data/cec2_data","content":"","keywords":""},{"title":"A. Training, development, evaluation data​","type":1,"pageTitle":"CEC2 Data","url":"docs/cec2/data/cec2_data#a-training-development-evaluation-data","content":"The dataset of 10,000 scenes is split into three: Training (train).Development (dev).Evaluation (eval). Uses of the data: You should not use the development or evaluation data set for training.The system submitted should be chosen on the evidence provided by the development set.The final listening and ranking will be performed with the (held-out) evaluation set.For more information on augmenting and supplementing the training data, please see the rules.The evaluation dataset will be made available one month before the challenge submission deadline. "},{"title":"B. The scene dataset​","type":1,"pageTitle":"CEC2 Data","url":"docs/cec2/data/cec2_data#b-the-scene-dataset","content":"The complete dataset is composed split into the following sets: Training (6000 scenes, 24 speakers);Development (2500 scenes, 10 speakers);Evaluation (1500 scenes, 6 speakers). Each scene corresponds to a unique target utterance and unique segment(s) of noise from the interferers. The training, development and evaluation sets are disjoint with respect to the target speakers. The three sets are balanced for the gender of the target talker. High-Order Ambisonic Impulse Responses (HOA-IRs) and Head-Related Impulse Response (HRIRs) are used to model how the sound is altered as it propagates through the room and interacts with the head. See the page on scene generation for more details. Time-domain acoustic signals are generated for: A hearing aid with 3 microphone inputs (front, mid, rear). The hearing aid has a Behind-The-Ear (BTE) form factor; see Figure 1. The distance between microphones is approx. 7.6 mm. The properties of the tube and ear mould are not considered.Close to the eardrum.The anechoic target reference (front microphone). Figure 1. Front (Fr), Middle (Mid) and Rear microphones on a BTE hearing aid form. Head Related Impulse Responses (HRIRs) are used to model how sound is altered as it propagates in a free-field and interacts with the head (i.e., no room is included). These are taken from the OlHeadHRTF database with permission. These include HRIRs for human heads and for three types of head-and-torso simulator/mannekin. The eardrum HRIRs (labelled ED) are for a position close to the eardrum of the open ear. rpf files and ac files are specification files for the geometric room acoustic model that include a complete description of the room, both in terms of geometry and room materials. "},{"title":"B.1 Training data​","type":1,"pageTitle":"CEC2 Data","url":"docs/cec2/data/cec2_data#b1-training-data","content":"For each scene in the training data the following signals and metadata are available: The target and interferer HOA-IRs (4 pairs: front, mid, rear and eardrum for left and right ears).The mono target and interferer signals (pre-convolution).For each hearing aid microphone (channels 1-3 where channel 1 is front, channel 2 is mid and channel 3 is rear) and a position close to the eardrum (channel 0): The target convolved with the appropriate HOA-IRs and downmixed;The interferers convolved with the appropriate HOA-IRs and downmixed;The sum of the target and interferer convolved with the appropriate HOA-IRs and downmixed; (i.e. the noisy signals that would be received by the hearing aid) The target convolved with the anechoic HOA-IRs and downmixed for channel 1 for each ear (‘target_anechoic’). For use as a reference when computing HASPI scores.Metadata describing the scene: a JSON file containing, e.g., the filenames of the sources, the location of the sources, the viewvector of the target source, the location and viewvector of the receiver, the room dimensions (see specification below), and the room number, which corresponds to the RAVEN BRIR, rpf and ac files.A signal describing the head rotation (i.e. azimuthal angle at each sample) "},{"title":"B.2 Development data​","type":1,"pageTitle":"CEC2 Data","url":"docs/cec2/data/cec2_data#b2-development-data","content":"This is made available to allow you to fully examine the performance of your system. Ground truth data (i.e., the premixed target and interferers are available in the development set) Development data also contains target speaker adaptation sentences, i.e., four utterances from each of the target speakers. These will also be available in the evaluation data. i.e., systems can use these utterances in conjunction with the known target ID to inform their system of the which speaker in the scene should be attended. Note, that the data available for the evaluation will be much more limited, e.g. it will not contain premixed ground truth signals or scene metadata, (see Section B.3). When using the development data for evaluation, your hearing aid enhancement model should only be using the types of data available in the evaluation data set (see below). "},{"title":"B.3 Evaluation data​","type":1,"pageTitle":"CEC2 Data","url":"docs/cec2/data/cec2_data#b3-evaluation-data","content":"The following data will only be available: Audio: the sum of the target and interferers for each hearing aid microphone.The ID of the listener who will be auditioning the processed scene.The listener characterisation data for these listeners.ID of target talker and a few examples of clean audio that are not the same as the target utterance.The head rotation signal, i.e. as might be recovered from hearing aid motion sensors. (Systems can use this signal but should also be evaluated without using it.)Speaker adaptation sentence - 4 clean utterances for each target speaker. One challenge will be identifying the target talker from the hearing aid microphone signals. There are two possibilities: The ID of the target talker is given with examples of clean audio. This would allow an algorithm to learn characteristics of the target talker to then help it identify the voice in the mixture.The azimuth of the target and the starting time of the utterance are both roughly known from the scene generation metadata statistics. These two approaches mimic what is available to human listeners. They might focus on a known voice or they might use visual cues to know roughly where and when someone is talking. "},{"title":"C Listener data​","type":1,"pageTitle":"CEC2 Data","url":"docs/cec2/data/cec2_data#c-listener-data","content":"We will provide metadata characterising the hearing abilities of the listeners so the audio signals you generate for evaluation can be individualised to the specific listeners who will be hearing them. The same types of data are available for training, development and evaluation. A panel of hearing-aided listeners will be recruited for evaluation. They will be experienced bilateral hearing-aid users: they use two hearing aids but the hearing loss may be asymmetrical. The average pure tone air-conduction hearing loss will be between 25 and about 60 dB in the better ear. They will be fluent in British English. The quantification of the listeners’ hearing is done with: Left and right pure tone air-conduction audiograms. These measure the threshold at which people can hear a pure-tone sound.Results from the DTT (digit-triplet test, also known as a triple digit test)​ The audiogram is the standard clinical measurement of hearing ability. It’s the pure-tone threshold of hearing in each ear, measured in quiet in a sound booth. The procedure is standardized e.g., British Society of Audiology Recommended Procedure. Typically it’s measured at octave frequencies and important intermediate frequencies.The values of the audiogram defines how much gain the hearing aid needs to apply, with the calculation typically done by one of a group of &quot;prescription rules&quot;, e.g. CAMFIT, NAL-NL2 or DSL . Note that the scale of an audiogram is in “dB HL” = “dB Hearing Level”. This is not dB SPL; instead, it’s relative to an international standard such that 0-dB is “normal hearing” at every frequency. For background see Why the Audiogram Is Upside-down | The Hearing Review and The Quest for Audiometric Zero | The Hearing Review The DTT is an adaptive test of speech-in-noise ability. In each trial a listener hears three spoken digits (e.g. 3-6-1) against a background of noise at a given signal-to-noise-ratio (SNR). The task is to respond on a keypad with those three digits in the order they were presented. If the listener gets all three correct, then the SNR is reduced for the next trial so making it slightly harder. If the listener makes any mistake (i.e., any digit wrong, or the order wrong) then the SNR is increased, so making the next trial slightly easier. The test carries on trial-by-trial. The test asymptotes to the SNR at which the participant is equally likely to get all three correct or not, with a few tens of trials needed to get an acceptable result. DTT tests are now used world-wide to measure hearing as they are easy to make in any local language, to explain to participants and to do, and moreover can be done over the internet or telephone as they measure a relative threshold (signal-to-noise ratio), not an absolute threshold in dB SPL. Listeners are encouraged to set a volume that is comfortable and that does not distort or crackle, but is not too quiet. This paper is a recent scoping review of the field. The particular version we used is Vlaming et al.'s high-frequency DTT, which uses a high-pass noise as the masker. Ours starts at -14 dB SNR, goes up/down at 2 dB steps per trial, and continues for 40 trials. In the datafile, an average of the SNR for the last 30 trials is provided (labelled 'threshold'). For reference, the SNRs are supplied for each trial as well. The very first trial is practice and is not scored. "},{"title":"D Data file formats and naming conventions​","type":1,"pageTitle":"CEC2 Data","url":"docs/cec2/data/cec2_data#d-data-file-formats-and-naming-conventions","content":""},{"title":"D.1 Abbreviations used in filenames​","type":1,"pageTitle":"CEC2 Data","url":"docs/cec2/data/cec2_data#d1-abbreviations-used-in-filenames","content":"The following abbreviations are used consistently throughout the filenames and references in the metadata. R – “room”: e.g., “R02678” # Room ID linking to RAVEN rpf fileS – “scene”: e.g., S00121 # Scene ID for a particular setup in a room I.e., room + choice of target and interferer signalsBNC – BNC sentence identifier e.g. BNC_A06_01702CH – CH0 – eardrum signalCH1 – front signal, hearing aid channelCH2 – middle signal, hearing aid channelCH3 – rear signal, hearing aid channel I/i1 – Interferer, i.e., noise or sentence ID for the interferer/maskerT – talker who produced the target speech sentencesL – listenerE – entrant (identifying a team participating in the challenge)t – target (used in BRIRs and RAVEN project ‘rpf’ files) "},{"title":"D.2 General​","type":1,"pageTitle":"CEC2 Data","url":"docs/cec2/data/cec2_data#d2-general","content":"Audio and HOA-IRs will be 44.1 kHz 32-bit wav files in either mono or stereo as appropriate.Where stereo signals are provided the two channels represent the left (0) and right (1) signals of the ear or hearing aid microphones.0 dB FS in the audio signals corresponds to 100 dB SPL.Metadata will be stored in JSON or csv format as appropriate with the exception of Room descriptions are stored as RAVEN project ‘rpf’ configuration files and ‘ac’ files. (However, key details are reflected in the scene.json files) Signals are saved within the Python code as 32-bit floating point by default.Output signals for the listening tests will be required to be in 16-bit format. "},{"title":"D.3 Prompt and transcription data​","type":1,"pageTitle":"CEC2 Data","url":"docs/cec2/data/cec2_data#d3-prompt-and-transcription-data","content":"The following text is available for the target speech: Prompts are the text that was given to the talkers to say.‘Dot’ transcriptions contain the text as it was spoken in a form more suitable for scoring tools.These are stored in the master json metadata file. "},{"title":"D.4 Source audio files​","type":1,"pageTitle":"CEC2 Data","url":"docs/cec2/data/cec2_data#d4-source-audio-files","content":"Wav files containing the original source materials. Original target sentence recordings: &lt;Talker ID&gt;_&lt;BNC sentence identifier&gt;.wav "},{"title":"D.5 Preprocessed scene signals​","type":1,"pageTitle":"CEC2 Data","url":"docs/cec2/data/cec2_data#d5-preprocessed-scene-signals","content":"Audio files storing the signals picked up by the hearing aid microphone that are ready for processing. Separate signals are generated for each hearing aid microphone pair or ‘channel’. &lt;Scene ID&gt;_target_&lt;Channel ID&gt;.wav&lt;Scene ID&gt;_interferer_&lt;Channel ID&gt;.wav&lt;Scene ID&gt;_mixed_&lt;Channel ID&gt;.wav&lt;Scene ID&gt;_target_anechoic.wav - at hearing device front microphone&lt;Scene ID&gt;_hr.wav - head rotation signal Scene ID – S00001 to S10000 S followed by 5 digit integer with 0 pre-padding Channel ID CH0 – Eardrum signalCH1 – Hearing aid front microphoneCH2 – Hearing aid middle microphoneCH3 – Hearing aid rear microphone The anechoic signal is the signal that will be used as the referernce in the HASPI evaluation. The head rotation signal indicates the precise azimuthal angle of the head at each sample. It is stored as a floating point wav file with values between -1 and +1 where the range maps linearly from -180 degrees to +180 degrees. Teams are free to use this signal in their hearing aid algorithms, but if you do so we will ask you to also submit a version of your system that does not use it, so that the benefit of known head motion can be measured. "},{"title":"D.6 Enhanced signals​","type":1,"pageTitle":"CEC2 Data","url":"docs/cec2/data/cec2_data#d6-enhanced-signals","content":"The signals that are output by the baseline enhancement (hearing aid) model. &lt;Scene ID&gt;_&lt;Listener ID&gt;_HA-output.wav #HA output signal (i.e., as submitted by the challenge entrants) Listener ID – ID of the listener panel member, e.g., L001 to L100 for initial ‘pseudo-listeners’, etc. We are no longer providing the script for post-processing signals in preparation for the listener panel. "},{"title":"D.7 Room metadata​","type":1,"pageTitle":"CEC2 Data","url":"docs/cec2/data/cec2_data#d7-room-metadata","content":"JSON file containing the description of a room. This is the data from which the ambisonic room impulse response are generated. It stores the fixed room, listener, target and interferer geometry but does not specify the dynamic factors (e.g. signals, SNRs, head movements etc) that are needed to fully define a scene. [ { &quot;name&quot;: &quot;R00001&quot;, # ID of room linking to RAVEN rpf and ac files &quot;dimensions&quot;: &quot;6.9933x3x3&quot; # Room dimensions in metres, &quot;target&quot;: { # target positions (x,y,z) and view vectors (look directions, x,y,z) &quot;position&quot;: [-0.3, 2.4, 1.2], &quot;view_vector&quot;: [0.071, 0.997, 0.0], }, &quot;listener&quot;: { &quot;position&quot;: [-0.1, 5.2, 1.2], &quot;view_vector&quot;: [0.071, 0.997, 0.0], }, &quot;interferers&quot;: [ { &quot;position&quot;: [0.4, 4.0, 1.2], }, { # etc, up to three interferers } ], }, ... ]  "},{"title":"D.8 Scene metadata​","type":1,"pageTitle":"CEC2 Data","url":"docs/cec2/data/cec2_data#d8-scene-metadata","content":"JSON file containing a description of the scene. It is a list of dictionaries with each entry representing a unique scene. A scene can be considered to be a room (see Section D.7) plus the full set of listener, target and interferer details. Note, many scenes can be generated from a single room, i.e. each using different listener, target and interferer settings. [ { &quot;scene&quot;: &quot;S00001&quot;, # the unique scene ID &quot;room&quot;:: &quot;R00001&quot;, # ID of room linking to rooms.json &quot;target&quot;: { &quot;name&quot;: &quot;T005_JYD_04274&quot;, # target speaker code and BNCid &quot;time_start&quot;: 107210, # start time of target in samples &quot;time_end&quot;: 217019 # end time of target in samples }, &quot;listener&quot;: { &quot;rotation&quot;: [ # Defines the head motion - list of time, direction pairs { &quot;sample&quot;: 88200, &quot;angle&quot;: 30 # Azimuth angle in degrees }, { &quot;sample&quot;: 176400, &quot;angle”: 50 } ], &quot;hrir_filename&quot;: [&quot;VP_N4-ED&quot;, &quot;VP_N4-BTE_fr&quot;, &quot;VP_N4-BTE_mid&quot;, &quot;VP_N4-BTE_rear&quot;] # HRIR filename for each channel to generate }, &quot;interferers&quot;: [ { &quot;position&quot;: 1, # Index of interferer position (See rooms.json) &quot;time_start&quot;: 0, # time of interferer onset in samples &quot;time_end&quot;: 261119, # time of interferer offset in samples &quot;name&quot;: &quot;track_1353255&quot;, # interferer name &quot;type&quot;: &quot;music&quot;, # interferer type: speech, noise or music &quot;offset&quot;: 4076256 # index into interferer file at which to extract sample }, { # etc, up to three interferers } ], &quot;dataset&quot;: &quot;train&quot;, # the dataset to which the scene belongs: train, dev or eval &quot;duration&quot;: 261119, # total duration of scene in samples &quot;SNR&quot;: 6.89 # targe SNR for the scene }, ... ]  There are JSON files containing the scene specifications per dataset, e.g., scenes.train.json.- Note, that the scene ID and room ID might have a one-to-one mapping in the challenge, but are not necessarily the same. Multiple scenes can be made by changing the target and masker choices for a given room. E.g., participants wanting to expand the training data could remix multiple scenes from the same room. The listener ID is not stored in the scene metadata; this information is stored separately in a scenes_listeners.json file which maps scenes to listeners, ie. telling you which listener (or listeners) will be listening to which scenes in the evaluation (see Section D.9). Noise interferers are labelled with a type “music”, “noise” or “speech” and then have a unique name identifying the file. For speech: &lt;ACCENT_CODE&gt;_&lt;SPEAKER_ID&gt; where ACCENT_CODE is a three letter code identify the accent region and gender of the speaker and SPEAKER_ID is a 5-digit ID specific to an individual speaker. E.g. &quot;mif_02484&quot; is a UK midlands accented female, speaker 02484. The speech comes from Demirshan et al. [1] which provides more details.For noise: CIN_&lt;NOISE_TYPE&gt;_&lt;NOISE_ID&gt; where NOISE_TYPE is one of dishwasher, fan, hairdryer, kettle, microwave, vacuum (vacuum cleaner) or washing (washing machine) and NOISE_ID is a unique 3-digit code for the sample.For music: track_&lt;TRACK_ID&gt; where TRACK_ID is unique 7-digit track identifier taken from the MTG Jamendo database. [2] Given the type and name, further interferer metadata can be found in the files masker_speech_list.json, masker_noise_list.json and masker_music_list.json which are distributed with the challenge. "},{"title":"D.9 Listener metadata​","type":1,"pageTitle":"CEC2 Data","url":"docs/cec2/data/cec2_data#d9-listener-metadata","content":"Audiogram data is stored in a single JSON file with the following format. { &quot;L0001&quot;: { &quot;name&quot;: &quot;L0001&quot;, &quot;audiogram_cfs&quot;: [250, 500, 1000, 2000, 3000, 4000, 6000, 8000], &quot;audiogram_levels_l&quot;: [10, 10, 20, 30, 40, 55, 55, 60], &quot;audiogram_levels_r&quot;: [ … ], }, &quot;L0002&quot;: { ... }, ... }  Additional metadata (e.g. digit triple test results) are stored in a csv file. DETAILS "},{"title":"D.10 Scene-Listener map​","type":1,"pageTitle":"CEC2 Data","url":"docs/cec2/data/cec2_data#d10-scene-listener-map","content":"JSON file named scenes_listeners.json dictates which scenes are to be processed by which listeners. { &quot;S00001&quot;: [&quot;L0001&quot;, &quot;L0002&quot;, &quot;L0003&quot;], &quot;S00002&quot;: [&quot;L0003&quot;, &quot;L0005&quot;, &quot;L0007&quot;], ... }  "},{"title":"References​","type":1,"pageTitle":"CEC2 Data","url":"docs/cec2/data/cec2_data#references","content":"Demirsahin, Isin and Kjartansson, Oddur and Gutkin, Alexander and Rivera, Clara, &quot;Open-source Multi-speaker Corpora of the English Accents in the British Isles&quot;, Proceedings of The 12th Language Resources and Evaluation Conference (LREC), 6532--6541, 2020, Avialable OnlineBogdanov, Dmitry and Won, Minz and Tovstogan, Philip and Porter, Alastair and Serra, Xavier, &quot;The MTG-Jamendo Dataset for Automatic Music Tagging&quot;, In Proc. Machine Learning for Music Discovery Workshop, International Conference on Machine Learning (ICML 2019), 2019, Long Beach, CA, United States&quot;, Available Online "},{"title":"The Challenge Rules","type":0,"sectionRef":"#","url":"docs/cpc2/cpc2_rules","content":"","keywords":""},{"title":"What information can I use?​","type":1,"pageTitle":"The Challenge Rules","url":"docs/cpc2/cpc2_rules#what-information-can-i-use","content":""},{"title":"Training and development​","type":1,"pageTitle":"The Challenge Rules","url":"docs/cpc2/cpc2_rules#training-and-development","content":"Teams should use the signals and listener responses provided in the CPC2.train.json file. In addition, teams can use their own data for training or expand the training data through simple automated modifications. Additional pre-training data could be generated by existing speech intelligibility and hearing loss models. The FAQ gives links to some models that might be used for this. Any audio or metadata can be used during training and development, but during evaluation the prediction model(s) will not have access to all of the data (see next section). "},{"title":"Evaluation​","type":1,"pageTitle":"The Challenge Rules","url":"docs/cpc2/cpc2_rules#evaluation","content":"The only data that can be used by the prediction model(s) during evaluation are described below. For non-intrusive methods: The output of the hearing aid processor/system.The IDs of the listeners assigned to the scene/hearing aid system in the metadata provided.The listener metadata. Additionally, for intrusive methods: The target reference signal, i.e. the target convolved with the anechoic BRIR (channel 1) for each ear (‘target_anechoic’).The prompt for the utterances (the text the actors were given to read). "},{"title":"Baseline models and computational restrictions​","type":1,"pageTitle":"The Challenge Rules","url":"docs/cpc2/cpc2_rules#baseline-models-and-computational-restrictions","content":"Teams may choose to use all or some of the provided baseline models.There is no limit on computational cost.Models can be non-causal. "},{"title":"What sort of model do I create?​","type":1,"pageTitle":"The Challenge Rules","url":"docs/cpc2/cpc2_rules#what-sort-of-model-do-i-create","content":"You model should report the speech intelligibility for the whole sentence for each audio sample/listener combination, i.e. a single score that represents a prediction of the proportion of words that would be recognised correctlyThe model architecture is entirely up to you, e.g. you can create a model that attempts to recognise individual words and then reduces this down to a proportion, or you can estimate an intelligibility score directly from the audio. Models may have explicit hearing loss model stages or be trained directly to map signals and audiograms to predictions. "},{"title":"Submitting multiple entries​","type":1,"pageTitle":"The Challenge Rules","url":"docs/cpc2/cpc2_rules#submitting-multiple-entries","content":"If you wish to submit multiple entries, Your systems must have significant differences in their approach.You must contact the organisers to discuss your plans.If accepted you will be issued with multiple Team IDs to distinguish your entries.In your documentation, you must make it clear how the submissions differ. "},{"title":"Evaluation of systems​","type":1,"pageTitle":"The Challenge Rules","url":"docs/cpc2/cpc2_rules#evaluation-of-systems","content":"Entries will be ranked according to their performance in predicting measured intelligibility scores.The system score will be taken to be the RMSE between the predicted and measured intelligibility scores across the complete test set.Separate rankings will be made for intrusive and non-intrusive methods.Systems will only be considered if the technical report has been submitted and the system is judged to be compliant with the challenge rules. "},{"title":"Teams​","type":1,"pageTitle":"The Challenge Rules","url":"docs/cpc2/cpc2_rules#teams","content":"Teams must have registered and nominated a contact person.Teams can be from one or more institutions.The organisers - and any person forming a team with one or more organisers - may enter the challenge themselves but will not be eligible to win the cash prizes. "},{"title":"Transparency​","type":1,"pageTitle":"The Challenge Rules","url":"docs/cpc2/cpc2_rules#transparency","content":"Teams must provide a technical document of up to 2 pages describing the system/model and any external data and pre-existing tools, software and models used.We will publish all technical documents on the challenge website (anonymous or otherwise).Teams are encouraged – but not required – to provide us with access to the system(s)/model(s) and to make their code open source.Anonymous entries are allowed but will not be eligible for cash prizes.If a group of people submits multiple entries, they cannot win more than one prize in a given category.All teams will be referred to using anonymous codenames if the rank ordering is published before the final results are announced.Teams are strongly encouraged to submit their report for presentation at the Clarity-2023 Interspeech Satellite Workshop. "},{"title":"Intellectual property​","type":1,"pageTitle":"The Challenge Rules","url":"docs/cpc2/cpc2_rules#intellectual-property","content":"The following terms apply to participation in this machine learning challenge (“Challenge”). The entrants' “Submission” will consist of a set of intelligibility predictions and an accompanying technical report. The Challenge is organised by the “Challenge Organiser”. Entrants retain ownership of all intellectual and industrial property rights (including moral rights) in and to Submissions. As a condition of submission, Entrant grants the Challenge Organiser, its subsidiaries, agents and partner companies, a perpetual, irrevocable, worldwide, royalty-free, and non-exclusive license to use, reproduce, adapt, modify, publish, distribute, publicly perform, create a derivative work from, and publicly display the Submission. Entrants provide Submissions on an “AS IS” BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. "},{"title":"Important dates","type":0,"sectionRef":"#","url":"docs/cpc2/cpc2_dates","content":"Important dates All dates are to be intended anywhere on earth time (AoE). 1st March 2023: Launch of challenge, release of data.1st July 2023: Release of evaluation data and opening of submission window.31st July 2023: Submission deadline. All entrants must have submitted their predictions plus a draft of their technical report. Scores will be returned to entrants within 24 hours of submission. 19th August 2023: Clarity 2023 workshop.19th September 2023: Deadline for submission of finalised Workshop papers Please note that while workshop attendance is not a pre-requisite for participation in the challenge, we strongly encourage all entrants to attend the workshop to present their work.","keywords":""},{"title":"Data","type":0,"sectionRef":"#","url":"docs/cpc1/cpc1_data","content":"","keywords":""},{"title":"A. Training, development, evaluation data​","type":1,"pageTitle":"Data","url":"docs/cpc1/cpc1_data#a-training-development-evaluation-data","content":"The dataset is split into these two subsets: training/development (train) and evaluation (eval). You are responsible for splitting the training/development dataset into data for training and development, e.g., using k-fold cross validation.The final evaluation and ranking will be performed with the (held-out) evaluation set.For more information on supplementing the training data, please see the rules, and also the FAQ. The evaluation dataset will be made available one month before the challenge submission deadline. "},{"title":"B. The scene dataset​","type":1,"pageTitle":"Data","url":"docs/cpc1/cpc1_data#b-the-scene-dataset","content":"The complete dataset is composed of a large number of scenes associated with 6 talkers, 10 hearing aid systems and around 25 listeners. Each scene corresponds to a unique target utterance and a unique segment of noise from an interferer. The training/development and evaluation sets are disjoint for system and listener. Binaural Room Impulse Responses (BRIRs) are used to model how the sound is altered as it propagates through the room and interacts with the head. The audio signals for the scenes are generated by convolving source signals with the BRIRs and summing. See the page on modelling the scenario for more details. Randomised room dimensions, target and interferer locations are used. RAVEN is the geometric room acoustic model used to create the BRIR. "},{"title":"B.1 Training/development data​","type":1,"pageTitle":"Data","url":"docs/cpc1/cpc1_data#b1-trainingdevelopment-data","content":"This contains all the information about how the signals processed by the hearing aids were created. For the prediction challenge, some of the data can be ignored (but is included because some may find it useful). Data and metadata most useful for the prediction challenge: The output of the hearing aid processor.The target convolved with the anechoic Binaural Room Impulse Response (BRIR) (channel 1) for each ear (‘target_anechoic’).The mono target and interferer signals (pre-convolution).Prompts of the utterances (what the actors were told to say)Metadata describing the scene: a JSON file containing, e.g., the filenames of the sources, the location of the sources, the viewvector of the target source, the location and viewvector of the receiver, the room dimensions (see specification below), and the room number, which corresponds to the RAVEN BRIR, rpf and ac files. For evaluation not all of the data is available, see below. Other information also provided, click me to expand Data used to create inputs to hearing aids, etc: The target and interferer BRIRs (4 pairs: front, mid, rear and eardrum for left and right ears).Head Related Impulse Responses (HRIRs) including those corresponding to the target azimuth.For each hearing aid microphone (channels 1-3 where channel 1 is front, channel 2 is mid and channel 3 is rear) and a position close to the eardrum (channel 0): The target convolved with the appropriate BRIR;The interferer convolved with the appropriate BRIR;The sum of the target and interferer convolved with the appropriate BRIRs. The BRIRs are generated for: A hearing aid with 3 microphone inputs (front, mid, rear). The hearing aid has a Behind-The-Ear (BTE) form factor; see Figure 1. The distance between microphones is approx. 7.6 mm. The properties of the tube and ear mould are not considered.Close to the eardrum.The anechoic target reference (front microphone; the premixed target signal convolved with the BRIR with the reflections “turned off”). Figure 1. Front (Fr), Middle (Mid) and Rear microphones on a BTE hearing aid form. (HRIRs) are used to model how sound is altered as it propagates in a free-field and interacts with the head (i.e., no room is included). These are taken from the OlHeadHRTF database with permission. These include HRIRs for human heads and for three types of head-and-torso simulator/mannekin. The eardrum HRIRs (labelled ED) are for a position close to the eardrum of the open ear. The RAVEN project files - termed &quot;rpf&quot; - are specification files for the geometric room acoustic model that include a complete description of the room. "},{"title":"B.2 Evaluation scene data​","type":1,"pageTitle":"Data","url":"docs/cpc1/cpc1_data#b2-evaluation-scene-data","content":"For each scene in the evaluation data only the following will be available: The output of the hearing aid processor.The target convolved with the anechoic BRIR (channel 1) for each ear (‘target_anechoic’).The IDs of the listeners assigned to the scene/hearing aid system in the metadata provided.The listener metadata.The prompt for the utterances (the text the actors were given to read) "},{"title":"C Listener data​","type":1,"pageTitle":"Data","url":"docs/cpc1/cpc1_data#c-listener-data","content":"We will provide metadata characterising the hearing abilities of our listening panel. The listening panel data will be split, so that the listeners in the held back evaluation data are different from those provided in the training and development data. The listening panel are experienced bilateral hearing-aid users (they use two hearing aids but the hearing loss may be asymmetrical) with an averaged hearing loss as measured by pure tone air-conduction of between 25 and about 60 dB in the better ear, with fluent speaking of (and listening to) British English. For every listener, you will be given the left and right pure tone air-conduction audiograms. These measure the threshold at which people can hear a pure-tone sound. For some listeners you will be provided with additional characterisation of their hearing. Consequently, if you wish to exploit this additional data, you will need to deal with the missing data. See the FAQ for more on missing data. Below is a description of each measure. SSQ12 - Speech, Spatial, &amp; Qualities of Hearing questionnaire, 12-question version​ This is a popular self-assessment questionnaire of hearing disability. Each item asks about listening situations typical of real life and asks how well a listener would do in it. The SSQ assesses ability to make successful use of one’s hearing (i.e. hearing disability, or activity limitation). This is an intermediate link between the audiological measurement of someone's hearing loss (i.e. their impairment) and a patient's assessment of how that hearing loss impacts their wider life (i.e. their handicap, or participation restriction). The 12 questions are given in table 1 of this paper and FYI a recent paper that used it is here. Responses to each question are on a scale from 0 to 10, with 0 representing &quot;not at all&quot; (or &quot;jumbled&quot;/&quot;concentrate hard&quot; for #11 &amp; #12), and 10 representing &quot;perfect&quot; (or &quot;not jumbled&quot;/&quot;no need to concentrate&quot;). We programmed this as a visual-analog slider, which the participant could set to any position from 0 to 10. The SSQ12 data supplied are the responses to each question, from 0-10 at 1 decimal place resolution, along with the mean of all 12 questions. GHAPB - Glasgow hearing-aid benefit profile questionnaire​ This is designed to assess the efficacy and effectiveness of someone's hearing aid(s) in different scenarios. It asks respondents to consider four scenarios involving speech and to rate on a five-point scale their perceived initial (i.e. unaided) hearing disability, initial handicap, aided benefit, aided handicap, hearing aid use, and hearing aid satisfaction. The listening panel are experienced hearing-aid users, so some of the rating would be about their normal hearing aid. This paper describes the GHABP and provides some normative data. For each scenario, the participant is asked a primary question about if a situation happens to them (relatable). If they answer No, it moves onto to the next scenario.Yes, then a list of six secondary questions are asked (see figure below) If one of the secondary questions is not relatable to the participant, they're asked to respond &quot;N/A&quot; for not applicable. Figure 2. The GHAPB questionnaire. There are four scenarios: listening to the television when the volume is adjusted for others.Having a conversation with one person in quiet.Having a conversation on a busy street or in a shop.Having a conversation with several people in a group. In the datafile, the question numbers are coded as x.y where x is the scenario number and y the secondary question number. If the answer to primary questions is No, then all the secondary questions are coded as 0. If the answer to primary questions is Yes, then each subsequent question is scored as 0. = N/A = first option in the list (eg &quot;no difficulty&quot;)= second= third= fourth= fifth (e.g. &quot;cannot manage at all&quot;) There is no global score for the GHABP. The six secondary questions ask about different things and so should not be averaged across questions, though it is fairly common to average within-question across scenario. DTT (digit-triplet test, also known as a triple digit test)​ This is an adaptive test of speech-in-noise ability. In each trial a listener hears three spoken digits (e.g. 3-6-1) against a background of noise at a given signal-to-noise-ratio (SNR). The task is to respond on a keypad with those three digits in the order they were presented. If the listener gets all three correct, then the SNR is reduced for the next trial so making it slightly harder. If the listener makes any mistake (i.e., any digit wrong, or the order wrong) then the SNR is increased, so making the next trial slightly easier. The test carries on trial-by-trial. The test asymptotes to the SNR at which the participant is equally likely to get all three correct or not, with a few tens of trials needed to get an acceptable result. DTT tests are now used world-wide to measure hearing as they are easy to make in any local language, to explain to participants and to do, and moreover can be done over the internet or telephone as they measure a relative threshold (signal-to-noise ratio), not an absolute threshold in dB SPL. Listeners are encouraged to set a volume that is comfortable and that does not distort or crackle, but is not too quiet. This paper is a recent scoping review of the field. The particular version we used is Vlaming et al's high-frequency DTT, which uses a high-pass noise as the masker. Ours starts at -14 dB SNR, goes up/down at 2 dB steps per trial, and continues for 40 trials. In the datafile, an average of the SNR for the last 30 trials is provided (labelled 'threshold'). For reference, the SNRs are supplied for each trial as well. The very first trial is practice and is not scored. "},{"title":"D Data file formats and naming conventions​","type":1,"pageTitle":"Data","url":"docs/cpc1/cpc1_data#d-data-file-formats-and-naming-conventions","content":""},{"title":"D.1 Abbreviations in Filenames​","type":1,"pageTitle":"Data","url":"docs/cpc1/cpc1_data#d1-abbreviations-in-filenames","content":"R – “room”: e.g., “R02678” # Room ID linking to RAVEN rpf fileS – “scene”: e.g., S00121 # Scene ID for a particular setup in a room I.e., room + choice of target and interferer signalsBNC – BNC sentence identifier e.g. BNC_A06_01702CH – CH0 – eardrum signalCH1 – front signal, hearing aid channelCH2 – middle signal, hearing aid channelCH3 – rear signal, hearing aid channel I/i1 – Interferer, i.e., noise or sentence ID for the interferer/maskerT – talker who produced the target speech sentencesL – listenerE – entrant (identifying a team participating in the challenge)t – target (used in BRIRs and RAVEN project ‘rpf’ files) "},{"title":"D.2 General​","type":1,"pageTitle":"Data","url":"docs/cpc1/cpc1_data#d2-general","content":"Audio and BRIRs will be 44.1 kHz 32 bit wav files in either mono or stereo as appropriate.Where stereo signals are provided, the two channels represent the left and right signals of the ear or hearing aid microphones.HRIRs have a sampling rate of 48 kHz.Metadata will be stored in JSON format wherever possible.Room descriptions are stored as RAVEN project ‘rpf’ configuration files.Signals are saved within the Python code as 32-bit floating point by default. "},{"title":"D.3 Prompt and transcription data​","type":1,"pageTitle":"Data","url":"docs/cpc1/cpc1_data#d3-prompt-and-transcription-data","content":"The following text is available for the target speech: Prompts are the text that was supposed to be spoken as presented to the readers.‘Dot’ transcriptions contain the text as it was spoken in a form more suitable for scoring tools.These are stored in the master json metadata file. "},{"title":"D.4 Timing in audio files​","type":1,"pageTitle":"Data","url":"docs/cpc1/cpc1_data#d4-timing-in-audio-files","content":"The target sound starts 2 seconds after the start of the interferer. This is so the target is clear and unambiguously identifiable for listening tests. This also gives the hearing aid algorithms some time to adjust to the background noise.The interferer continues 1 second after the target has finished, so that all words in the target utterance can be masked. "},{"title":"D.5 Source audio files​","type":1,"pageTitle":"Data","url":"docs/cpc1/cpc1_data#d5-source-audio-files","content":"Wav files containing the original source materials.Could be used as the clean speech reference in an intrusive (double-ended) prediction modelOriginal target sentence recordings:  &lt;Talker ID&gt;_&lt;BNC sentence identifier&gt;.wav  "},{"title":"D.6 Preprocessed scene signals​","type":1,"pageTitle":"Data","url":"docs/cpc1/cpc1_data#d6-preprocessed-scene-signals","content":"Audio files storing the signals picked up by the hearing aid microphone ready for processing.Target_anechoic could be used as the clean speech reference in an intrusive (double-ended) prediction model.Separate signals are generated for each hearing aid microphone pair or ‘channel’. &lt;Scene ID&gt;_target_&lt;Channel ID&gt;.wav &lt;Scene ID&gt;_interferer_&lt;Channel ID&gt;.wav &lt;Scene ID&gt;_mixed_&lt;Channel ID&gt;.wav &lt;Scene ID&gt;_target_anechoic.wav  Scene ID – S00001 to S10000 S followed by 5 digit integer with 0 pre-padding Channel ID CH0 – Eardrum signalCH1 – Hearing aid front microphoneCH2 – Hearing aid middle microphoneCH3 – Hearing aid rear microphone "},{"title":"D.7 Enhanced signals​","type":1,"pageTitle":"Data","url":"docs/cpc1/cpc1_data#d7-enhanced-signals","content":"These are the audio signals that the listeners heard during the speech intelligibility testing. The signals that are output by a given enhancement (hearing aid) model or system. &lt;Entrant ID&gt;_&lt;Scene ID&gt;_&lt;Listener ID&gt;_HA-output.wav # HA output signal (i.e., as submitted by the challenge entrants) Listener ID – ID of the listener panel member, e.g., L200 to L244. "},{"title":"D.8 Scene metadata​","type":1,"pageTitle":"Data","url":"docs/cpc1/cpc1_data#d8-scene-metadata","content":"A JSON file called scenes.CPC1_train.json containing a description of each scene that is used in the listening experiments. It is a hierarchical dictionary, with the top level being scenes indexed by unique scene ID, and each scene described by a second-level dictionary. Here, viewvector indicates the direction vector or line of sight. [ { scene&quot;: &quot;S00001&quot;, &quot;room&quot;: { &quot;name&quot;: &quot;R00001&quot;, &quot;dimensions&quot;: &quot;5.9x3.4186x2.9&quot; # Room dimensions in metres }, &quot;SNR&quot;: 3.8356, &quot;hrirfilename&quot;: &quot;VP_N5-ED&quot;, # HRIR filename &quot;target&quot;: { # target positions (x,y,z) and view vectors (look directions, x,y,z) &quot;Positions&quot;: [ -0.5, 3.4, 1.2 ], &quot;ViewVectors&quot;: [ 0.291, -0.957, 0 ], &quot;name&quot;: &quot;T022_HCS_00002&quot;, # target speaker code and BNCid &quot;nsamples&quot;: 153468, # length of target speech in samples }, &quot;listener&quot;: { &quot;Positions&quot;: [ 0.2, 1.1, 1.2 ], &quot;ViewVectors&quot;: [ -0.414, 0.91, 0 ] }, &quot;interferer&quot;: { &quot;Positions&quot;: [ 0.4, 3.2, 1.2 ], &quot;name&quot;: &quot;CIN_dishwasher_012&quot;, # interferer name &quot;nsamples&quot;: 1190700, # interferer length in samples &quot;duration&quot;: 27, # interferer duration in seconds &quot;type&quot;: &quot;noise&quot;, # interferer type: noise or speech &quot;offset&quot;: 182115, # interferer segment starts at n samples from beginning of recording }, &quot;azimuth_target_listener&quot;: -7.55, # angle azimuth in degrees of target for receiver &quot;azimuth_interferer_listener&quot;: -29.92, # angle azimuth in degrees of interferer for receiver &quot;dataset&quot;: &quot;train&quot;, # dataset: train, dev or eval/test &quot;pre_samples&quot;: 88200, # number of samples of interferer before target onset &quot;post_samples&quot;: 44100 # number of samples of interferer after target offset }, { etc. } ]  There are JSON files containing the scene specifications per dataset, e.g., scenes.train.json.Note that the scene ID and room ID might have a one-to-one mapping in the challenge, but are not necessarily the same.A scene is completely described by the room ID and target and interferer source IDs, as all other information, e.g., source + target geometry are already in the RAVEN project rpf files. Only the room ID is needed to identify the BRIR files.The listener ID is not stored in the scene metadata; this information is stored separately in a scenes_listeners.json file.Non-speech interferers are labelled CIN_&lt;noise type&gt;_XXX, while speech interferers are labelled &lt;three letter code including dialect and talker gender&gt;_XXXXX . "},{"title":"D.9 Listener metadata​","type":1,"pageTitle":"Data","url":"docs/cpc1/cpc1_data#d9-listener-metadata","content":"Listener audiogram data stored in a single JSON file called listeners.CPC1_train.json with the following format. {“L0001”: { “name”: “L0001”, &quot;audiogram_cfs&quot;: [250, 500, 1000, 2000, 3000, 4000, 6000, 8000], “audiogram_levels_l”: [10, 10, 20, 30, 40, 55, 55, 60], “audiogram_levels_r”: [ … ], }, “L0002”: { }, ... }  A spreadsheet named listener_data.CPC1_train.xlsx containing the SSQ12, GHAPB, DTT data for each listener where it is available. "},{"title":"D.10 Listener intelligibility data​","type":1,"pageTitle":"Data","url":"docs/cpc1/cpc1_data#d10-listener-intelligibility-data","content":"JSON files CPC1.train.json (for Track 1) and CPC1.train_indep.json (for Track 2) which provides the responses made by the listeners when presented with a particular scene processed by a particular system. The data is a simple list of dictionaries with one entry for each listener response [ { &quot;scene&quot;:&quot;S08510&quot;, # The identity of the scene &quot;listener&quot;:&quot;L0239&quot;, # The identity of the listener &quot;system&quot;:&quot;E001&quot;, # The identify of the HA system &quot;prompt&quot;:&quot;i suppose you wouldn't be free for dinner this evening&quot;, # The target sentence (prompt) &quot;response&quot;:&quot;freeze evening&quot;, # The listeners response (transcript) &quot;n_words&quot;:10, # Number of words in the target sentence &quot;hits&quot;:1, # Number of words recognised correctly &quot;correctness&quot;:10.0, # The percentage of words recognised correctly &quot;signal&quot;:&quot;S08510_L0239_E001&quot; # The name of the file containing the signal listened to. }, { ... }  "},{"title":"E. Reproduction Levels​","type":1,"pageTitle":"Data","url":"docs/cpc1/cpc1_data#e-reproduction-levels","content":"The graph gives the SPL from one of our headsets based on the volume level of the tablet. The input signal was ICRA speech-shaped noise [1], unmodulated in time, and scaled to an RMS of 0.3. Figure 3. Headset SPL by tablet volume level. "},{"title":"References​","type":1,"pageTitle":"Data","url":"docs/cpc1/cpc1_data#references","content":"[1] ICRA standard noises, https://icra-audiology.org/Repository/icra-noise. We used track #1. "},{"title":"Overview","type":0,"sectionRef":"#","url":"docs/cpc1/cpc1_scenario","content":"","keywords":""},{"title":"Simulating the audio signals that were processed by the hearing aids​","type":1,"pageTitle":"Overview","url":"docs/cpc1/cpc1_scenario#simulating-the-audio-signals-that-were-processed-by-the-hearing-aids","content":"A listener (or receiver) is in a small room that has low to moderate reverberation. They are listening to a target talker, who is selected from our set of 40 speakers. The target talker is producing one of our unique 7-10 word Clarity sentences. Simultaneously, an interferer sound is playing. This is either a competing talker or a continuous noise source (e.g., a washing machine). The target and interferer are at the same height as the listener. The room dimensions, boundary materials, and the locations of the listener, target and interferer are randomised (discussed below). An example of the scenario is shown in Figure 1. The room geometry showing origin location is defined in Figure 2. Example SceneRoom Geometry Figure 1. Example scene. Figure 3, below, shows the basic scene generator. The sound at the receiver is generated first by convolving the source signals with Binaural Room Impulse Responses (BRIRs). This generates reverberated speech and noise that includes the effects of the room and reflections from the listener's head. The reverberated speech and noise signals are then summed after appropriate gains are applied. The gains are set to achieve a Signal-to-Noise Ratio (SNR), which is chosen randomly between limits. The BRIRs are generated using the RAVEN Geometric Room Acoustic Model [1]. There are additional signal paths and outputs generated that have been omitted from Figure 3 for clarity. In addition to the reverberated signals associated with the hearing aid microphones, the signal close to the eardrum is also generated. You can also access the reverberated speech and noise signals before they are mixed. Figure 3. Simplified diagram of the scene generator. RIR refers to Room Impulse Response, HRTFs refers to Head Related Transfer Functions, SNRs are signal-to-noise ratios, and gain calc. indicates gain calculation. Dry here means anechoic. The outputs are noisy speech signals. "},{"title":"Room Geometry​","type":1,"pageTitle":"Overview","url":"docs/cpc1/cpc1_scenario#room-geometry","content":"Cuboid rooms with dimensions length, LLL, by width, WWW, by height, HHH.Length LLL set using a uniform probability distribution random number generator with 3≤L(m)≤83 \\le L (m) \\le 83≤L(m)≤8.Height HHH set using a Gaussian distribution random number generator with a mean of 2.7m2.7 m2.7m and standard deviation of 0.8m0.8 m0.8m.Area L×WL \\times WL×W set using a Gaussian distribution random number generator with mean 17.7m217.7 m^217.7m2 and standard deviation of 5.5m25.5 m^25.5m2. "},{"title":"Room Materials​","type":1,"pageTitle":"Overview","url":"docs/cpc1/cpc1_scenario#room-materials","content":"One of the walls of the room is randomly selected for the location of the door. The door can be at any position with the constraint of being at least at 20 cm from the corner of the wall. A window is placed on one of the other three walls. The window could be at any position of the wall but at 1.9 m height and at 0.4 m from any corner. The curtains are simulated to the side of the window. For larger rooms, a second window and curtains are simulated following a similar methodology. A sofa is simulated at a random position as a layer on the wall and the floor. Finally, a rug is simulated at a random location on the floor. "},{"title":"The receiver​","type":1,"pageTitle":"Overview","url":"docs/cpc1/cpc1_scenario#the-receiver","content":"The receiver has position, r⃗=(xr,yr,zr)\\vec{r} = (x_r,y_r,z_r)r=(xr​,yr​,zr​) This is positioned within the room using uniform probability distribution random number generators for the x and y coordinates (see Figure 2 for origin location). The reciver can have one of two heights (seated or standing height). There are constraints to ensure that the receiver is not too close to the wall: −W/2+1≤xr≤W/2−1-W/2+1 \\le x_r \\le W/2-1−W/2+1≤xr​≤W/2−11≤yr≤L−11 \\le y_r \\le L-11≤yr​≤L−1zrz_rzr​ either 1.2m1.2 m1.2m (sitting) or 1.6m1.6 m1.6m (standing). The receiver is positioned so as to be roughly facing the target talker. That is to say, within ±30\\pm 30±30 degrees of target. The angle = 7.5n7.5n7.5n where nnn is an integer and ∣n∣≤4|n| \\le 4∣n∣≤4. "},{"title":"The target talker​","type":1,"pageTitle":"Overview","url":"docs/cpc1/cpc1_scenario#the-target-talker","content":"The target talker has position t⃗=(xt,yt,zt)\\vec{t} = (x_t,y_t,z_t)t=(xt​,yt​,zt​) The target talker is positioned within the room using uniform probability distribution random number generators for the coordinates. Constraints ensure the target is not too close to the wall or receiver. It is set to have the same height as the receiver. −W/2+1≤xt≤W/2−1-W/2+1 \\le x_t \\le W/2-1−W/2+1≤xt​≤W/2−11≤yt≤L−11 \\le y_t \\le L-11≤yt​≤L−1∣r−t∣&gt;1|r-t| &gt; 1∣r−t∣&gt;1zt=zrz_t=z_rzt​=zr​ A speech directivity pattern is used, which is directed at the listener. "},{"title":"The interferer​","type":1,"pageTitle":"Overview","url":"docs/cpc1/cpc1_scenario#the-interferer","content":"The interferer has position i⃗=(xi,yi,zi)\\vec{i} = (x_i,y_i,z_i)i=(xi​,yi​,zi​) The interferer is a single point source radiating speech or non-speech noise omnidirectionally. It is placed within the room using uniform probability distribution random number generators for the coordinates. These constraints ensure the interferer is not too close to the wall or receiver. It is set to be at the same height as the receiver. Note, this means that the interferer can be at any angle relative to the receiver. −W/2+1≤xi≤W/2−1-W/2+1 \\le x_i \\le W/2-1−W/2+1≤xi​≤W/2−11≤yi≤L−11 \\le y_i \\le L-11≤yi​≤L−1∣r−i∣&gt;1|r-i| \\gt 1∣r−i∣&gt;1zi=zrz_i = z_rzi​=zr​ "},{"title":"Timing​","type":1,"pageTitle":"Overview","url":"docs/cpc1/cpc1_scenario#timing","content":"The target sound starts 2 seconds after the start of the interferer. This is so the target is clear and unambiguously identifiable for listening tests. This also gives the hearing aid algorithms some time to adjust to the background noise.The interferer continues 1 second after the target has finished, so that all words in the target utterance can be masked. "},{"title":"Signal-to-Noise Ratio (SNR)​","type":1,"pageTitle":"Overview","url":"docs/cpc1/cpc1_scenario#signal-to-noise-ratio-snr","content":"The mixtures are engineered such that the target utterances are at an appropriate level of intelligibility when processed by the default hearing aid software. This is achieved by scaling the interferer. Pilot tests have been conducted to get this approximately correct. Scaling is done this way because it does not require recomputing the BRIRs. Note that the interferer can be at any azimuth from the point of view of the listener/receiver. A desired signal-to-noise ratio, SNRD (dB), is chosen using a uniform probability distribution random number generator between the limits of ranges specified for the speech and non-speech interferers. The calculation is based on the ear that has the better signal to noise ratio, as this mimics the better ear effect in binaural listening, where listeners focus on the ear that has the best SNR. The better ear SNR (BE_SNR) is calculated for the reference channel (channel 1, which corresponds to the front microphone of the hearing aid). This value is used to scale all interferer channels. The procedure is described below. For the reference channel, The segment of the interferer that overlaps with the target (without padding) , i‘, and the target (without padding), t‘, are extractedSpeech-weighted SNRs are calculated for each ear, SNRL and SNRR: Signals i‘ and t’ are separately convolved with a speech-weighting filter, h (specified below).The rms is calculated for each convolved signal.SNRL and SNRR are calculated as the ratio of these rms values. The BE_SNR is selected as the maximum of the two SNRs: BE_SNR = max(SNRL and SNRR). Then per channel, The whole interferer signal, i, is scaled by the BE_SNR i=i∗BESNRi = i*BE_{SNR}i=i∗BESNR​ Finally, i is scaled as follows: i=i∗10((−SNRD)/20)i = i*10^{((-SNR_D)/20)}i=i∗10((−SNRD​)/20) The speech-weighting filter is an FIR designed using the host window method [2, 3]. The specification is: Frequency (Hz) = [0, 150, 250, 350, 450, 4000, 4800, 5800, 7000, 8500, 9500, 22050];Magnitude of transfer function at each frequency = [0.0001, 0.0103, 0.0261, 0.0419, 0.0577, 0.0577, 0.046, 0.0343, 0.0226, 0.0110, 0.0001, 0.0001]; Figure 4, Speech weighting filter transfer function graph. "},{"title":"References​","type":1,"pageTitle":"Overview","url":"docs/cpc1/cpc1_scenario#references","content":"Schröder, D. and Vorländer, M., 2011, January. RAVEN: A real-time framework for the auralization of interactive virtual environments. In Proceedings of Forum Acusticum 2011 (pp. 1541-1546). Denmark: Aalborg.Abed, A.H.M. and Cain, G.D., 1978. Low-pass digital filtering with the host windowing design technique. Radio and Electronic Engineer, 48(6), pp.293-300.Abed, A.E. and Cain, G., 1984. The host windowing technique for FIR digital filter design. IEEE transactions on acoustics, speech, and signal processing, 32(4), pp.683-694. "},{"title":"CPC2 Registration","type":0,"sectionRef":"#","url":"docs/cpc2/taking_part/cpc2_registration","content":"","keywords":""},{"title":"Google group​","type":1,"pageTitle":"CPC2 Registration","url":"docs/cpc2/taking_part/cpc2_registration#google-group","content":"If you haven't done so already, please sign up to Clarity's Google group to keep up to date with the challenges. "},{"title":"CPC2 Submission","type":0,"sectionRef":"#","url":"docs/cpc2/taking_part/cpc2_submission","content":"","keywords":""},{"title":"Registration​","type":1,"pageTitle":"CPC2 Submission","url":"docs/cpc2/taking_part/cpc2_submission#registration","content":"Teams are required to register to help us organise the challenge. Registered teams will be assigned a unique team ID. "},{"title":"What evaluation data is provided?​","type":1,"pageTitle":"CPC2 Submission","url":"docs/cpc2/taking_part/cpc2_submission#what-evaluation-data-is-provided","content":"The evaluation data consists of audio signals processed by hearing aid systems, clean reference signals, listener metadata, and a mapping of which listeners listened to which scenes/hearing aid systems. The evaluation data is will be made available when the submission period opens. See the download page for more details. There will be three evaluation sets (eval1, eval2 and eval3), corresponding to the three three training data partitions. i.e., predictions for the eval1 set should be made with systems trained on the train1 partition; eval2 with train2 and eval3 with train3. Note, the evaluation data does not contain the listener responses. We will score your submission for you and return your score (we aim to do this within 24 hours of submission). We will then release the true listener responses the day after the submission deadline to allow teams to perform analysis of their results. "},{"title":"What do I need to submit?​","type":1,"pageTitle":"CPC2 Submission","url":"docs/cpc2/taking_part/cpc2_submission#what-do-i-need-to-submit","content":"All teams must submit Their predicted intelligibility scoresA two page technical report "},{"title":"The predicted intelligibility scores​","type":1,"pageTitle":"CPC2 Submission","url":"docs/cpc2/taking_part/cpc2_submission#the-predicted-intelligibility-scores","content":"Scores for each evaluation set should be stored in a separate CSV file named as follows CPC2_&lt;TEAM_ID&gt;.&lt;SET&gt;.csv, where &lt;TEAM_ID&gt; is your individual team ID, e.g. 'E001' and &lt;SET&gt; is the evaluation set number, either 1, 2, or 3. The CSV files should have two columns, signal_ID, intelligibility_score  where the signal_ID is the unique signal identifier used for the wav file name (e.g., S08510_L0239_E001) and intelligibility_score is the predicted intelligibility given in terms of the percentage words recognised correctly for the signal (i.e., from 0 to 100). The three CSV files should be sent as email attachments to the email address: claritychallengecontact@gmail.com Please use &quot;CPC2 Submission &lt;TEAM_ID&gt;&quot; as the subject line. We also encourage you to make your prediction model code available via an open-source license, but this is not a pre-requisite for entry (see challenge rules). info All registered teams will be emailed with a reminder of their unique team ID shortly before the submission deadline. If you plan to submit please register before the submission deadline. "},{"title":"The technical report​","type":1,"pageTitle":"CPC2 Submission","url":"docs/cpc2/taking_part/cpc2_submission#the-technical-report","content":"**The two page technical report must be submitted in the format required for the Clarity-2023 Workshop. An author kit and submission instructions will be made available. A draft of the report needs to be submitted along with your predictions. The draft needs to be sufficiently complete for us to judge whether your system(s)/model(s) is compliant with the challenge rules. You can find a list of key challenge dates here. Your report should include an abstract and introduction and sections on experimental setup/methodology including system/model information and model/network architecture, evaluation/results, discussion, conclusion and references. Please provide an estimation of the computational resources needed. You must describe any external data and pre-existing tools, software and models used. Please make it clear how your system(s)/model(s) meets the challenge rules. "},{"title":"How will intellectual property be handled?​","type":1,"pageTitle":"CPC2 Submission","url":"docs/cpc2/taking_part/cpc2_submission#how-will-intellectual-property-be-handled","content":"See here under Intellectual Property. "},{"title":"The 2nd Clarity Prediction Challenge","type":0,"sectionRef":"#","url":"docs/cpc2/cpc2_intro","content":"","keywords":""},{"title":"Short Description​","type":1,"pageTitle":"The 2nd Clarity Prediction Challenge","url":"docs/cpc2/cpc2_intro#short-description","content":"The task involves estimating the intelligibility of speech-in-noise signals that have been processed by hearing aid algorithms and presented to listeners with hearing loss. Each signal contains a short sentence that the listeners were asked to repeat. The system you build needs to be able to predict how many of the words were recognised correctly by the listeners. It is not expected that systems can do this accurately on a per sentence basis, but rather we will rank systems on this basis of how well they perform over a large evaluation set, i.e., which system produces the lowest average estimation error. The hearing aid signals being assessed vary widely in quality. Examples of good, fair and poor signals are provided below. Your prediction algorithm needs to be able to cope with this variation. Good\tFair\tPoor Your browser does not support the audio element.\tYour browser does not support the audio element.\tYour browser does not support the audio element. Your browser does not support the audio element.\tYour browser does not support the audio element.\tYour browser does not support the audio element. "},{"title":"The data​","type":1,"pageTitle":"The 2nd Clarity Prediction Challenge","url":"docs/cpc2/cpc2_intro#the-data","content":"You will be provided with a set of training data which you can use to develop your systems. This data consists of Audio produced by a variety of (simulated) hearing aids for speech-in-noise;The corresponding clean reference signals (the original speech);Characteristics of the listeners (pure tone audiograms, etc);The measured speech intelligibility scores from listening tests, where the listener was asked to say what they heard after listening to the hearing aid processed signal. For full details of the data see the Data page. "},{"title":"The task​","type":1,"pageTitle":"The 2nd Clarity Prediction Challenge","url":"docs/cpc2/cpc2_intro#the-task","content":"You will be provided with an evaluation set containing Audio produced by a variety of (simulated) hearing aids for speech-in-noise;The audiogram of a listener;The clean reference signal (the original speech). Your task will be to produce a score (0.0 to 1.0), which should predict the proportion of words in the reference signal that the listener would be able to repeat correctly after listening to the hearing aid processed signal. We will be considering two types of system: intrusive and non-intrusive. Intrusive systems (also known as double-ended) are those that require a clean speech reference, and non-intrusive systems (also known as single-ended) are those that use the hearing aid output alone. Intrusive and non-intrusive systems will be separately ranked according to the RMSE between their predictions and the true values. To help you get started we have provided a baseline system that uses the HASPI metric to predict the speech intelligibility score. Details of this system are available on the Baseline page. For full details of the task see the rules page. "},{"title":"Registering and submitting​","type":1,"pageTitle":"The 2nd Clarity Prediction Challenge","url":"docs/cpc2/cpc2_intro#registering-and-submitting","content":"To take part in the challenge you will need to register your team and download the data. Entrants will have until 31st July to complete their submissions. Full instructions for submission are available on the Submission page. "},{"title":"ICASSP 2023 More ecologically-valid eval set","type":0,"sectionRef":"#","url":"docs/icassp2023/data/icassp2023_new_evaluation","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"ICASSP 2023 More ecologically-valid eval set","url":"docs/icassp2023/data/icassp2023_new_evaluation#overview","content":"This more ecologically-valid eval set (eval2) has been designed to answer the following research question: Can systems trained on simulated data generalise to more ecologically-valid measurement data? Recordings were carried in a real room using live talkers.The talkers were recorded on both a close microphone and also a 1st-order ambisonic microphone at the listener position. Head rotations are done using the spherical harmonic representation of the sound.HRTFs are applied to get the hearing-aid microphone signals, as for the simulated datasets. The talkers were recorded in noise-free conditions.Noise, music and speech interferers were played from loudspeaker and recorded on the ambisonic microphone.The target talker and intereferer are then mixed to create a scene with a desired SNR.The random positions of the sources and receivers were achieved using the same limitations as applied to the simulated set (e.g. target talker and listener at least 1m apart) Differences between simulated and ecologically-valid datasets: Talkers speaking and behaving different when asked to talk to a distant microphone in a real room.Real room acoustic altering sound instead of simulation using geometric room acoustic model.Directivity of interferers not omni-directional.Transducer noise on the distant ambisonic microphone.Measurements had lower order Ambisonics than used in the simulations. "},{"title":"Environment​","type":1,"pageTitle":"ICASSP 2023 More ecologically-valid eval set","url":"docs/icassp2023/data/icassp2023_new_evaluation#environment","content":"Recordings were done in the Acoustics Research Centre's listening room at the University of Salford. Mid-frequency reverberation time: 0.27sRoom dimensions: 6.6m × 5.8m × 2.8mBackground noise: 5.7 dBA Figure 1. The listening room (photo not from evaluation set recording). "},{"title":"Equipment​","type":1,"pageTitle":"ICASSP 2023 More ecologically-valid eval set","url":"docs/icassp2023/data/icassp2023_new_evaluation#equipment","content":"Close microphone: Neumann KM184 cardioidClose microphone preamp: Alice mic.amp.pak1Ambisonic microphone: Sennheiser Ambeo VRInterface: RME Fireface UFXLoudspeaker for interferer: M-audio BX8a "},{"title":"Target speech​","type":1,"pageTitle":"ICASSP 2023 More ecologically-valid eval set","url":"docs/icassp2023/data/icassp2023_new_evaluation#target-speech","content":"A new set of 1,600 sentences generated from the British National Corpus not previously used by Clarity. These were generated using the same process as before [1]. The sentences were read live by 10 actors: 5 male and 5 female. Ages ranged from 20 to 62.Actors were standing. The talker faced the ambisonic microphone. They were told to talk to that microphone and ignore the close microphone.Recorded in noise-free conditions.Each speaker recorded 160 unique sentences, in blocks of 10 talking positions.A cardioid microphone about 50 cm from the talker recorded the reference speech for HASPI and HASQI. "},{"title":"Interferers​","type":1,"pageTitle":"ICASSP 2023 More ecologically-valid eval set","url":"docs/icassp2023/data/icassp2023_new_evaluation#interferers","content":"Recordings reproduced by loudspeakers.Recordings of speech, noise and muisc same sources as CEC2 evaluation set.Each interferer recorded separately on the ambisonics microphone.Loudspeaker facing ambisonic microphone "},{"title":"Listener​","type":1,"pageTitle":"ICASSP 2023 More ecologically-valid eval set","url":"docs/icassp2023/data/icassp2023_new_evaluation#listener","content":"Recordings on a 1st order ambisonics microphone.Front of ambisonic room along x-axis of room.Head rotation done virtually via spherical harmonics with the same statistics as the training set.HRTFs applied to the ambisonic recordings using a virtual loudspeaker set-up to give the signals on the hearing aid microphones. "},{"title":"Talker, noise and listener position​","type":1,"pageTitle":"ICASSP 2023 More ecologically-valid eval set","url":"docs/icassp2023/data/icassp2023_new_evaluation#talker-noise-and-listener-position","content":"16 different room layouts (see Figure 2) with random talker, interferer and listener positions. These positions determined using the same protocol as used for the simulation. A block of 10 sentences read for each layout.Sources and receivers at the same height (but some variation in the talker z-coordinate because of height differences in the actors). Figure 2. The 16 layouts. T talker; A ambisonic mic; N noise interferer; S speech interferer; M music interferer. "},{"title":"Publication​","type":1,"pageTitle":"ICASSP 2023 More ecologically-valid eval set","url":"docs/icassp2023/data/icassp2023_new_evaluation#publication","content":"The target speech and interferers will be mixed to gain the desired signal to noise ratio using the same process as for the simulation set. The dataset will be available 1st February 2023. "},{"title":"Example recordings​","type":1,"pageTitle":"ICASSP 2023 More ecologically-valid eval set","url":"docs/icassp2023/data/icassp2023_new_evaluation#example-recordings","content":"Recording of script reading by someone not used for the evaluation set. The audio starts 3-4 seconds into the recording. Close microphone: Your browser does not support the audio element. Ambisonic microphone, A-format: Front-left-up: Your browser does not support the audio element. Front-right-down: Your browser does not support the audio element. Back-left-down: Your browser does not support the audio element. Back-right-up: Your browser does not support the audio element. "},{"title":"References​","type":1,"pageTitle":"ICASSP 2023 More ecologically-valid eval set","url":"docs/icassp2023/data/icassp2023_new_evaluation#references","content":"[1] Graetzer, S., Akeroyd, M.A., Barker, J., Cox, T.J., Culling, J.F., Naylor, G., Porter, E. and Viveros-Muñoz, R., 2022. Dataset of British English speech recordings for psychoacoustics and speech processing research: The clarity speech corpus. Data in Brief, 41, p.107951. "},{"title":"Data description","type":0,"sectionRef":"#","url":"docs/cpc2/cpc2_data","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"Data description","url":"docs/cpc2/cpc2_data#overview","content":"The training data essentially consists of signals and corresponding listener responses that you can use to train a prediction model. To maximise the value of the data we have imposed a cross-validation evaluation design: The training data has been split into three partitions, train.1, train.2 and train.3 which are paired with three disjoint evaluation partitions eval.1, eval.2 and eval.3 which will be released. You are asked to train three versions of your final system, i.e., one for each training data subset. This will ensure that we can evaluate your system on unseen listeners and hearing aid systems. Note, some signals and responses come from CEC1 and some from CEC2. CEC1 used simple scenes with a single interferer; CEC2 had multiple interferers. The evaluation data will only use CEC2 scenes. "},{"title":"The hearing aid output signals​","type":1,"pageTitle":"Data description","url":"docs/cpc2/cpc2_data#the-hearing-aid-output-signals","content":"The hearing aid output signals are stored under clarity_data/HA_output and separated into three separate directories, train.1, train.2 and train.3. Each of these directories contains two subdirectories, CEC1 and CEC2, which contain the hearing aid output signals from the CEC1 and CEC2 datasets respectively. The signals are stored in 16-bit stereo WAV format, with a sampling rate of 32 kHz. The signals are named according to the following convention: &lt;SCENE_ID&gt;_&lt;LISTENER_ID&gt;_&lt;SYSTEM_ID&gt;.wav # e.g., S09463_L0242_E009.wav  Where &lt;SCENE_ID&gt; is the scene identifier, &lt;LISTENER_ID&gt; is the listener identifier and &lt;SYSTEM_ID&gt; is the hearing aid system identifier. "},{"title":"The scene reference signals​","type":1,"pageTitle":"Data description","url":"docs/cpc2/cpc2_data#the-scene-reference-signals","content":"The target reference signals and hearing aid input signals are stored under clarity_data/scenes and separated into two separate directories, CEC1 and CEC2, which contain the target reference signals from the CEC1 and CEC2 datasets respectively. (Note, data for the three training set partitions is stored in the same directory.) There are a set of stereo audio files for each scene, as follows: &lt;SCENE_ID&gt;_target_ref.wav # The target reference signal for the intrusiveness intelligibility prediction task &lt;SCENE_ID&gt;_target_anechoic.wav # The anechoic speech target signal &lt;SCENE_ID&gt;_target_&lt;CHANNEL&gt;.wav # The target speech signal for the scene &lt;SCENE_ID&gt;_interferer_&lt;CHANNEL&gt;.wav # The interfering noise for the scene &lt;SCENE_ID&gt;_mixed_&lt;CHANNEL&gt;.wav # The mixed target and interfering noise.  where &lt;SCENE_ID&gt; is the scene identifier and &lt;CHANNEL&gt; can be either CH0, CH1, CH2 or CH3. The channels CH1, CH2 and CH3 are the front, middle and rear hearing aid microphones respectively (each is stereo pair). CH0 is the eardrum signal, i.e., as would be received by the listener's ear canal. Of these signals, the following is the most important: &lt;SCENE_ID&gt;_target_ref.wavThis is the signal that should be used as the reference for your intrusive intelligibility prediction model. Note, this is the only signal that will be available in the evaluation data. It is a non-reverberant version of the target signal aligned with the target component of the mixed signal received by the hearing aid. It has been scaled to have the same energy as the target component of the mixed signal received by the hearing aid. The remaining hearing aid input signals are provided for completeness &lt;SCENE_ID&gt;_mixed_&lt;CHANNEL&gt;.wav The noisy speech signals that were received by the hearing aid, i.e. the signals that were processed to produce the HA output signals.&lt;SCENE_ID&gt;_target_&lt;CHANNEL&gt;.wav The target speech component of the mixed signals that were received by the hearing aid.&lt;SCENE_ID&gt;_interferer_&lt;CHANNEL&gt;.wav The interfering noise component of the mixed signals that were received by the hearing aid.&lt;SCENE_ID&gt;_target_anechoic.wav The anechoic target speech signal (i.e., same as the target reference signal but without the correct scaling). It is not anticipated that you will necessarily need these signals for training prediction models but they have been included to help participants gain a better understanding of the data. "},{"title":"The metadata​","type":1,"pageTitle":"Data description","url":"docs/cpc2/cpc2_data#the-metadata","content":"The metadata directory (clarity_data/metadata) stores the listener responses to the signals, the listener characteristics and metadata related to each of the scenes (e.g., interferer types, input SNR, etc.). You will find the following JSON format files, CEC1.train.1.json, CEC2.train.1.json CEC1.train.2.json, CEC2.train.2.json CEC1.train.3.json, CEC2.train.3.json listeners.json scenes.CEC1.json, scenes.CEC2.json  The contents of these files are as follows. "},{"title":"The listener responses (CECx.train.x.json)​","type":1,"pageTitle":"Data description","url":"docs/cpc2/cpc2_data#the-listener-responses-cecxtrainxjson","content":"The CEC1.train.x.json and CEC2.train.x.json files contains a list of dictionaries, each describing a listener response to a signal. The fields are as follows:  { &quot;prompt&quot;: &quot;i don't want us to apportion blame she said&quot;, &quot;scene&quot;: &quot;S08547&quot;, &quot;n_words&quot;: 9, &quot;hits&quot;: 4, &quot;listener&quot;: &quot;L0239&quot;, &quot;system&quot;: &quot;E001&quot;, &quot;correctness&quot;: 44.4444444444, &quot;response&quot;: &quot;i don't want to have to report he said&quot;, &quot;volume&quot;: 56, &quot;signal&quot;: &quot;S08547_L0239_E001&quot; }  In the above, signal identifies the hearing aid output signal that you will find in the the HA_outputs\\train.1\\CEC1 or HA_outputs\\train.1\\CEC2 directorieslistener is the ID of the listener who provided the response. Using this you can look up the listener's audiogram in the listeners.json file.correctness is the percentage of words that the listener correctly identified. This is the number that you are being asked to predict.volume is the value of the volume control on the hearing aid that the listener used to listen to the signal. This is on a scale of 0 to 100 and was set by default to 50 but listeners were free to adjust it at the start of each session to achieve a comfortable listening level. "},{"title":"The listener characteristics (listeners.json)​","type":1,"pageTitle":"Data description","url":"docs/cpc2/cpc2_data#the-listener-characteristics-listenersjson","content":"The listeners.json provides the pure tone audiogram of the left and right ear of each listener. This is stored as a dictionary with the listener ID as the key to facilitate easy look-up. For each listener the audiogram is stored as a list of frequencies and the corresponding list of levels for the left and right ear. The frequencies are in Hz and the levels are in dB HL. { &quot;L0200&quot;: { &quot;name&quot;: &quot;L0200&quot;, &quot;audiogram_cfs&quot;: [250, 500, 1000, 2000, 3000, 4000, 6000, 8000], &quot;audiogram_levels_l&quot;: [35, 30, 25, 50, 55, 65, 70, 65], &quot;audiogram_levels_r&quot;: [45, 40, 35, 60, 65, 75, 80, 75], }, ... }  Note, listener audiograms will also be provided in the evaluation data, however, the listeners in the evaluation sets will not overlap with listeners in the corresponding training sets, i.e., your predictions systems are expected to be able generalise to new listeners. "},{"title":"The scene metadata (scenes.CECx.json)​","type":1,"pageTitle":"Data description","url":"docs/cpc2/cpc2_data#the-scene-metadata-scenescecxjson","content":"The scene metadata contains information about the scene, the target and interfering noise signals, and the SNR of the mixed signal, etc. The data will not be available for the evaluation signals and is being provided here for context and to help participants gain an understanding of the signals. For a complete description of the scene metadata please see the documentation for the CEC1 and CEC2 challenges. The scenes.CEC1.json has the format described here, and the scenes.CEC2.json has the format described here. "},{"title":"Scene Generation","type":0,"sectionRef":"#","url":"docs/icassp2023/data/icassp2023_scene_generation","content":"","keywords":""},{"title":"The scenario​","type":1,"pageTitle":"Scene Generation","url":"docs/icassp2023/data/icassp2023_scene_generation#the-scenario","content":"The scenario is someone listening to a target speaker in a room with two or three interfering sound sources (Figure 1). The scenes are described by a large number of randomised parameters: The room size and materials (which create moderate reverberation typical of a living room).The identity of the target talker (one of 40 possible speakers).The 7-10 word sentence being uttered by the target talker.The listener, target talker and noise interferer locations.The head orientation of the listener. Initially, the listener is not facing the target talker, but around the time the target speech starts, the listener rotates their head to face the target approximately.The interferer sound samples, which can be a: stream of competing speech; continuous domestic noise source (e.g., a washing machine); or music source.The speech onset and offset times.While scene generating software is provided, we anticipate most entrants would use our database of pre-mixed signals. The website will provide a full description of the scene generation.The main audio signals provided are for 3 microphones on two Behind-The-Ear (BTE) hearing aids (left and right ear). While scene generating software is provided, we anticipate most entrants would use our database of pre-mixed signals. The website will provide a full description of the scene generation. The main audio signals provided are for 3 microphones on two Behind-The-Ear (BTE) hearing aids (left and right ear). Figure 1. An example scenario with two noise interferers. "},{"title":"ICASSP 2023 Clarity Challenge Download","type":0,"sectionRef":"#","url":"docs/icassp2023/icassp2023_download","content":"","keywords":""},{"title":"Software​","type":1,"pageTitle":"ICASSP 2023 Clarity Challenge Download","url":"docs/icassp2023/icassp2023_download#software","content":"All the necessary software tools are available as a single GitHub repository. A new version of the repository code, v0.2.0, has been released for use with the challenge. It contains a recipe for running the baseline and standard evaluation (recipes/icassp2023). "},{"title":"Data​","type":1,"pageTitle":"ICASSP 2023 Clarity Challenge Download","url":"docs/icassp2023/icassp2023_download#data","content":"The data is available for download here. On the download site you will see four data packages are available, clarity_CEC2_core.v1_1.tgz [28 GB] - metadata and dev set clarity_CEC2_train.v1_1.tgz [69 GB] - scenes for training systems clarity_CEC2_hoairs.v1_0.tgz [144 GB] - impulse responses for generating extended training data clarity_CEC2_icassp2023_eval.v1_0.tgz [6.5 GB] - the eval1 and eval2 evaluation sets (added on 2nd Feb 2023). All participants will require the core data package. Participants using machine learning approaches will additionally require the train data package. Participants wishing to extend the training set by using our provided scene rendering tools will also require the high order ambisonic impulse responses (i.e., the hoairs package). To unpack the data we recommend you follow the instructions in the Clarity Challenge GitHub repository. "},{"title":"The ICASSP 2023 Clarity Challenge","type":0,"sectionRef":"#","url":"docs/icassp2023/icassp2023_intro","content":"","keywords":""},{"title":"Organisers​","type":1,"pageTitle":"The ICASSP 2023 Clarity Challenge","url":"docs/icassp2023/icassp2023_intro#organisers","content":"Michael Akeroyd, Graham Naylor, University of NottinghamJon Barker, Will Bailey, Zehai Tu, University of SheffieldTrevor Cox, Simone Graetzer, University of SalfordJohn Culling, Cardiff University "},{"title":"Overview of challenge​","type":1,"pageTitle":"The ICASSP 2023 Clarity Challenge","url":"docs/icassp2023/icassp2023_intro#overview-of-challenge","content":"Speech enhancement is a major research area with thousands of papers each year, yet only a tiny percentage of these explicitly consider improvements for listeners who have a hearing loss. Consequently, this signal processing challenge is designed to get the latest advancements in speech enhancement applied to hearing aids. Entrants are tasked to enhance speech-in-noise for input into a hearing aid amplification stage. The hearing aid will be tuned to the hearing characteristics of particular people. Thus you can enter without in-depth knowledge of hearing aids, and just concentrate on the task of de-noising. The scenario is listening to speech in the presence of typical domestic noise. We provide the signals captured by the microphones on a pair of behind-the-ear hearing aids and those captured at the eardrum. The target speech will be a short sentence. The interfering noises will be a mix of speech, domestic appliance noise and music. The audio includes the simulation of the acoustic of typical small living rooms. The challenge is to improve the speech intelligibility without excessive loss of quality. To this end, entries will be evaluated using an objective metric that is an average of the Hearing Aid Speech Perception Index (HASPI) and Hearing Aid Speech Quality Index (HASQI). "},{"title":"What is be provided​","type":1,"pageTitle":"The ICASSP 2023 Clarity Challenge","url":"docs/icassp2023/icassp2023_intro#what-is-be-provided","content":"Premixed speech + interferer scenes for training and evaluation.Databases of target sentences, along with speech, noise and music interferers.Listener characteristics, including audiograms and speech-in-noise testing.Software including tools for augmenting training data, a baseline enhancement system, a fixed hearing aid implementation and code for scoring signals using the HASPI and HASQI hearing aid metrics. The scenario is similar to the second Clarity Enhancement Challenge but with the following key differences: Participants are asked to focus on speech enhancement only. Hearing aid processing/simulation is not part of the challengeSpeech quality (HASQI) will be assessed in conjunction with speech intelligibility (HASPI) This site provides access to all the software, data and information that you need to get started. "},{"title":"Find collaborators","type":0,"sectionRef":"#","url":"docs/icassp2023/icassp2023_find_a_team","content":"Find collaborators If you'd like to team up with someone else to compete in the challenges, we can help. Please complete this Google form to let us know your own expertise, and what you're looking for in a collaborator. We'll then put people in contact with possible collaborators. We encourage everyone to join the Clarity Challenge’s Google group to stay updated with project news and announcements. We post in there when we have new people seeking team members (we don't share any personally-identifying details to the group). You are welcome to contact us if you have any questions about forming a team or participating in the challenge: Email the Clarity Team","keywords":""},{"title":"Modelling the scenario","type":0,"sectionRef":"#","url":"docs/icassp2023/data/cec2_scenario","content":"","keywords":""},{"title":"The scenario​","type":1,"pageTitle":"Modelling the scenario","url":"docs/icassp2023/data/cec2_scenario#the-scenario","content":"We want entrants to improve speech in the presence of background noise; see Figure 1. On the left there is a person with a quantified hearing loss who is listening to speech from the target talker on the right. Both people are in a living room. There is interfering noise from a number of sources (a TV and washing machine in this case). The speech and noise are sensed by microphones on the hearing aids of the listener. The task is to take these microphone feeds and the listener’s hearing characteristics, and produce signals for the hearing aid processor that will make the speech more intelligible. We will evaluate the success of the processing using a combination of objective metrics for speech intelligibility and quality. Figure 1. The scenario involves one talker, a listener who rotates their head, and at least two common sources of unwanted sound. "},{"title":"Baseline system and software tools​","type":1,"pageTitle":"Modelling the scenario","url":"docs/icassp2023/data/cec2_scenario#baseline-system-and-software-tools","content":"Challenge entrants are supplied with an end-to-end baseline system. Figure 2 shows a simplified schematic, which comprises: Figure 2. Baseline schematic. A scene generator (blue box) creates speech in noise (SPIN).A listener is chosen (green ellipse), so the processing can be individualised for each listener with quantified hearing characteristics.The speech is enhanced (pink box). The entrants are tasked to improve this.The hearing aid we provide then amplifies the improved speech (yellow box)The amplified and improved speech that is emitted by your hearing aid is then passed to the prediction stage (red boxes). A combination of HASPI and HASQI is the output of the objective metrics for intelligibility and quality respectively (Kates and Arehart, 2021, Kates and Arehart 2014).All software tools will be available as a single GitHub repository. The software is split into core components e.g. HASPI, HASQI, and additional tools e.g. a hearing loss model. All software is open-source and in Python. "},{"title":"Room geometry​","type":1,"pageTitle":"Modelling the scenario","url":"docs/icassp2023/data/cec2_scenario#room-geometry","content":"Cuboid rooms with dimensions length LLL by width WWW by height HHH.Length LLL set using a uniform probability distribution random number generator with 3 &lt;&lt;&lt; LLL (m) ≤≤≤ 8.Height HHH set using a Gaussian distribution random number generator with a mean of 2.7 m and standard deviation of 0.8 m.Area L×WL×WL×W set using a Gaussian distribution random number generator with mean 17.7 m2^22 and standard deviation of 5.5 m2^22 "},{"title":"Room materials​","type":1,"pageTitle":"Modelling the scenario","url":"docs/icassp2023/data/cec2_scenario#room-materials","content":"One of the walls of the room is randomly selected for the location of the door. The door can be at any position with the constraint of being at least 20 cm from the corner of the wall. A window is placed on one of the other three walls. The window could be at any position of the wall but at 1.9 m height and at 0.4 m from any corner. The curtains are simulated to the side of the window. For larger rooms, a second window and curtains are simulated following a similar methodology. A sofa is simulated at a random position as a layer on the wall and the floor. Finally, a rug is simulated at a random location on the floor. "},{"title":"The listener (receiver)​","type":1,"pageTitle":"Modelling the scenario","url":"docs/icassp2023/data/cec2_scenario#the-listener-receiver","content":"The listener has position, r⃗=(xr,yr,zr)\\vec{r} = (x_r,y_r,z_r)r=(xr​,yr​,zr​) This is positioned within the room using uniform probability distribution random number generators for the x and y coordinates (see Figure 2 for origin location). There are constraints to ensure that the receiver is not too close to the wall: −W/2+1≤xr≤W/2−1-W/2+1 \\le x_r \\le W/2-1−W/2+1≤xr​≤W/2−11≤yr≤L−11 \\le y_r \\le L-11≤yr​≤L−1zrz_rzr​ either 1.2 m (sitting) or 1.6 m (standing). "},{"title":"Head rotation​","type":1,"pageTitle":"Modelling the scenario","url":"docs/icassp2023/data/cec2_scenario#head-rotation","content":"The listener is initially oriented away from the target and will turn to be roughly facing the target talker around the time when the target speech starts Orientation of listener at start of the sample ~25° from facing the target (standard deviation = 5°), limited to +-2 standard deviations.Start of rotation is between -0.635 s to 0.865s (rectangular probability)The rotation lasts for 200 ms (standard deviation =10 ms)Orientation after rotation is 0-10° (random with rectangular probability distribution). "},{"title":"The target talker​","type":1,"pageTitle":"Modelling the scenario","url":"docs/icassp2023/data/cec2_scenario#the-target-talker","content":"​​The target talker has position t⃗=(xt,yt,zt)\\vec{t} = (x_t,y_t,z_t)t=(xt​,yt​,zt​) The target talker is positioned within the room using uniform probability distribution random number generators for the coordinates. Constraints ensure the target is not too close to the wall or receiver. It is set to have the same height as the receiver. −W/2+1≤xt≤W/2−1-W/2+1 \\le x_t \\le W/2-1−W/2+1≤xt​≤W/2−11≤yt≤L−11 \\le y_t \\le L-11≤yt​≤L−1∣r−t∣&gt;1|r-t| &gt; 1∣r−t∣&gt;1zt=zrz_t=z_rzt​=zr​ A speech directivity pattern is used, which is directed at the listener. The target speech starts between 1.0 and 1.5 seconds into the mixed sound files (rectangular probability distribution). "},{"title":"The interferers​","type":1,"pageTitle":"Modelling the scenario","url":"docs/icassp2023/data/cec2_scenario#the-interferers","content":"The interferers have position i1,2,3⃗=(xi,yi,zi)\\vec{i_{1,2,3}} = (x_i,y_i,z_i)i1,2,3​​=(xi​,yi​,zi​) Each interferer is modelled as an omnidirectional point source. They will be radiating: speech, noise or music. They are placed within the room using uniform probability distribution random number generators for the coordinates. The following constraints ensure the interferer is not too close to the wall or listener. However, interferers are independently positioned with no constraint on their position relative to each other. They are set to be at the same height as the listener. Note, this means that the interferers can be at any angle relative to the listener. −W/2+1≤xi≤W/2−1-W/2+1 \\le x_i \\le W/2-1−W/2+1≤xi​≤W/2−11≤yi≤L−11 \\le y_i \\le L-11≤yi​≤L−1∣r−i∣&gt;1|r-i| \\gt 1∣r−i∣&gt;1zi=zrz_i = z_rzi​=zr​ The interferers are present over the whole mixed sound file. "},{"title":"Signal-to-noise ratio (SNR)​","type":1,"pageTitle":"Modelling the scenario","url":"docs/icassp2023/data/cec2_scenario#signal-to-noise-ratio-snr","content":"The SNR of the mixtures are engineered to achieve a suitable range of speech intelligibility values. A desired signal-to-noise ratio, SNRD (dB), is chosen at random. This is generated with a uniform probability distribution between limits determined by pilot listening tests. The better ear SNR (BE_SNR) models the better ear effect in binaural listening. It is calculated for the reference channel (channel 1, which corresponds to the front microphone of the hearing aid). This value is used to scale all interferer channels. The procedure is described below. For the reference channel, The segment of the summed interferers that overlaps with the target (without padding), i′i'i′, and the target (without padding), t′t't′, are extractedSpeech-weighted SNRs are calculated for each ear, SNRL_LL​ and SNRR_RR​: Signals i′i'i′ and t′t't′ are separately convolved with a speech-weighting filter, h (specified below).The rms is calculated for each convolved signal.SNRL_LL​ and SNRR_RR​ are calculated as the ratio of these rms values. The BE_SNR is selected as the maximum of the two SNRs: BE_SNR = max(SNRL_LL​ and SNRR_RR​). Then per channel, The summed interferer signal, i, is scaled by the BE_SNR i=i×i = i \\timesi=i× BE_SNR Finally, i is scaled as follows: i=i×10((−SNRD)/20)i = i \\times 10^{((-SNR_D)/20)}i=i×10((−SNRD​)/20) The speech-weighting filter is an FIR designed using the host window method [2, 3]. The frequency response is shown in Figure 2. The specification is: Frequency (Hz) = [0, 150, 250, 350, 450, 4000, 4800, 5800, 7000, 8500, 9500, 22050]Magnitude of transfer function at each frequency = [0.0001, 0.0103, 0.0261, 0.0419, 0.0577, 0.0577, 0.046, 0.0343, 0.0226, 0.0110, 0.0001, 0.0001] Figure 2, Speech weighting filter transfer function graph. "},{"title":"References​","type":1,"pageTitle":"Modelling the scenario","url":"docs/icassp2023/data/cec2_scenario#references","content":" Schröder, D. and Vorländer, M., 2011, January. RAVEN: A real-time framework for the auralization of interactive virtual environments. In Proceedings of Forum Acusticum 2011 (pp. 1541-1546). Denmark: Aalborg.Abed, A.H.M. and Cain, G.D., 1978. Low-pass digital filtering with the host windowing design technique. Radio and Electronic Engineer, 48(6), pp.293-300.Abed, A.E. and Cain, G., 1984. The host windowing technique for FIR digital filter design. IEEE transactions on acoustics, speech, and signal processing, 32(4), pp.683-694. "},{"title":"Baseline System","type":0,"sectionRef":"#","url":"docs/icassp2023/software/icassp2023_baseline","content":"","keywords":""},{"title":"Baseline performance​","type":1,"pageTitle":"Baseline System","url":"docs/icassp2023/software/icassp2023_baseline#baseline-performance","content":"Baseline performance using amplification with no enhancement will appear shortly. "},{"title":"References​","type":1,"pageTitle":"Baseline System","url":"docs/icassp2023/software/icassp2023_baseline#references","content":" Kates, J.M. and Arehart, K.H., 2021. The hearing-aid speech perception index (HASPI) version 2. Speech Communication, 131, pp.35-46.Kates, J.M. and Arehart, K.H., 2014. &quot;The hearing-aid speech quality index (HASQI) version 2&quot;. Journal of the Audio Engineering Society. 62 (3): 99–117. "},{"title":"Core Software","type":0,"sectionRef":"#","url":"docs/icassp2023/software/icassp2023_core_software","content":"","keywords":""},{"title":"A. Scene generator​","type":1,"pageTitle":"Core Software","url":"docs/icassp2023/software/icassp2023_core_software#a-scene-generator","content":"Fully open-source Python code for generating hearing aid inputs for each scene Inputs: target and interferer signals, HOA-IRs, RAVEN project (rpf) files, scene description JSON filesOutputs: Mixed target+interferer signals for each hearing aid channel, direct path (simulating a measurement close to the eardrum). Reverberated pre-mixed signals can also be optionally generated. "},{"title":"B. Hearing aid enhancement stage​","type":1,"pageTitle":"Core Software","url":"docs/icassp2023/software/icassp2023_core_software#b-hearing-aid-enhancement-stage","content":"The hearing aid enhancement stage supplied simply reduces the six channel input to two channels by selection the 'front' microphone on each ear. This is the component that you are challenged to replace. Inputs: 6 channel hearing aid input (3 microphones per for each ear)Outputs: An enhanced stereo signal that is passed to the amplification stage. "},{"title":"C. The hearing aid amplification stage​","type":1,"pageTitle":"Core Software","url":"docs/icassp2023/software/icassp2023_core_software#c-the-hearing-aid-amplification-stage","content":"The hearing aid amplifier consists of a NAL-R fitting amplification stage [1] followed by a simple automatic gain compressor. It produces output signals in 16-bit wav format ready for HASPI and HASQI evaluation. Inputs: Stereo output of the enhancement stage and audiograms to characterise the listeners.Outputs: Stereo hearing aid (HA) outputs signals. "},{"title":"D. HASPI Speech Intelligibility model​","type":1,"pageTitle":"Core Software","url":"docs/icassp2023/software/icassp2023_core_software#d-haspi-speech-intelligibility-model","content":"Python implementation of the Hearing Aid Speech Perception Index (HASPI) [2] model which is used for objective intelligibility estimation. This will be one component of the evaluation metric. Inputs: reference target signal (i.e., the premixed target signal convolved with the BRIR with the reflections “turned off”, specified as ‘target_anechoic’), HA output signals, audiogram, level reference (level in dB SPL which corresponds to 0 dB FS)Outputs: predicted intelligibility score It is important to remember that both reference target and HA output signals have to be calibrated to the same dB SPL level before calculating HASPI. "},{"title":"E. HASQI Speech Quality model​","type":1,"pageTitle":"Core Software","url":"docs/icassp2023/software/icassp2023_core_software#e-hasqi-speech-quality-model","content":"Python implementation of the Hearing Aid Speech Quality Index (HASQI) [3] model which is used for objective quality estimation. This will be one component of the evaluation metric. Inputs: reference target signal (i.e., the premixed target signal convolved with the BRIR with the reflections “turned off”, specified as ‘target_anechoic’), HA output signals, audiogram, level reference (level in dB SPL which corresponds to 0 dB FS)Outputs: predicted intelligibility score It is important to remember that both reference target and HA output signals have to be calibrated to the same dB SPL level before calculating HASPI. "},{"title":"References​","type":1,"pageTitle":"Core Software","url":"docs/icassp2023/software/icassp2023_core_software#references","content":" Byrne, Denis, and Harvey Dillon. &quot;The National Acoustic Laboratories'(NAL) new procedure for selecting the gain and frequency response of a hearing aid.&quot; Ear and hearing 7.4 (1986): 257-265.Kates, J.M. and Arehart, K.H., 2021. &quot;The hearing-aid speech perception index (haspi) version 2&quot;. Speech Communication, 131, pp.35-46.Kates, J.M. and Arehart, K.H., 2014. &quot;The hearing-aid speech quality index (HASQI) version 2&quot;. Journal of the Audio Engineering Society. 62 (3): 99–117. "},{"title":"ICASSP 2023 Clarity Challenge Schedule","type":0,"sectionRef":"#","url":"docs/icassp2023/icassp2023_dates","content":"ICASSP 2023 Clarity Challenge Schedule Key dates are as follows 28th Nov 2022: Challenge launch: Release training/dev data; tools; baseline; rules &amp; documentation.2nd Feb 2023: Release of evaluation data.10th Feb 2023: Teams submit processed signals and technical reports.14th Feb 2023: Results released. Top 5 ranked teams invited to submit papers to ICASSP-202320th Feb 2023: Invited papers submitted to ICASSP-20234-9th June 2023: Overview paper and invited papers presented at dedicated ICASSP session","keywords":""},{"title":"ICASSP 2023 Clarity Grand Challenge Registration","type":0,"sectionRef":"#","url":"docs/icassp2023/taking_part/icassp2023_registration","content":"ICASSP 2023 Clarity Grand Challenge Registration Teams are required to register using the form below. Please register as soon as possible. Please submit one form per team, providing a single contact email address. Once you have registered, you will receive an email confirmation with a team ID. When the submission date approaches, you will be sent an individualised link to a Google Drive for submitting materials. Loading…","keywords":""},{"title":"ICASSP 2023 Clarity Challenge - Speech Enhancement for Hearing Aids FAQ","type":0,"sectionRef":"#","url":"docs/icassp2023/icassp2023_faq","content":"","keywords":""},{"title":"Speech Intelligibility​","type":1,"pageTitle":"ICASSP 2023 Clarity Challenge - Speech Enhancement for Hearing Aids FAQ","url":"docs/icassp2023/icassp2023_faq#speech-intelligibility","content":""},{"title":"What is Speech Intelligibility?​","type":1,"pageTitle":"ICASSP 2023 Clarity Challenge - Speech Enhancement for Hearing Aids FAQ","url":"docs/icassp2023/icassp2023_faq#what-is-speech-intelligibility","content":"The term Speech Intelligibility is generally used in two different ways. It can refer to how much speech is understood by a listener, or to the number of words correctly identified by a listener as a proportion or percentage of the total number of words. In the Clarity project, we are using the latter definition, i.e., the percentage of words in a sentence that a listener identified correctly. This percentage is the target for your prediction models. Speech intelligibility captures how a listener's ability to participate in conversation is changed when the speech signal is degraded, e.g., by background noise and room reverberation, or is processed, e.g., by a hearing aid. Your prediction model will need to incorporate a model of the hearing abilities of each listener. "},{"title":"How is Speech Intelligibility measured with listeners?​","type":1,"pageTitle":"ICASSP 2023 Clarity Challenge - Speech Enhancement for Hearing Aids FAQ","url":"docs/icassp2023/icassp2023_faq#how-is-speech-intelligibility-measured-with-listeners","content":"In the Clarity project, a set of listeners listen to a sentence and then say what words they heard. In this project, speech intelligibility is measured as the number of words identified correctly as a percentage of the total number of words in a sentence. You might consider looking at other metrics, such as Word Error Rate (WER), which picks up on, e.g., where listeners insert words not in the original sentence. You might do this if you think that an estimate of WER or other metrics would help your system to estimate speech intelligibility, as defined in the Clarity project. "},{"title":"How is Speech Intelligibility objectively measured by a computer?​","type":1,"pageTitle":"ICASSP 2023 Clarity Challenge - Speech Enhancement for Hearing Aids FAQ","url":"docs/icassp2023/icassp2023_faq#how-is-speech-intelligibility-objectively-measured-by-a-computer","content":"When fitting a hearing aid, it would be beneficial for an audiologist to be able to use an objective measure of speech intelligibility to determine what signal processing algorithm(s) should be used to compensate for the listener's hearing impairment. Objective measures are also useful when measured speech intelligibility scores are unavailable, such as when developing a machine learning-based hearing aid algorithm or some other speech enhancement method. Another advantage of non-intrusive measures is that they do not require time-alignment of processed and reference signals. Objective measures - or metrics - of speech intelligibility are used to allow a computer to estimate the likely performance of humans in listening tests. The main goal of entries to the prediction challenge is to produce one of these measures that performs well for listeners with hearing loss. There are two broad classes of speech intelligibility models: Intrusive metrics (also known as double-ended) are most common. This is where the intelligibility is estimated by comparing the degraded or processed speech signal with the original clean speech signal.Non-intrusive metrics (also known as single-ended or blind) are less well developed. This is where intelligibility is estimated from the degraded or processed speech signal alone. In the Clarity project, both types of metrics are of interest. Intrusive metrics will be more accurate in many cases. However, there are hearing aid processes where the speech content is shifted in frequency, which will defeat most current intrusive speech intelligibility metrics. We also hypothesise that there might be issues with intrusive metrics and machine learning approaches in hearing aids that revoice the original speech. "},{"title":"What speech intelligibility models already exist and what are they used for?​","type":1,"pageTitle":"ICASSP 2023 Clarity Challenge - Speech Enhancement for Hearing Aids FAQ","url":"docs/icassp2023/icassp2023_faq#what-speech-intelligibility-models-already-exist-and-what-are-they-used-for","content":"There aren't many speech intelligibility models that consider hearing impairment, but one that does is HASPI by Kates and Arehart. In this seminar from the first Clarity workshop, James Kates discusses speech intelligibility models with a focus on the ones he has developed. He also discusses the speech quality metric HASQI. If you're interested in using HASPI or HASQI for the challenge, James Kates has kindly made the MATLAB code and user guide available for download.  Click arrow to see synposis. Signal degradations, such as additive noise and nonlinear distortion, can reduce the intelligibility and quality of a speech signal. Predicting intelligibility and quality for hearing aids is especially difficult since these devices may contain intentional nonlinear distortion designed to make speech more audible to a hearing-impaired listener. This speech processing often takes the form of time-varying multichannel gain adjustments. Intelligibility and quality metrics used for hearing aids and hearing-impaired listeners must therefore consider the trade-offs between audibility and distortion introduced by hearing-aid speech envelope modifications. This presentation uses the Hearing Aid Speech Perception Index (HASPI) and the Hearing Aid Speech Quality Index (HASQI) to predict intelligibility and quality, respectively. These indices incorporate a model of the auditory periphery that can be adjusted to reflect hearing loss. They have been trained on intelligibility scores and quality ratings from both normal-hearing and hearing-impaired listeners for a wide variety of signal and processing conditions. The basics of the metrics are explained, and the metrics are then used to analyse the effects of additive noise on speech, to evaluate noise suppression algorithms, and to measure differences among commercial hearing aids. "},{"title":"Hearing Loss​","type":1,"pageTitle":"ICASSP 2023 Clarity Challenge - Speech Enhancement for Hearing Aids FAQ","url":"docs/icassp2023/icassp2023_faq#hearing-loss","content":"There are many types of hearing loss, but the focus of the Clarity project is the hearing loss that happens with ageing. This is a form of sensorineural hearing loss. "},{"title":"How does hearing loss affect the perception of audio signals, and how do modern hearing aids process sound to help with this?​","type":1,"pageTitle":"ICASSP 2023 Clarity Challenge - Speech Enhancement for Hearing Aids FAQ","url":"docs/icassp2023/icassp2023_faq#how-does-hearing-loss-affect-the-perception-of-audio-signals-and-how-do-modern-hearing-aids-process-sound-to-help-with-this","content":"In this seminar from the first Clarity workshop, Karolina Smeds from ORCA Europe and WS Audiology discusses the effects of hearing loss and the hearing aid processing strategies that are typically used to counter the sensory deficits.  Click arrow to see synposis. Hearing loss leads to several unwanted effects. Loss of audibility for soft sounds is one effect, but also when amplification is used to create audibility for soft sounds, many [suprathreshold](https://www.lexico.com/en/definition/suprathreshold) deficits remain. The most common type of hearing loss is a [cochlear](https://www.lexico.com/definition/cochlear) hearing loss, where haircells or nerve synapses in the cochlea are damaged. Ageing and noise exposure are the most common causes of cochlear hearing loss. This type of hearing loss is associated with atypical loudness perception and difficulties in noisy situations. Background noise masks for instance speech to a higher degree than for a person with healthy hair cells. This explains why listening to speech-in-noise (SPIN) is such an important topic to work on. A brief introduction to signal processing in hearing aids will be presented. With the use of frequency-specific amplification and compression (automatic gain control, AGC), hearing aids are usually doing a good job in compensating for reduced audibility and for atypical suprathreshold loudness perception. However, it is more difficult to compensate for the increased masking effect. Some examples of strategies will be presented. Finally, natural conversations in noise will be discussed. The balance between being able to have a conversation with a specific communication partner in a group of people and being able to switch attention if someone else starts to talk will be touched upon. "},{"title":"ICASSP 2023 Grand Challenge Rules","type":0,"sectionRef":"#","url":"docs/icassp2023/taking_part/icassp2023_rules","content":"","keywords":""},{"title":"Teams​","type":1,"pageTitle":"ICASSP 2023 Grand Challenge Rules","url":"docs/icassp2023/taking_part/icassp2023_rules#teams","content":"Teams must have pre-registered and nominated a contact person.Teams can be from one or more institutions. "},{"title":"Transparency​","type":1,"pageTitle":"ICASSP 2023 Grand Challenge Rules","url":"docs/icassp2023/taking_part/icassp2023_rules#transparency","content":"Teams must provide a technical document of up to 2 pages describing the system/model and any external data and pre-existing tools, software and models used.We will publish all technical documents (anonymous or otherwise).Teams are encouraged – but not required – to provide us with access to the system/model and to make their code open source.Teams may reserve the right to be referred to using anonymous code names in the published rank ordering. "},{"title":"What information can I use?​","type":1,"pageTitle":"ICASSP 2023 Grand Challenge Rules","url":"docs/icassp2023/taking_part/icassp2023_rules#what-information-can-i-use","content":""},{"title":"Training and development​","type":1,"pageTitle":"ICASSP 2023 Grand Challenge Rules","url":"docs/icassp2023/taking_part/icassp2023_rules#training-and-development","content":"For training, teams can not use external data but can expand the official training data through automated modifications and remixing, i.e. data augmentation strategies. However, teams that do this must make a second submission using only the official audio files. Any audio or metadata can be used during training and development, but during evaluation, the enhancement algorithm will not have access to all of the data (see next section). "},{"title":"Evaluation​","type":1,"pageTitle":"ICASSP 2023 Grand Challenge Rules","url":"docs/icassp2023/taking_part/icassp2023_rules#evaluation","content":"The only data that can be used by the Enhancement Processor during evaluation are The audio input signals (the sum of the target and interferers for each hearing aid microphone).The listener characterisation (pure tone air-conduction audiograms and/or digit triple test results).The provided clean audio examples for the target talker (these will not be the same as any of the target utterances.) "},{"title":"Computational restrictions​","type":1,"pageTitle":"ICASSP 2023 Grand Challenge Rules","url":"docs/icassp2023/taking_part/icassp2023_rules#computational-restrictions","content":"Teams may choose to use all, some or none of the parts of the baseline model.Systems must be causal; the output from the hearing aid at time t must not use any information from input samples more than 5 ms into the future (i.e., no information from input samples &gt;t+5 ms).There is no limit on computational cost. Please see this blog post for further explanation of these last two rules about latency and computation time. "},{"title":"Submitting multiple entries​","type":1,"pageTitle":"ICASSP 2023 Grand Challenge Rules","url":"docs/icassp2023/taking_part/icassp2023_rules#submitting-multiple-entries","content":"It is intended that there should be one submission per registered team. Submitting multiple entries is discouraged. "},{"title":"Evaluation of systems​","type":1,"pageTitle":"ICASSP 2023 Grand Challenge Rules","url":"docs/icassp2023/taking_part/icassp2023_rules#evaluation-of-systems","content":"Each signal will be scored using the average of its HASPI and HASQI scores. A system score will then be computed by averaging over the evaluation set. Separate scores will be computed for the real and simulated evaluation sets, along with a summary score formed by their average. Systems will be ranked according to their summary scores with all three (real, simulated and summary) scores being reported. "},{"title":"Intellectual property​","type":1,"pageTitle":"ICASSP 2023 Grand Challenge Rules","url":"docs/icassp2023/taking_part/icassp2023_rules#intellectual-property","content":"The following terms apply to participation in this machine learning challenge (“Challenge”). Entrants may create original solutions, prototypes, datasets, scripts, or other content, materials, discoveries or inventions (a “Submission”). The Challenge is organised by the Challenge Organiser. Entrants retain ownership of all intellectual and industrial property rights (including moral rights) in and to Submissions. As a condition of submission, Entrant grants the Challenge Organiser, its subsidiaries, agents and partner companies, a perpetual, irrevocable, worldwide, royalty-free, and non-exclusive licence to use, reproduce, adapt, modify, publish, distribute, publicly perform, create a derivative work from, and publicly display the Submission. Entrants provide Submissions on an “AS IS” BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. "},{"title":"Additional Tools","type":0,"sectionRef":"#","url":"docs/icassp2023/software/icassp2023_additional_tools","content":"","keywords":""},{"title":"Hearing loss model​","type":1,"pageTitle":"Additional Tools","url":"docs/icassp2023/software/icassp2023_additional_tools#hearing-loss-model","content":"This is an open-source python implementation of a hearing loss model developed by Brian Moore, Michael Stone and other members of the Auditory Perception Group, University of Cambridge [1, 2]. Inputs: A stereo wav audio signal, e.g., the output of the hearing aid model and audiograms for left and right ear.Outputs: The signal after simulating the hearing loss as specified by the set of audiograms (stereo wav file), &lt;scene&gt;_&lt;listener&gt;_HL-output.wav "},{"title":"Differentiable source separation and hearing aid amplification modules​","type":1,"pageTitle":"Additional Tools","url":"docs/icassp2023/software/icassp2023_additional_tools#differentiable-source-separation-and-hearing-aid-amplification-modules","content":"The modules are from the Sheffield E009 system in CEC1. The source separation module is a multi-channel Conv-TasNet optimised with a SNR objective. The hearing aid amplification module is an FIR filter optimised with an objective, which is the combination of a differentiable approximation to the hearing loss model and a STOI loss. Inputs: six channels of mixed signals, i.e., mixed_CH1.wav, mixed_CH2.wav, and mixed_CH3.wavOutputs: a single channel enhanced signal, therefore two source separation and amplification modules for left and right ears need to be optimised for the enhanced binaural signal. "},{"title":"Speech intelligibility model (MBSTOI)​","type":1,"pageTitle":"Additional Tools","url":"docs/icassp2023/software/icassp2023_additional_tools#speech-intelligibility-model-mbstoi","content":"Python implementation of a binaural intelligibility model, Modified Binaural Short-Time Objective Intelligibility (MBSTOI) [3]. Note that MBSTOI requires signal time-alignment (and alignment within one-third octave bands). Inputs: HL-model output signals, audiogram, reference target signal (i.e., the premixed target signal convolved with the BRIR with the reflections 'turned off', specified as 'target_anechoic'), (scene metadata)Outputs: predicted intelligibility score "},{"title":"References​","type":1,"pageTitle":"Additional Tools","url":"docs/icassp2023/software/icassp2023_additional_tools#references","content":" Moore, B. C. J., Alcantara, J. I., Stone, M. and Glasberg, B. R., 1999. Use of a loudness model for hearing aid fitting: II. Hearing aids with multi-channel compression. British Journal of Audiology, 33(3), pp. 157-170.Nejime, Y. and Moore, B. C., 1997. Simulation of the effect of threshold elevation and loudness recruitment combined with reduced frequency selectivity on the intelligibility of speech in noise. Journal of the Acoustical Society of America, 102(1), pp. 603-615.Andersen, A. H., de Haan, J. M., Tan, Z. H. and Jensen, J., 2018. Refinement and validation of the binaural short-time objective intelligibility measure for spatially diverse conditions. Speech Communication, 102, pp. 1-13. "},{"title":"ICASSP 2023 Submission","type":0,"sectionRef":"#","url":"docs/icassp2023/taking_part/icassp2023_submission","content":"","keywords":""},{"title":"What evaluation data is provided?​","type":1,"pageTitle":"ICASSP 2023 Submission","url":"docs/icassp2023/taking_part/icassp2023_submission#what-evaluation-data-is-provided","content":"There will be two sets of evaluation data: i) the simulate set consisting of 1500 scenes generated in the same way as the training and development data (eval1), ii) the real data consisting of real acoustic mixtures (eval2). For details see the data description page. For each scene, you are provided with the signals received at each of the three microphones on the left and right hearing aid device. You will also be provided with JSON or csv formatted metadata consisting of the audiograms for a set of listeners anda mapping of which listeners will listen to which scenes. There will also be some clean example utterances from the target talker, that are not the same as the target utterance, but which can be used to identify the target talker, i.e., to disambiguate scenes in which other speakers are present. For HASPI/HASQI evaluation, there will be one listener per scene and the scene-listener mapping will be the same for all teams. "},{"title":"What audio do I need to submit?​","type":1,"pageTitle":"ICASSP 2023 Submission","url":"docs/icassp2023/taking_part/icassp2023_submission#what-audio-do-i-need-to-submit","content":"You must submit the stereo audio signals produced at the output of your enhancement stage, which the organisers will process by the hearing aid amplification stage and the HASPI/HASQI evaluation metric. Signals should be submitted as stereo, floating point wav format signals, at the same sampling rate as the signals provided "},{"title":"Naming and packaging signals​","type":1,"pageTitle":"ICASSP 2023 Submission","url":"docs/icassp2023/taking_part/icassp2023_submission#naming-and-packaging-signals","content":"Your processed signals should be named using the conventions used by the baseline system, i.e., &lt;Scene ID&gt;_&lt;Listener ID&gt;_enhanced.wav and explained on the data page. Place the processed signals for the two sets into separate directories named eval1 and eval2. These should be placed in a directory whose name is the unique team ID that you will be sent, e.g., ICASSP2023_E001 and then packaged using zip or tar or any standard packaging tool, e.g., to make a packaged file called &lt;TEAM_ID&gt;.zip The packaged file will have the following structure, ICASSP2023_E001 ├── eval1 (1500 processed signals) └── eval2 (1500 processed signals)  The resulting file should be about 4 GB. Upload the packaged data to the Google Drive link that you will have been sent. "},{"title":"Using head rotation data and/or extended training data​","type":1,"pageTitle":"ICASSP 2023 Submission","url":"docs/icassp2023/taking_part/icassp2023_submission#using-head-rotation-data-andor-extended-training-data","content":"We would like to be able to separately evaluate the benefit of using the head rotation and extra training data, so in accordance with the challenge rules, If you have trained on data which was not included in the core database, then please also provide outputs of a system trained only with the standard data. If you have made use of the head rotation data you should also provide outputs of an equivalent system that does not use the head rotation data. If you have used extra training data and/or the head rotation data, then please package the outputs separately using the following naming convention, ‹TEAM_ID›.zip - standard training data and no head rotation (all teams) ‹TEAM_ID›_hr.zip - standard data and using head rotation ‹TEAM_ID›_data.zip - extended training data without using head rotation ‹TEAM_ID›_hr_data.zip - extended training data and using head rotation "},{"title":"Technical report​","type":1,"pageTitle":"ICASSP 2023 Submission","url":"docs/icassp2023/taking_part/icassp2023_submission#technical-report","content":"For every entry, a technical report needs to be uploaded to the Google Drive along with your evaluation signals - see here for deadline. The draft needs to be sufficiently complete for us to judge whether your system is compliant with the challenge rules.Your report should include an abstract and introduction and sections on experimental setup/methodology including system information and model/network architecture, evaluation/results, discussion, conclusion and references. Please provide an estimation of the computational resources needed. You must describe any external data and pre-existing tools, software and models used.The report can be placed in the Google Drive alongside your data.The top five systems will be invited to submit a paper to the ICASSP 2023 special session. "},{"title":"How will intellectual property be handled?​","type":1,"pageTitle":"ICASSP 2023 Submission","url":"docs/icassp2023/taking_part/icassp2023_submission#how-will-intellectual-property-be-handled","content":"See here under Intellectual Property. "},{"title":"Where do I submit the signals?​","type":1,"pageTitle":"ICASSP 2023 Submission","url":"docs/icassp2023/taking_part/icassp2023_submission#where-do-i-submit-the-signals","content":"When you have registered you will receive a link to a Google Drive to which you will be able to securely upload your signals. We also encourage you to submit your enhancement code via this link. Materials uploaded will be visible to the Clarity Team but not to other entrants. danger Note, in order to use the Google Drive you will need to have a Google account. If you anticipate problems using Google then please make arrangements to send us the materials by other means, e.g., via a service such as WeTransfer or similar. "},{"title":"ICASSP 2023 Data","type":0,"sectionRef":"#","url":"docs/icassp2023/data/icassp2023_data","content":"","keywords":""},{"title":"A. Training, development and evaluation data​","type":1,"pageTitle":"ICASSP 2023 Data","url":"docs/icassp2023/data/icassp2023_data#a-training-development-and-evaluation-data","content":"The dataset of 10,000 simulated scenes is split into three sets: 6000 training scenes (available now)2500 development scenes (available now)1500 evaluation scenes (released 1st Feb. 2023) In addition there will be: A secondary 'real data' evaluation set that will be based on real ecologically-valid recordings and so can highlight the generalizability of the entrants’ approaches beyond the simulations (released 1st February 2023). More information. "},{"title":"B. The scene dataset​","type":1,"pageTitle":"ICASSP 2023 Data","url":"docs/icassp2023/data/icassp2023_data#b-the-scene-dataset","content":"For the dataset of 10,000 simulated scenes Each scene corresponds to a unique target utterance and unique segment(s) of noise from the interferers.The training, development and evaluation sets are disjoint with respect to the target speakers.Sets are balanced for the gender of the target talker.Entrants must not use the development or evaluation data sets for training.The system submitted should be chosen on the evidence provided by the development set. For evaluation The final ranking will be performed with the (held-out) evaluation sets.Neither evaluation datasets (simulation nor real) have been used in previous Clarity challenges.The secondary 'real data' evaluation set will be made using real acoustic mixtures but using loudspeaker playback of target talkers so that the reference speech can be extracted as needed by the objective metrics. For the training and development set, entrants have access to a diverse range of signals and metadata, with the most important being: The hearing aid microphone signalsThe hearing characteristics of the listener (e.g. audiogram)The anechoic target reference and interferer signals. For training, teams can not use external data but can expand the official training data through automated modifications and remixing, i.e. data augmentation strategies. However, teams that do this must make a second submission using only the official audio files. For evaluation, the data available is more limited, i.e., The hearing aid microphone signalsThe hearing characteristics of the listener (e.g. audiogram)The anechoic target reference signal which will be used by the organisers but not released to entrants. High-Order Ambisonic Impulse Responses (HOA-IRs) and Head-Related Impulse Response (HRIRs) are used to model how the sound is altered as it propagates through the room and interacts with the head. See the page on scene generation for more details. Time-domain acoustic signals are generated for: A hearing aid with 3 microphone inputs (front, mid, rear). The hearing aid has a Behind-The-Ear (BTE) form factor; see Figure 1. The distance between microphones is approx. 7.6 mm. The properties of the tube and ear mould are not considered.Close to the eardrum.The anechoic target reference (front microphone). Figure 1. Front (Fr), Middle (Mid) and Rear microphones on a BTE hearing aid form. Head Related Impulse Responses (HRIRs) are used to model how sound is altered as it propagates in a free-field and interacts with the head (i.e., no room is included). These are taken from the OlHeadHRTF database with permission. These include HRIRs for human heads and for three types of head-and-torso simulator/mannekin. The eardrum HRIRs (labelled ED) are for a position close to the eardrum of the open ear. rpf files and ac files are specification files for the geometric room acoustic model that include a complete description of the room, both in terms of geometry and room materials. "},{"title":"B.1 Training data​","type":1,"pageTitle":"ICASSP 2023 Data","url":"docs/icassp2023/data/icassp2023_data#b1-training-data","content":"For each scene in the training data the following signals and metadata are available: The target and interferer HOA-IRs (4 pairs: front, mid, rear and eardrum for left and right ears).The mono target and interferer signals (pre-convolution).For each hearing aid microphone (channels 1-3 where channel 1 is front, channel 2 is mid and channel 3 is rear) and a position close to the eardrum (channel 0): The target convolved with the appropriate HOA-IRs and downmixed;The interferers convolved with the appropriate HOA-IRs and downmixed;The sum of the target and interferer convolved with the appropriate HOA-IRs and downmixed; (i.e. the noisy signals that would be received by the hearing aid) The target convolved with the anechoic HOA-IRs and downmixed for channel 1 for each ear (‘target_anechoic’). For use as a reference when computing HASPI scores.Metadata describing the scene: a JSON file containing, e.g., the filenames of the sources, the location of the sources, the viewvector of the target source, the location and viewvector of the receiver, the room dimensions (see specification below), and the room number, which corresponds to the RAVEN BRIR, rpf and ac files.A signal describing the head rotation (i.e. azimuthal angle at each sample) "},{"title":"B.2 Development data​","type":1,"pageTitle":"ICASSP 2023 Data","url":"docs/icassp2023/data/icassp2023_data#b2-development-data","content":"This is made available to allow you to fully examine the performance of your system. Ground truth data (i.e., the premixed target and interferers are available in the development set) Development data also contains target speaker adaptation sentences, i.e., four utterances from each of the target speakers. These will also be available in the evaluation data. i.e., systems can use these utterances in conjunction with the known target ID to inform their system of the which speaker in the scene should be attended. Note, that the data available for the evaluation will be much more limited, e.g. it will not contain premixed ground truth signals or scene metadata, (see Section B.3). When using the development data for evaluation, your hearing aid enhancement model should only be using the types of data available in the evaluation data set (see below). "},{"title":"B.3 Simulated Evaluation data (eval1)​","type":1,"pageTitle":"ICASSP 2023 Data","url":"docs/icassp2023/data/icassp2023_data#b3-simulated-evaluation-data--eval1","content":"The following data will only be available: Audio: the sum of the target and interferers for each hearing aid microphone.The ID of the listener who will be auditioning the processed scene.The listener characterisation data for these listeners.ID of target talker and a few examples of clean audio that are not the same as the target utterance.The head rotation signal, i.e. as might be recovered from hearing aid motion sensors. (Systems can use this signal but should also be evaluated without using it.)Speaker adaptation sentence - 4 clean utterances for each target speaker. One challenge will be identifying the target talker from the hearing aid microphone signals. There are two possibilities: The ID of the target talker is given with examples of clean audio. This would allow an algorithm to learn characteristics of the target talker to then help it identify the voice in the mixture.The azimuth of the target and the starting time of the utterance are both roughly known from the scene generation metadata statistics. These two approaches mimic what is available to human listeners. They might focus on a known voice or they might use visual cues to know roughly where and when someone is talking. "},{"title":"B.4 Real Evaluation data (eval2)​","type":1,"pageTitle":"ICASSP 2023 Data","url":"docs/icassp2023/data/icassp2023_data#b4-real-evaluation-data-eval2","content":"The following data will only be available: Audio: the sum of the target and interferers for each hearing aid microphone.The ID of the listener who will be auditioning the processed scene.The listener characterisation data for these listeners.ID of target talker and a few examples of clean audio that are not the same as the target utterance.Speaker adaptation sentence - 4 clean utterances for each target speaker.Further details to be confirmed. "},{"title":"C Listener data​","type":1,"pageTitle":"ICASSP 2023 Data","url":"docs/icassp2023/data/icassp2023_data#c-listener-data","content":"We will provide metadata characterising the hearing abilities of the listeners so the audio signals you generate for evaluation can be individualised to the specific listeners who will be hearing them. The same types of data are available for training, development and evaluation. A panel of hearing-aided listeners will be recruited for evaluation. They will be experienced bilateral hearing-aid users: they use two hearing aids but the hearing loss may be asymmetrical. The average pure tone air-conduction hearing loss will be between 25 and about 60 dB in the better ear. They will be fluent in British English. The quantification of the listeners’ hearing is done with: Left and right pure tone air-conduction audiograms. These measure the threshold at which people can hear a pure-tone sound.Results from the DTT (digit-triplet test, also known as a triple digit test)​ The audiogram is the standard clinical measurement of hearing ability. It’s the pure-tone threshold of hearing in each ear, measured in quiet in a sound booth. The procedure is standardized e.g., British Society of Audiology Recommended Procedure. Typically it’s measured at octave frequencies and important intermediate frequencies.The values of the audiogram defines how much gain the hearing aid needs to apply, with the calculation typically done by one of a group of &quot;prescription rules&quot;, e.g. CAMFIT, NAL-NL2 or DSL . Note that the scale of an audiogram is in “dB HL” = “dB Hearing Level”. This is not dB SPL; instead, it’s relative to an international standard such that 0-dB is “normal hearing” at every frequency. For background see Why the Audiogram Is Upside-down | The Hearing Review and The Quest for Audiometric Zero | The Hearing Review The DTT is an adaptive test of speech-in-noise ability. In each trial a listener hears three spoken digits (e.g. 3-6-1) against a background of noise at a given signal-to-noise-ratio (SNR). The task is to respond on a keypad with those three digits in the order they were presented. If the listener gets all three correct, then the SNR is reduced for the next trial so making it slightly harder. If the listener makes any mistake (i.e., any digit wrong, or the order wrong) then the SNR is increased, so making the next trial slightly easier. The test carries on trial-by-trial. The test asymptotes to the SNR at which the participant is equally likely to get all three correct or not, with a few tens of trials needed to get an acceptable result. DTT tests are now used world-wide to measure hearing as they are easy to make in any local language, to explain to participants and to do, and moreover can be done over the internet or telephone as they measure a relative threshold (signal-to-noise ratio), not an absolute threshold in dB SPL. Listeners are encouraged to set a volume that is comfortable and that does not distort or crackle, but is not too quiet. This paper is a recent scoping review of the field. The particular version we used is Vlaming et al.'s high-frequency DTT, which uses a high-pass noise as the masker. Ours starts at -14 dB SNR, goes up/down at 2 dB steps per trial, and continues for 40 trials. In the datafile, an average of the SNR for the last 30 trials is provided (labelled 'threshold'). For reference, the SNRs are supplied for each trial as well. The very first trial is practice and is not scored. "},{"title":"D Data file formats and naming conventions​","type":1,"pageTitle":"ICASSP 2023 Data","url":"docs/icassp2023/data/icassp2023_data#d-data-file-formats-and-naming-conventions","content":""},{"title":"D.1 Abbreviations used in filenames​","type":1,"pageTitle":"ICASSP 2023 Data","url":"docs/icassp2023/data/icassp2023_data#d1-abbreviations-used-in-filenames","content":"The following abbreviations are used consistently throughout the filenames and references in the metadata. R – “room”: e.g., “R02678” # Room ID linking to RAVEN rpf fileS – “scene”: e.g., S00121 # Scene ID for a particular setup in a room I.e., room + choice of target and interferer signalsBNC – BNC sentence identifier e.g. BNC_A06_01702CH – CH0 – eardrum signalCH1 – front signal, hearing aid channelCH2 – middle signal, hearing aid channelCH3 – rear signal, hearing aid channel I/i1 – Interferer, i.e., noise or sentence ID for the interferer/maskerT – talker who produced the target speech sentencesL – listenerE – entrant (identifying a team participating in the challenge)t – target (used in BRIRs and RAVEN project ‘rpf’ files) "},{"title":"D.2 General​","type":1,"pageTitle":"ICASSP 2023 Data","url":"docs/icassp2023/data/icassp2023_data#d2-general","content":"Audio and HOA-IRs will be 44.1 kHz 32-bit wav files in either mono or stereo as appropriate.Where stereo signals are provided the two channels represent the left (0) and right (1) signals of the ear or hearing aid microphones.0 dB FS in the audio signals corresponds to 100 dB SPL.Metadata will be stored in JSON or csv format as appropriate with the exception of Room descriptions are stored as RAVEN project ‘rpf’ configuration files and ‘ac’ files. (However, key details are reflected in the scene.json files) Signals are saved within the Python code as 32-bit floating point by default.Output signals for the listening tests will be required to be in 16-bit format. "},{"title":"D.3 Prompt and transcription data​","type":1,"pageTitle":"ICASSP 2023 Data","url":"docs/icassp2023/data/icassp2023_data#d3-prompt-and-transcription-data","content":"The following text is available for the target speech: Prompts are the text that was given to the talkers to say.‘Dot’ transcriptions contain the text as it was spoken in a form more suitable for scoring tools.These are stored in the master json metadata file. "},{"title":"D.4 Source audio files​","type":1,"pageTitle":"ICASSP 2023 Data","url":"docs/icassp2023/data/icassp2023_data#d4-source-audio-files","content":"Wav files containing the original source materials. Original target sentence recordings: &lt;Talker ID&gt;_&lt;BNC sentence identifier&gt;.wav "},{"title":"D.5 Preprocessed scene signals​","type":1,"pageTitle":"ICASSP 2023 Data","url":"docs/icassp2023/data/icassp2023_data#d5-preprocessed-scene-signals","content":"Audio files storing the signals picked up by the hearing aid microphone that are ready for processing. Separate signals are generated for each hearing aid microphone pair or ‘channel’. &lt;Scene ID&gt;_target_&lt;Channel ID&gt;.wav&lt;Scene ID&gt;_interferer_&lt;Channel ID&gt;.wav&lt;Scene ID&gt;_mixed_&lt;Channel ID&gt;.wav&lt;Scene ID&gt;_target_anechoic.wav - at hearing device front microphone&lt;Scene ID&gt;_hr.wav - head rotation signal Scene ID – S00001 to S10000 S followed by 5 digit integer with 0 pre-padding Channel ID CH0 – Eardrum signalCH1 – Hearing aid front microphoneCH2 – Hearing aid middle microphoneCH3 – Hearing aid rear microphone The anechoic signal is the signal that will be used as the referernce in the HASPI evaluation. The head rotation signal indicates the precise azimuthal angle of the head at each sample. It is stored as a floating point wav file with values between -1 and +1 where the range maps linearly from -180 degrees to +180 degrees. Teams are free to use this signal in their hearing aid algorithms, but if you do so we will ask you to also submit a version of your system that does not use it, so that the benefit of known head motion can be measured. "},{"title":"D.6 Enhanced signals​","type":1,"pageTitle":"ICASSP 2023 Data","url":"docs/icassp2023/data/icassp2023_data#d6-enhanced-signals","content":"The signals that are output by the baseline enhancement algorithm. &lt;Scene ID&gt;_&lt;Listener ID&gt;_enhanced.wav # Enhancement output signal (i.e., as submitted by the challenge entrants) Listener ID – ID of the listener panel member, e.g., L001 to L100 for initial ‘pseudo-listeners’, etc. "},{"title":"D.7 Hearing-aid output signals​","type":1,"pageTitle":"ICASSP 2023 Data","url":"docs/icassp2023/data/icassp2023_data#d7-hearing-aid-output-signals","content":"&lt;Scene ID&gt;_&lt;Listener ID&gt;_HA-output.wav # i.e., the enhanced signals after processing with the supplied hearing aid amplification. Listener ID – ID of the listener panel member, e.g., L001 to L100 for initial ‘pseudo-listeners’, etc. "},{"title":"D.8 Room metadata​","type":1,"pageTitle":"ICASSP 2023 Data","url":"docs/icassp2023/data/icassp2023_data#d8-room-metadata","content":"JSON file containing the description of a room. This is the data from which the ambisonic room impulse response are generated. It stores the fixed room, listener, target and interferer geometry but does not specify the dynamic factors (e.g. signals, SNRs, head movements etc) that are needed to fully define a scene. [ { &quot;name&quot;: &quot;R00001&quot;, # ID of room linking to RAVEN rpf and ac files &quot;dimensions&quot;: &quot;6.9933x3x3&quot; # Room dimensions in metres, &quot;target&quot;: { # target positions (x,y,z) and view vectors (look directions, x,y,z) &quot;position&quot;: [-0.3, 2.4, 1.2], &quot;view_vector&quot;: [0.071, 0.997, 0.0], }, &quot;listener&quot;: { &quot;position&quot;: [-0.1, 5.2, 1.2], &quot;view_vector&quot;: [0.071, 0.997, 0.0], }, &quot;interferers&quot;: [ { &quot;position&quot;: [0.4, 4.0, 1.2], }, { # etc, up to three interferers } ], }, ... ]  "},{"title":"D.9 Scene metadata​","type":1,"pageTitle":"ICASSP 2023 Data","url":"docs/icassp2023/data/icassp2023_data#d9-scene-metadata","content":"JSON file containing a description of the scene. It is a list of dictionaries with each entry representing a unique scene. A scene can be considered to be a room (see Section D.7) plus the full set of listener, target and interferer details. Note, many scenes can be generated from a single room, i.e. each using different listener, target and interferer settings. [ { &quot;scene&quot;: &quot;S00001&quot;, # the unique scene ID &quot;room&quot;:: &quot;R00001&quot;, # ID of room linking to rooms.json &quot;target&quot;: { &quot;name&quot;: &quot;T005_JYD_04274&quot;, # target speaker code and BNCid &quot;time_start&quot;: 107210, # start time of target in samples &quot;time_end&quot;: 217019 # end time of target in samples }, &quot;listener&quot;: { &quot;rotation&quot;: [ # Defines the head motion - list of time, direction pairs { &quot;sample&quot;: 88200, &quot;angle&quot;: 30 # Azimuth angle in degrees }, { &quot;sample&quot;: 176400, &quot;angle”: 50 } ], &quot;hrir_filename&quot;: [&quot;VP_N4-ED&quot;, &quot;VP_N4-BTE_fr&quot;, &quot;VP_N4-BTE_mid&quot;, &quot;VP_N4-BTE_rear&quot;] # HRIR filename for each channel to generate }, &quot;interferers&quot;: [ { &quot;position&quot;: 1, # Index of interferer position (See rooms.json) &quot;time_start&quot;: 0, # time of interferer onset in samples &quot;time_end&quot;: 261119, # time of interferer offset in samples &quot;name&quot;: &quot;track_1353255&quot;, # interferer name &quot;type&quot;: &quot;music&quot;, # interferer type: speech, noise or music &quot;offset&quot;: 4076256 # index into interferer file at which to extract sample }, { # etc, up to three interferers } ], &quot;dataset&quot;: &quot;train&quot;, # the dataset to which the scene belongs: train, dev or eval &quot;duration&quot;: 261119, # total duration of scene in samples &quot;SNR&quot;: 6.89 # targe SNR for the scene }, ... ]  There are JSON files containing the scene specifications per dataset, e.g., scenes.train.json.- Note, that the scene ID and room ID might have a one-to-one mapping in the challenge, but are not necessarily the same. Multiple scenes can be made by changing the target and masker choices for a given room. E.g., participants wanting to expand the training data could remix multiple scenes from the same room. The listener ID is not stored in the scene metadata; this information is stored separately in a scenes_listeners.json file which maps scenes to listeners, ie. telling you which listener (or listeners) will be listening to which scenes in the evaluation (see Section D.9). Noise interferers are labelled with a type “music”, “noise” or “speech” and then have a unique name identifying the file. For speech: &lt;ACCENT_CODE&gt;_&lt;SPEAKER_ID&gt; where ACCENT_CODE is a three letter code identify the accent region and gender of the speaker and SPEAKER_ID is a 5-digit ID specific to an individual speaker. E.g. &quot;mif_02484&quot; is a UK midlands accented female, speaker 02484. The speech comes from Demirshan et al. [1] which provides more details.For noise: CIN_&lt;NOISE_TYPE&gt;_&lt;NOISE_ID&gt; where NOISE_TYPE is one of dishwasher, fan, hairdryer, kettle, microwave, vacuum (vacuum cleaner) or washing (washing machine) and NOISE_ID is a unique 3-digit code for the sample.For music: track_&lt;TRACK_ID&gt; where TRACK_ID is unique 7-digit track identifier taken from the MTG Jamendo database. [2] Given the type and name, further interferer metadata can be found in the files masker_speech_list.json, masker_noise_list.json and masker_music_list.json which are distributed with the challenge. "},{"title":"D.10 Listener metadata​","type":1,"pageTitle":"ICASSP 2023 Data","url":"docs/icassp2023/data/icassp2023_data#d10-listener-metadata","content":"Audiogram data is stored in a single JSON file with the following format. { &quot;L0001&quot;: { &quot;name&quot;: &quot;L0001&quot;, &quot;audiogram_cfs&quot;: [250, 500, 1000, 2000, 3000, 4000, 6000, 8000], &quot;audiogram_levels_l&quot;: [10, 10, 20, 30, 40, 55, 55, 60], &quot;audiogram_levels_r&quot;: [ … ], }, &quot;L0002&quot;: { ... }, ... }  Additional metadata (e.g. digit triple test results) are stored in a csv file. DETAILS "},{"title":"D.11 Scene-Listener map​","type":1,"pageTitle":"ICASSP 2023 Data","url":"docs/icassp2023/data/icassp2023_data#d11-scene-listener-map","content":"JSON file named scenes_listeners.json dictates which scenes are to be processed by which listeners. { &quot;S00001&quot;: [&quot;L0001&quot;, &quot;L0002&quot;, &quot;L0003&quot;], &quot;S00002&quot;: [&quot;L0003&quot;, &quot;L0005&quot;, &quot;L0007&quot;], ... }  "},{"title":"References​","type":1,"pageTitle":"ICASSP 2023 Data","url":"docs/icassp2023/data/icassp2023_data#references","content":"Demirsahin, Isin and Kjartansson, Oddur and Gutkin, Alexander and Rivera, Clara, &quot;Open-source Multi-speaker Corpora of the English Accents in the British Isles&quot;, Proceedings of The 12th Language Resources and Evaluation Conference (LREC), 6532--6541, 2020, Avialable OnlineBogdanov, Dmitry and Won, Minz and Tovstogan, Philip and Porter, Alastair and Serra, Xavier, &quot;The MTG-Jamendo Dataset for Automatic Music Tagging&quot;, In Proc. Machine Learning for Music Discovery Workshop, International Conference on Machine Learning (ICML 2019), 2019, Long Beach, CA, United States&quot;, Available Online "}]