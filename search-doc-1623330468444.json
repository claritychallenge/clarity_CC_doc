[{"title":"CEC1 eval data released","type":0,"sectionRef":"#","url":"blog/CEC1 eval data released","content":"The evaluation dataset is now available to download from the myairbridge download site. The evaluation data filename is clarity_CEC1_data.scenes_eval.v1_1.tgz. Full details of how to prepare your submission are now available on this site. Please read them carefully. Registration: Teams must register via the Google form on the How To Submit page of this site. (Please complete this even if you have already completed a pre-registration form). Only one person from each team should register. Only those who have registered will be eligible to proceed to the evaluation. Once you have registered you will receive a confirmation email, a team ID and a link to a Google Drive to which you can upload your signals. Submission deadline: The deadline for submission is the 15th June. The submission consists of two components: i) a technical document of up to 2 pages describing the system/model and any external data and pre-existing tools, software and models used. This should be prepared as a Clarity-2021 workshop abstract and submitted to the workshop. ii) the set of processed signals that we will evaluate using the MBSTOI metric. Details of how to name and package your signals for upload can be found on the How To Submit page. Listening Tests: Teams that do well in the MBSTOI evaluation will be notified on 22nd June and invited to submit further signals for the second stage Listening Test evaluation. For any questions please contact us at claritychallengecontact@gmail.com or by posting to the Clarity challenge google group.","keywords":""},{"title":"Welcome","type":0,"sectionRef":"#","url":"blog/welcome","content":"Welcome to the new Clarity CEC1 site.","keywords":""},{"title":"Baseline System","type":0,"sectionRef":"#","url":"docs/cec1_baseline","content":"Figure 1 shows a simplified schematic of the baseline system. For simplicity, not all signal paths are shown. A scene generator (blue box) creates the speech in noise (SPIN) that the hearing aid model then enhances (yellow box). This enhancement is individualised for each listener, hence there is also a system to select a random listener (white ellipse) with a particular set of pure tone air-conduction thresholds or audiograms. The speech in noise that has been improved by the hearing aid is then passed to the prediction stage (orange box). This includes: (i) a simulation of hearing loss and (ii) a binaural model of intelligibility that estimates the speech intelligibility. Figure 1 Simplified overview of the baseline As stated in the rules of the first Enhancement Challenge, you are free to choose which parts of the baseline are useful to your approach, and reconfigure the system as you feel fit. More details of the different parts of the baseline appear on the software page, see, Scene GeneratorHearing aid modelHearing loss modelSpeech intelligibility model Download baseline software and data.","keywords":""},{"title":"Download","type":0,"sectionRef":"#","url":"docs/cec1_download","content":"The baseline code is available on Github. The github repository includes instructions for obtaining the data.","keywords":""},{"title":"Listening Tests","type":0,"sectionRef":"#","url":"docs/cec1_listening_tests","content":"","keywords":""},{"title":"Overview","type":1,"pageTitle":"Listening Tests","url":"docs/cec1_listening_tests#overview","content":"Our panel members will be provided with a tablet preloaded with our Listen@Home software and headphones to complete the listening experiment. They will listen to an entrant’s sentence, respond verbally with what they think was said by the target talker, and then move on to the next sentence. Their response will be recorded by the tablet microphone(s) and then processed using automatic speech recognition. Intelligibility will be evaluated as the number of words identified correctly in the sentence. Our plan is that each listener will evaluate 1,200 sentences, which is about 4 hours of listening, and that every listener will evaluate sentences from every entrant. We will use a combinatorial design to equate this as far as possible. Should a listener drop out from the panel, we will endeavour to replace them with someone with a similar hearing loss, but should that prove impractical we will reduce the size of the panel, and inform entrants which listener has withdrawn. "},{"title":"Listen@Home hardware","type":1,"pageTitle":"Listening Tests","url":"docs/cec1_listening_tests#listenhome-hardware","content":"We will be using a Lenovo 10e Chromebook running Android 81.0 and Sennheiser PC-8 headsets to play the sounds to our participants. We will allow participants to set the volume so that the sounds are not so loud to be uncomfortable. Without loudness-recruitment measures for our listeners, we cannot be sure just what loudnesses every participant will hear, so we need to allow them to make the choice here. We have measurements on the output capability of a system in the laboratory: A 1 kHz pure tone set to be the most powerful it can be (i.e., an amplitude range of +/-1 = RMS amplitude of 0.707, and the volume controls at 100%) gave 99 dB(A) SPL on the PC-8 headphones.An ICRA speech-shaped noise [1], unmodulated in time, and scaled to an RMS of 0.3, gave 90 dB(A) at the same volume level. With this RMS, the noise had 0.1% of its samples clipped at +/- 1. It is important to note that there is a convention for the prediction model that a +/-1 square wave has RMS = 0 dB FS and corresponds to 120 dB, while for listening tests, 0 dB FS corresponds to approximately 100 dB, given the above capabilities of the reproduction equipment. For the listening tests, we will require the signals to be provided as 16-bit WAV files with a 32 kHz sampling rate (see this page). We will play the signals as is using a HTML/PHP audio player coded on a webpage. The responsibility for the final signal level is therefore yours. It’s worth bearing in mind that should your signals overall seem too loud to be comfortable to a participant, they may well turn down the volume themselves.  "},{"title":"References","type":1,"pageTitle":"Listening Tests","url":"docs/cec1_listening_tests#references","content":"[1] ICRA standard noises, https://icra-audiology.org/Repository/icra-noise. We used track #1. "},{"title":"Prizes","type":0,"sectionRef":"#","url":"docs/cec1_prizes","content":"","keywords":""},{"title":"The Team Prize","type":1,"pageTitle":"Prizes","url":"docs/cec1_prizes#the-team-prize","content":"Team prizes have been made available by the generosity of the Hearing Industry Research ConsortiumThere will be separate MBSTOI and listening test prizes for the top systems.  MBSTOI prize emoji_events 1st Place $1000 emoji_events 2nd Place $500 emoji_events 3rd Place $250 Listening Test prize emoji_events 1st Place $1000 emoji_events 2nd Place $500 emoji_events 3rd Place $250 "},{"title":"The Amazon Student Prize","type":1,"pageTitle":"Prizes","url":"docs/cec1_prizes#the-amazon-student-prize","content":"Student prizes have been made available by the generosity of Amazon TTS Research Amazon prize for top student contribution emoji_events 1st Place $1000 emoji_events 2nd Place $500 emoji_events 3rd Place $250 The award will be judged by a panel formed from members of the Clarity-2021 workshop scientific committee. There will be a lightweight nomination process. Details to be announced. info Anonymous entries and those with direct links to the Clarity project team are ineligible for cash prizes, sorry. "},{"title":"Rules","type":0,"sectionRef":"#","url":"docs/cec1_rules","content":"","keywords":""},{"title":"Teams","type":1,"pageTitle":"Rules","url":"docs/cec1_rules#teams","content":"Teams must have pre-registered and nominated a contact person.Teams can be from one or more institutions.The organisers may enter the challenge themselves but will not be eligible to win the cash prizes. "},{"title":"Transparency","type":1,"pageTitle":"Rules","url":"docs/cec1_rules#transparency","content":"Teams must provide a technical document of up to 2 pages describing the system/model and any external data and pre-existing tools, software and models used.We will publish all technical documents (anonymous or otherwise).Teams are encouraged – but not required – to provide us with access to the system/model and to make their code open source.Anonymous entries are allowed but will not be eligible for cash prizes.All teams will be referred to using anonymous codenames in rank ordering. "},{"title":"Intellectual property","type":1,"pageTitle":"Rules","url":"docs/cec1_rules#intellectual-property","content":"The following terms apply to participation in this machine learning challenge (“Challenge”). Entrants may create original solutions, prototypes, datasets, scripts, or other content, materials, discoveries or inventions (a “Submission”). The Challenge is organised by the Challenge Organiser. Entrants retain ownership of all intellectual and industrial property rights (including moral rights) in and to Submissions. As a condition of submission, Entrant grants the Challenge Organiser, its subsidiaries, agents and partner companies, a perpetual, irrevocable, worldwide, royalty-free, and non-exclusive license to use, reproduce, adapt, modify, publish, distribute, publicly perform, create a derivative work from, and publicly display the Submission. Entrants provide Submissions on an “AS IS” BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. "},{"title":"What information can I use?","type":1,"pageTitle":"Rules","url":"docs/cec1_rules#what-information-can-i-use","content":""},{"title":"Training and development","type":1,"pageTitle":"Rules","url":"docs/cec1_rules#training-and-development","content":"There is no limit on the amount of training data that can be generated using our tools. Teams can also use their own data for training or expand the training data through simple automated modifications. However, teams that do this must make a second submission using only the official audio files and signal generation tool. Any audio or metadata can be used during training and development, but during evaluation the proposed simulated hearing aid or Enhancement Processor will not have access to all of the data (see next section). "},{"title":"Evaluation","type":1,"pageTitle":"Rules","url":"docs/cec1_rules#evaluation","content":"The only data that can be used by the Enhancement Processor during evaluation are The audio input signals (the sum of the target and interferer for each hearing aid microphone), andThe listener characterisation (pure tone air-conduction audiograms). "},{"title":"Computational restrictions","type":1,"pageTitle":"Rules","url":"docs/cec1_rules#computational-restrictions","content":"Teams may choose to use all or some of the provided baseline models.Systems must be causal; the output at time t must not use any information from input samples more than 5 ms into the future (i.e., no information from input samples >t+5 ms).There is no limit on computational cost. Please see this blog post for further explanation of these last two rules about latency and computation time. "},{"title":"Submitting multiple entries","type":1,"pageTitle":"Rules","url":"docs/cec1_rules#submitting-multiple-entries","content":"If you wish to submit two entries, where one is optimised for MBSTOI and the other, for listening tests, Both systems must be submitted for MBSTOI evaluation.You must register two teams, submitting each entry as a different team.In your documentation, you must make it clear which has been optimised for listening tests and the relationship between the two entries. We will assume that if only one of these systems is to go forward to listening tests, your preference is to use the one optimised for listening tests. "},{"title":"Evaluation of systems","type":1,"pageTitle":"Rules","url":"docs/cec1_rules#evaluation-of-systems","content":""},{"title":"Stage 1: Objective evaluation","type":1,"pageTitle":"Rules","url":"docs/cec1_rules#stage-1-objective-evaluation","content":"Entries will be ranked according to average Modified Binaural Short-Time Objective Intelligibility (MBSTOI) score across all samples in the evaluation/test dataset (i.e., all signals submitted for the MBSTOI evaluation). "},{"title":"Stage 2: Listening test evaluation","type":1,"pageTitle":"Rules","url":"docs/cec1_rules#stage-2-listening-test-evaluation","content":"There is a limit on how many systems can be evaluated by the listener panel.A maximum of two entries can go through to the listener panel from any individual entrant. Furthermore, a second will only be allowed if it is judged by us to use significantly different signal processing approaches.We will choose which will go to the listener panel based on The top N scored using the objective evaluation method.A sample of M others that use contrasting and promising approaches. "},{"title":"CEC1 Data","type":0,"sectionRef":"#","url":"docs/cec1_data","content":"","keywords":""},{"title":"A. Training, development, evaluation data","type":1,"pageTitle":"CEC1 Data","url":"docs/cec1_data#a-training-development-evaluation-data","content":"The dataset is split into these three subsets: training (train), development (dev) and evaluation (eval). You should only train on the training set.The system submitted should be chosen on the evidence provided by the development set.The final listening and ranking will be performed with the (held-out) evaluation set.For more information on supplementing the training data, please see the rules. The evaluation dataset will be made available one month before the challenge submission deadline. "},{"title":"B. The scene dataset","type":1,"pageTitle":"CEC1 Data","url":"docs/cec1_data#b-the-scene-dataset","content":"The complete dataset is composed of 10,000 scenes split into the following sets: Training (6000 scenes, 24 speakers);Development (2500 scenes, 10 speakers);Evaluation (1500 scenes, 6 speakers). Each scene corresponds to a unique target utterance and a unique segment of noise from an interferer. The training, development and evaluation sets are disjoint for target speaker. The three sets are balanced for target speaker gender. Binaural Room Impulse Responses (BRIRs) are used to model how the sound is altered as it propagates through the room and interacts with the head. The audio signals for the scenes are generated by convolving source signals with the BRIRs and summing. See the page on modelling the scenario for more details. Randomised room dimensions, target and interferer locations are used. The BRIRs are generated for: A hearing aid with 3 microphone inputs (front, mid, rear). The hearing aid has a Behind-The-Ear (BTE) form factor; see Figure 1. The distance between microphones is approx. 7.6 mm. The properties of the tube and ear mould are not considered.Close to the eardrum.The anechoic target reference (front microphone).  Figure 1. Front (Fr), Middle (Mid) and Rear microphones on a BTE hearing aid form. Head Related Impulse Responses (HRIRs) are used to model how sound is altered as it propagates in a free-field and interacts with the head (i.e., no room is included). These are taken from the OlHeadHRTF database with permission. These include HRIRs for human heads and for three types of head-and-torso simulator/mannekin. The eardrum HRIRs (labelled ED) are for a position close to the eardrum of the open ear. rpf files are specification files for the geometric room acoustic model that include a complete description of the room. "},{"title":"B.1 Training data","type":1,"pageTitle":"CEC1 Data","url":"docs/cec1_data#b1-training-data","content":"For each scene in the training data the following signals and metadata are available: The target and interferer BRIRs (4 pairs: front, mid, rear and eardrum for left and right ears).HRIRs including those corresponding to the target azimuth.The mono target and interferer signals (pre-convolution).For each hearing aid microphone (channels 1-3 where channel 1 is front, channel 2 is mid and channel 3 is rear) and a position close to the eardrum (channel 0): The target convolved with the appropriate BRIR;The interferer convolved with the appropriate BRIR;The sum of the target and interferer convolved. The target convolved with the anechoic BRIR (channel 1) for each ear (‘target_anechoic’).Metadata describing the scene: a JSON file containing, e.g., the filenames of the sources, the location of the sources, the viewvector of the target source, the location and viewvector of the receiver, the room dimensions (see specification below), and the room number, which corresponds to the RAVEN BRIR, rpf and ac files. Software for generating more training data is also available. "},{"title":"B.2 Development data","type":1,"pageTitle":"CEC1 Data","url":"docs/cec1_data#b2-development-data","content":"The same data as for the training will be made available to allow you to fully examine the performance of your system. Note, that the data available for the evaluation will be much more limited (see B.3). For each scene, during development, your hearing aid enhancement model must only use the following input signals/data: The sum of the target and interferer – mixed at the SNR specified in the scene metadata – at one or more hearing aid microphones (CH1, CH2 and/or CH3).The IDs of the listeners assigned to the scene in the metadata provided.The audiograms of these listeners. "},{"title":"B.3 Evaluation scene data","type":1,"pageTitle":"CEC1 Data","url":"docs/cec1_data#b3-evaluation-scene-data","content":"For each scene in the evaluation data only the following will be available: The sum of the target and interferer for each hearing aid microphone.The ID of the evaluation panel members/listeners who will be listening to the processed scene.The audiograms of these listeners. "},{"title":"C Listener data","type":1,"pageTitle":"CEC1 Data","url":"docs/cec1_data#c-listener-data","content":""},{"title":"C.1 Training and development data","type":1,"pageTitle":"CEC1 Data","url":"docs/cec1_data#c1-training-and-development-data","content":"A sample of pure tone air-conduction audiograms that characterise the hearing impairment of potential listeners, split into training and development sets. "},{"title":"C.2 Evaluation data","type":1,"pageTitle":"CEC1 Data","url":"docs/cec1_data#c2-evaluation-data","content":"You will be given the left and right pure tone air-conduction audiograms for the listening panel, so the signals you generate for evaluation can be individualised to the listeners. A panel of 50 hearing-aided listeners will be recruited for the evaluation panel. We plan that they will be experienced bilateral hearing-aid users (they use two hearing aids but the hearing loss may be asymmetrical) with an averaged hearing loss as measured by pure tone air-conduction of between 25 and about 60 dB in the better ear, with fluent speaking of (and listening to) British English. "},{"title":"D Data file formats and naming conventions","type":1,"pageTitle":"CEC1 Data","url":"docs/cec1_data#d-data-file-formats-and-naming-conventions","content":""},{"title":"D.1 Abbreviations in Filenames","type":1,"pageTitle":"CEC1 Data","url":"docs/cec1_data#d1-abbreviations-in-filenames","content":"R – “room”: e.g., “R02678” # Room ID linking to RAVEN rpf fileS – “scene”: e.g., S00121 # Scene ID for a particular setup in a room I.e., room + choice of target and interferer signalsBNC – BNC sentence identifier e.g. BNC_A06_01702CH – CH0 – eardrum signalCH1 – front signal, hearing aid channelCH2 – middle signal, hearing aid channelCH3 – rear signal, hearing aid channel I/i1 – Interferer, i.e., noise or sentence ID for the interferer/maskerT – talker who produced the target speech sentencesL – listenerE – entrant (identifying a team participating in the challenge)t – target (used in BRIRs and RAVEN project ‘rpf’ files) "},{"title":"D.2 General","type":1,"pageTitle":"CEC1 Data","url":"docs/cec1_data#d2-general","content":"Audio and BRIRs will be 44.1 kHz 32 bit wav files in either mono or stereo as appropriate.Where stereo signals are provided the two channels represent the left and right signals of the ear or hearing aid microphones. HRIRs have a sampling rate of 48 kHz.Metadata will be stored in JSON format wherever possible.Room descriptions are stored as RAVEN project ‘rpf’ configuration files.Signals are saved within the Python code as 32-bit floating point by default. "},{"title":"D.3 Prompt and transcription data","type":1,"pageTitle":"CEC1 Data","url":"docs/cec1_data#d3-prompt-and-transcription-data","content":"The following text is available for the target speech: Prompts are the text that was supposed to be spoken as presented to the readers.‘Dot’ transcriptions contain the text as it was spoken in a form more suitable for scoring tools.These are stored in the master json metadata file. "},{"title":"D.4 Source audio files","type":1,"pageTitle":"CEC1 Data","url":"docs/cec1_data#d4-source-audio-files","content":"Wav files containing the original source materials.Original target sentence recordings:  <Talker ID>_<BNC sentence identifier>.wav Copy "},{"title":"D.5 Preprocessed scene signals","type":1,"pageTitle":"CEC1 Data","url":"docs/cec1_data#d5-preprocessed-scene-signals","content":"Audio files storing the signals picked up by the hearing aid microphone ready for processing. Separate signals are generated for each hearing aid microphone pair or ‘channel’. <Scene ID>_target_<Channel ID>.wav <Scene ID>_interferer_<Channel ID>.wav <Scene ID>_mixed_<Channel ID>.wav <Scene ID>_target_anechoic.wav Copy Scene ID – S00001 to S10000 S followed by 5 digit integer with 0 pre-padding Channel ID CH0 – Eardrum signalCH1 – Hearing aid front microphoneCH2 – Hearing aid middle microphoneCH3 – Hearing aid rear microphone "},{"title":"D.6 Enhanced signals","type":1,"pageTitle":"CEC1 Data","url":"docs/cec1_data#d6-enhanced-signals","content":"The signals that are output by the enhancement (hearing aid) model. <Scene ID>_<Listener ID>_HA-output.wav #HA output signal (i.e., as submitted by the challenge entrants) Listener ID – ID of the listener panel member, e.g., L001 to L100 for initial ‘pseudo-listeners’, etc. We are no longer providing the script for post-processing signals in preparation for the listener panel. "},{"title":"D.7 Enhanced signals processed by the hearing loss model","type":1,"pageTitle":"CEC1 Data","url":"docs/cec1_data#d7-enhanced-signals-processed-by-the-hearing-loss-model","content":"The signals that are produced by the hearing loss (HL) model. <Scene ID>_<Listener ID>_HL-output.wav HL output signal<Scene ID>_<Listener ID>_HL-mixoutput.wav HL-processed CH0 signal, bypassing HA processing, for comparison<Scene ID>_<Listener ID>_flat0dB_HL-output HL-output for flat 0 dB audiogram processed signal for comparison<Scene ID>_<Listener ID>_HLddf-output unit impulse signal output by HL model for time-alignment of signals before processing by the baseline speech intelligibility model "},{"title":"D.8 Scene metadata","type":1,"pageTitle":"CEC1 Data","url":"docs/cec1_data#d8-scene-metadata","content":"JSON file containing a description of the scene and assigns the scene to a specific member of the listening panel. It is a hierarchical dictionary, with the top level being scenes indexed by unique scene ID, and each scene described by a second-level dictionary. Here, viewvector indicates the direction vector or line of sight. [ { scene\": \"S00001\", \"room\": { \"name\": \"R00001\", \"dimensions\": \"5.9x3.4186x2.9\" # Room dimensions in metres }, \"SNR\": 3.8356, \"hrirfilename\": \"VP_N5-ED\", # HRIR filename \"target\": { # target positions (x,y,z) and view vectors (look directions, x,y,z) \"Positions\": [ -0.5, 3.4, 1.2 ], \"ViewVectors\": [ 0.291, -0.957, 0 ], \"name\": \"T022_HCS_00002\", # target speaker code and BNCid \"nsamples\": 153468, # length of target speech in samples }, \"listener\": { \"Positions\": [ 0.2, 1.1, 1.2 ], \"ViewVectors\": [ -0.414, 0.91, 0 ] }, \"interferer\": { \"Positions\": [ 0.4, 3.2, 1.2 ], \"name\": \"CIN_dishwasher_012\", # interferer name \"nsamples\": 1190700, # interferer length in samples \"duration\": 27, # interferer duration in seconds \"type\": \"noise\", # interferer type: noise or speech \"offset\": 182115, # interferer segment starts at n samples from beginning of recording }, \"azimuth_target_listener\": -7.55, # angle azimuth in degrees of target for receiver \"azimuth_interferer_listener\": -29.92, # angle azimuth in degrees of interferer for receiver \"dataset\": \"train\", # dataset: train, dev or eval/test \"pre_samples\": 88200, # number of samples of interferer before target onset \"post_samples\": 44100 # number of samples of interferer after target offset }, { etc. } ] Copy There are JSON files containing the scene specifications per dataset, e.g., scenes.train.json.- Note, that the scene ID and room ID might have a one-to-one mapping in the challenge, but are not necessarily the same. Multiple scenes can be made by changing the target and masker choices for a given room. E.g., participants wanting to expand the training data could remix multiple scenes from the same room.A scene is completely described by the room ID and target and interferer source IDs, as all other information, e.g., source + target geometry are already in the RAVEN project rpf files. Only the room ID is needed to identify the BRIR files.The listener ID is not stored in the scene metadata; this information is stored separately in a scenes_listeners.json file.Non-speech interferers are labelled CIN_<noise type>_XXX, while speech interferers are labelled <three letter code including dialect and talker gender>_XXXXX . "},{"title":"D.9 Listener metadata","type":1,"pageTitle":"CEC1 Data","url":"docs/cec1_data#d9-listener-metadata","content":"Listener data stored in a single JSON file with the following format. {“L0001”: { “name”: “L0001”, \"audiogram_cfs\": [250, 500, 1000, 2000, 3000, 4000, 6000, 8000], “audiogram_levels_l”: [10, 10, 20, 30, 40, 55, 55, 60], “audiogram_levels_r”: [ … ], }, “L0002”: { }, ... } Copy "},{"title":"D.10 Scene-Listener map","type":1,"pageTitle":"CEC1 Data","url":"docs/cec1_data#d10-scene-listener-map","content":"JSON file named scenes_listeners.json dictates which scenes are to be processed by which listeners. {“S00001”: [“L0001”, “L0002”, “L0003”], “S00002”: [“L0003”. “L0005”, “L0007”], etc } Copy "},{"title":"Software","type":0,"sectionRef":"#","url":"docs/cec1_software","content":"","keywords":""},{"title":"A. Scene generator","type":1,"pageTitle":"Software","url":"docs/cec1_software#a-scene-generator","content":"Fully open-source python code for generating hearing aid inputs for each scene Inputs: target and interferer signals, BRIRs, RAVEN project (rpf) files, scene description JSON filesOutputs: Mixed target+interferer signals for each hearing aid channel, direct path (simulating a measurement close to the eardrum). Reverberated pre-mixed signals can also be optionally generated. "},{"title":"B. Baseline hearing aid processor","type":1,"pageTitle":"Software","url":"docs/cec1_software#b-baseline-hearing-aid-processor","content":"The baseline hearing aid processor is based on openMHA. The python code configures openMHA with a Camfit compressive fitting for a specific listener’s audiogram. This includes a python implementation of the Camfit compressive prescription and python code for driving openMHA. This configuration of openMHA includes multiband dynamic compression and non-adaptive differential processing. The intention was to produce a basic hearing aid without various aspects of signal processing that are common in high-end hearing aids, but tend to be implemented in proprietary forms so cannot be replicated exactly. The main inputs and outputs for the processor are as follows: Inputs: Mixed scene signals for each hearing aid channel, a listener ID drawn from scene-listener pairs identified in ‘scenes_listeners.json’ and an entry in the listener metadata json file ‘listeners.json’ for that IDOutputs: The stereo hearing aid output signal, <scene>_<listener>_HA-output.wav "},{"title":"C. Hearing Loss model","type":1,"pageTitle":"Software","url":"docs/cec1_software#c-hearing-loss-model","content":"Open-source python implementation of the Cambridge Auditory Group Moore/Stone/Baer/Glasberg hearing loss model. Inputs: A stereo wav audio signal, e.g., the output of the baseline hearing aid processor, and a set of audiograms (both L and R ears).Outputs: The signal after simulating the hearing loss as specified by the set of audiograms (stereo wav file), <scene>_<listener>_HL-output.wav "},{"title":"D. Speech Intelligibility model","type":1,"pageTitle":"Software","url":"docs/cec1_software#d-speech-intelligibility-model","content":"Python implementation of a binaural intelligibility model, Modified Binaural Short-Time Objective Intelligibility (MBSTOI). This is an experimental baseline tool that will be used in the stage 1 evaluation of entrants (see Rules). Note that MBSTOI requires signal time-alignment (and alignment within one-third octave bands). Inputs: HL-model output signals, audiogram, reference target signal (i.e., the premixed target signal convolved with the BRIR with the reflections “turned off”, specified as ‘target_anechoic’), (scene metadata)Outputs: predicted intelligibility score "},{"title":"Submission","type":0,"sectionRef":"#","url":"docs/cec1_submission","content":"","keywords":""},{"title":"Registration","type":1,"pageTitle":"Submission","url":"docs/cec1_submission#registration","content":"Teams are required to register using the form below. Please submit one form per team, i.e., providing a single contact email address. Once you have registered, you will receive an email confirmation with a team ID and an individualised link to a Google Drive for submitting materials. Loading… info It is important that all teams who are intending to submit an entry complete the registration form no later than 11th June. "},{"title":"What evaluation data is provided?","type":1,"pageTitle":"Submission","url":"docs/cec1_submission#what-evaluation-data-is-provided","content":"The evaluation data consists of 1500 scenes. For each scene you are provided with the signals received at each of the three microphones on the left and right hearing aid device. You will also be provided with JSON formatted metadata consisting of the audiograms of a set of listeners and a mapping of which listeners will listen to which scenes.  For the MBSTOI evaluation, there will be one listener per scene and the scene-listener mapping will be the same for all teams. For the listening test evaluation, there will be two listeners per scene and each team will have a separate scene-listener mapping. The file formats will be the same as used for the development data; for details see the CEC1 Data page. "},{"title":"What audio do I need to submit?","type":1,"pageTitle":"Submission","url":"docs/cec1_submission#what-audio-do-i-need-to-submit","content":"You must submit the audio signals produced at the output of your simulated hearing aid for the evaluation datasets. You will be asked to provide two sets of signals: the first for the MBSTOI evaluation (due 15th June) and the second for the listening tests (due 29th June). MBSTOI evaluation. Signals should be submitted in floating point WAV format with a 44.1 kHz sampling rate. For levels, we will follow the convention in the baseline hearing aid (at the output) and hearing loss models. That is, a +/-1 square wave has RMS = 0 dB FS and corresponds to 120 dB. Listening tests. Signals should be submitted as 16-bit WAV files with a 32 kHz sampling rate (due to hardware limitations). You should ensure that any samples that are >+1 or <-1 have been hard-clipped at +/-1 before submission. Here, 0 dB FS corresponds to approximately 100 dB, given the capabilities of the reproduction equipment. These signals will be played as is to the listener panel. We also encourage you to submit your simulated hearing aid code. See the page on listening tests for more information about the levels that can be reproduced by the listening test equipment. When playing signals to listeners we will then play them as is. The responsibility for the final signal level is therefore yours. It’s worth bearing in mind that should your signals overall seem too loud to be comfortable to a participant, they may well turn down the volume themselves. "},{"title":"Naming and packaging signals","type":1,"pageTitle":"Submission","url":"docs/cec1_submission#naming-and-packaging-signals","content":"Your processed signals should be named using the conventions used by the baseline system, i.e., <Scene ID>_<Listener ID>_HA-output.wav and explained on the CEC1 data page. These should be placed in a directory whose name is the unique team ID that you will be sent, e.g., E001 and then packaged using zip or tar or any standard packaging tool. The resulting file should be about 2 GB for the first round. "},{"title":"Two page technical report","type":1,"pageTitle":"Submission","url":"docs/cec1_submission#two-page-technical-report","content":"The report must be submitted as a paper to the Clarity-2021 Workshop. Deadline 22nd June. An author kit and submission instructions are available at the workshop website. A draft of the report needs to be uploaded to the Google Drive along with your MBSTOI signals by 15th June. The draft needs to be sufficiently complete for us to judge whether your system is compliant with the challenge rules. Your report should include an abstract and introduction and sections on experimental setup/methodology including system information and model/network architecture, evaluation/results, discussion, conclusion and references. Please provide an estimation of the computational resources needed. You must describe any external data and pre-existing tools, software and models used. Your report should cite the following document, which provides an overview of the challenge and the baseline system: S. Graetzer, J. Barker, T. J. Cox, M. Akeroyd, J. F. Culling, G. Naylor, E. Porter, and R. Viveros Muñoz, “Clarity-2021 challenges: Machine learning challenges for advancing hearing aid processing,” in Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH 2021, Brno, Czech Republic, 2021. The document can be accessed here. "},{"title":"How will intellectual property be handled?","type":1,"pageTitle":"Submission","url":"docs/cec1_submission#how-will-intellectual-property-be-handled","content":"See here under Intellectual Property. "},{"title":"Where do I submit the signals?","type":1,"pageTitle":"Submission","url":"docs/cec1_submission#where-do-i-submit-the-signals","content":"When you have registered you will receive a link to a Google Drive to which you will be able to securely upload your signals. You will be able to use the same link to upload materials for both the 1st submission, and the 2nd submission if you are selected for the 2nd round. We also encourage you to submit your simulated hearing aid code via this link. Materials uploaded will be visible to the Clarity Team but not to other entrants. warning Note, in order to use the Google Drive you will need to have a Google account. If you anticipate problems using Google then please make arrangements to send us the materials by other means, e.g., via a service such as WeTransfer or similar. "},{"title":"Overview","type":0,"sectionRef":"#","url":"docs/intro","content":"Scenario - a description of the listening scenario and how it has been simulated. Baseline System - a description of the baseline hearing aid model. CEC1 Data - the data that can be used to train and evaluate your system during development. CEC1 Software - the software tools that we are providing to help you build and evaluate a challenge entry. Challenge Rules - the rules to which all challenge entries must adhere. Listening Tests - information about the listening tests which will be used to evaluate the best systems. Submission - information about how to prepare your submission. Prizes - information about our prizes. Download - where to go to download the software and challenge data.","keywords":""},{"title":"Modelling the scenario","type":0,"sectionRef":"#","url":"docs/cec1_scenario","content":"","keywords":""},{"title":"Simulating the audio signals received by the hearing aid","type":1,"pageTitle":"Modelling the scenario","url":"docs/cec1_scenario#simulating-the-audio-signals-received-by-the-hearing-aid","content":"A listener – or receiver – is sitting or standing in a small room that has low to moderate reverberation. The person is listening to a target talker, who is selected from our set of 40 speakers. The target talker is producing one of our unique 7-10 word Clarity utterances. Simultaneously, an interferer sound is playing; this is either a competing talker or a continuous noise source (e.g., a washing machine). The target and interferer are at the same height as the listener. The room dimensions, boundary materials, and the locations of the listener, target and interferer are randomised (discussed below). An example scenario is shown in Figure 1. The room geometry showing origin location is defined in Figure 2. Example OverviewGeometry Definition Figure 1. Example overview Figure 3, below, shows the basic scene generator. The sound at the receiver is generated first by convolving the source signals with Binaural Room Impulse Responses (BRIRs). The reverberated speech and noise signals are then summed after appropriate gains are applied. The gains are set to achieve a Signal-to-Noise Ratio (SNR), which is chosen pseudo-randomly between limits. The BRIRs are generated using the RAVEN Geometric Room Acoustic Model [1]. There are additional signal paths and outputs generated that have been omitted from Figure 3 for clarity. In addition to the reverberated signals associated with the hearing aid microphones, the signal close to the eardrum is also generated. You can also access the reverberated speech and noise signals before they are mixed.  Figure 3. Simplified diagram of the scene generator. RIR refers to Room Impulse Response, HRTFs refers to Head Related Transfer Functions, SNRs are signal-to-noise ratios, and gain calc. indicates gain calculation. Dry here means anechoic. The outputs are noisy speech signals. "},{"title":"Room Geometry","type":1,"pageTitle":"Modelling the scenario","url":"docs/cec1_scenario#room-geometry","content":"Cuboid rooms with dimensions length, LLL, by width, WWW, by height, HHH.Length LLL set using a uniform probability distribution random number generator with 3≤L(m)≤83 \\le L (m) \\le 83≤L(m)≤8.Height HHH set using a Gaussian distribution random number generator with a mean of 2.7m2.7 m2.7m and standard deviation of 0.8m0.8 m0.8m.Area L×WL \\times WL×W set using a Gaussian distribution random number generator with mean 17.7m217.7 m^217.7m2 and standard deviation of 5.5m25.5 m^25.5m2. "},{"title":"Room Materials","type":1,"pageTitle":"Modelling the scenario","url":"docs/cec1_scenario#room-materials","content":"One of the walls of the room is randomly selected for the location of the door. The door can be at any position with the constraint of being at least at 20 cm from the corner of the wall. A window is placed on one of the other three walls. The window could be at any position of the wall but at 1.9 m height and at 0.4 m from any corner. The curtains are simulated to the side of the window. For larger rooms, a second window and curtains are simulated following a similar methodology. A sofa is simulated at a random position as a layer on the wall and the floor. Finally, a rug is simulated at a random location on the floor. "},{"title":"The receiver","type":1,"pageTitle":"Modelling the scenario","url":"docs/cec1_scenario#the-receiver","content":"The receiver has position, r⃗=(xr,yr,zr)\\vec{r} = (x_r,y_r,z_r)r=(xr​,yr​,zr​) This is positioned within the room using uniform probability distribution random number generators for the x and y coordinates (see Figure 2 for origin location). There are constraints to ensure that the receiver is not too close to the wall: −W/2+1≤xr≤W/2−1-W/2+1 \\le x_r \\le W/2-1−W/2+1≤xr​≤W/2−11≤yr≤L−11 \\le y_r \\le L-11≤yr​≤L−1zrz_rzr​ either 1.2m1.2 m1.2m (sitting) or 1.6m1.6 m1.6m (standing). The receiver is positioned so as to be roughly facing the target talker. That is to say, within ±30\\pm 30±30 degrees of target. The angle = 7.5n7.5n7.5n where nnn is an integer and ∣n∣≤4|n| \\le 4∣n∣≤4. "},{"title":"The target talker","type":1,"pageTitle":"Modelling the scenario","url":"docs/cec1_scenario#the-target-talker","content":"The target talker has position t⃗=(xt,yt,zt)\\vec{t} = (x_t,y_t,z_t)t=(xt​,yt​,zt​) The target talker is positioned within the room using uniform probability distribution random number generators for the coordinates. Constraints ensure the target is not too close to the wall or receiver. It is set to have the same height as the receiver. −W/2+1≤xt≤W/2−1-W/2+1 \\le x_t \\le W/2-1−W/2+1≤xt​≤W/2−11≤yt≤L−11 \\le y_t \\le L-11≤yt​≤L−1∣r−t∣>1|r-t| > 1∣r−t∣>1zt=zrz_t=z_rzt​=zr​ A speech directivity pattern is used, which is directed at the listener. "},{"title":"The interferer","type":1,"pageTitle":"Modelling the scenario","url":"docs/cec1_scenario#the-interferer","content":"The interferer has position i⃗=(xi,yi,zi)\\vec{i} = (x_i,y_i,z_i)i=(xi​,yi​,zi​) The interferer is a single point source radiating speech or non-speech noise omnidirectionally. It is placed within the room using uniform probability distribution random number generators for the coordinates. These constraints ensure the interferer is not too close to the wall or receiver. It is set to be at the same height as the receiver. Note, this means that the interferer can be at any angle relative to the receiver. −W/2+1≤xi≤W/2−1-W/2+1 \\le x_i \\le W/2-1−W/2+1≤xi​≤W/2−11≤yi≤L−11 \\le y_i \\le L-11≤yi​≤L−1∣r−i∣>1|r-i| \\gt 1∣r−i∣>1zi=zrz_i = z_rzi​=zr​ "},{"title":"Timing","type":1,"pageTitle":"Modelling the scenario","url":"docs/cec1_scenario#timing","content":"The target sound starts 2 seconds after the start of the interferer. This is so the target is clear and unambiguously identifiable for listening tests. This also gives the hearing aid algorithms some time to adjust to the background noise.The interferer continues 1 second after the target has finished, so that all words in the target utterance can be masked. "},{"title":"Signal-to-Noise Ratio (SNR)","type":1,"pageTitle":"Modelling the scenario","url":"docs/cec1_scenario#signal-to-noise-ratio-snr","content":"The mixtures are engineered such that the target utterances are at an appropriate level of intelligibility when processed by the default hearing aid software. This is achieved by scaling the interferer. Pilot tests have been conducted to get this approximately correct. Scaling is done this way because it does not require recomputing the BRIRs. Note that the interferer can be at any azimuth from the point of view of the listener/receiver. A desired signal-to-noise ratio, SNRD (dB), is chosen using a uniform probability distribution random number generator between the limits of ranges specified for the speech and non-speech interferers. The better ear SNR, here termed BE_SNR, which models the better ear effect in binaural listening, is calculated for the reference channel (channel 1, which corresponds to the front microphone of the hearing aid). This value is used to scale all interferer channels. The procedure is described below. For the reference channel, The segment of the interferer that overlaps with the target (without padding) , i‘, and the target (without padding), t‘, are extractedSpeech-weighted SNRs are calculated for each ear, SNRL and SNRR: Signals i‘ and t’ are separately convolved with a speech-weighting filter, h (specified below).The rms is calculated for each convolved signal.SNRL and SNRR are calculated as the ratio of these rms values. The BE_SNR is selected as the maximum of the two SNRs: BE_SNR = max(SNRL and SNRR). Then per channel, The whole interferer signal, i, is scaled by the BE_SNR i=i∗BESNRi = i*BE_{SNR}i=i∗BESNR​ Finally, i is scaled as follows: i=i∗10((−SNRD)/20)i = i*10^{((-SNR_D)/20)}i=i∗10((−SNRD​)/20) The speech-weighting filter is an FIR designed using the host window method [2, 3]. The specification is: Frequency (Hz) = [0, 150, 250, 350, 450, 4000, 4800, 5800, 7000, 8500, 9500, 22050];Magnitude of transfer function at each frequency = [0.0001, 0.0103, 0.0261, 0.0419, 0.0577, 0.0577, 0.046, 0.0343, 0.0226, 0.0110, 0.0001, 0.0001];   Figure 4, Speech weighting filter transfer function graph. "},{"title":"References","type":1,"pageTitle":"Modelling the scenario","url":"docs/cec1_scenario#references","content":"Schröder, D. and Vorländer, M., 2011, January. RAVEN: A real-time framework for the auralization of interactive virtual environments. In Proceedings of Forum Acusticum 2011 (pp. 1541-1546). Denmark: Aalborg.Abed, A.H.M. and Cain, G.D., 1978. Low-pass digital filtering with the host windowing design technique. Radio and Electronic Engineer, 48(6), pp.293-300.Abed, A.E. and Cain, G., 1984. The host windowing technique for FIR digital filter design. IEEE transactions on acoustics, speech, and signal processing, 32(4), pp.683-694. "}]